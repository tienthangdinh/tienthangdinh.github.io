[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "haha",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning\n\n\n\nmachine-learning\n\nRL\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nDinh Tien Thang\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 22, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html",
    "href": "posts/reinforcement-learning/index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) is a subfield of machine learning concerned with how agents ought to take actions in an environment in order to maximize cumulative reward."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#markov-decision-process-mdp",
    "href": "posts/reinforcement-learning/index.html#markov-decision-process-mdp",
    "title": "Reinforcement Learning",
    "section": "Markov Decision Process (MDP)",
    "text": "Markov Decision Process (MDP)\nAn agent’s interaction with the environment is usually modeled as a Markov Decision Process (MDP):\ns₀, a₀, r₀, s₁, a₁, r₁, s₂, a₂, r₂, ...\nThis sequence is called an episode, and it may or may not terminate depending on the task.\n\nMarkov Property\n\nThe probability of the next state depends only on the current state and action — not the full history: \\[\nP(s_{t+1} \\mid s_t, a_t) = P(s_{t+1} \\mid s_1, a_1, ..., s_t, a_t)\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#goal-of-reinforcement-learning",
    "href": "posts/reinforcement-learning/index.html#goal-of-reinforcement-learning",
    "title": "Reinforcement Learning",
    "section": "Goal of Reinforcement Learning",
    "text": "Goal of Reinforcement Learning\nThe goal is to learn a policy \\(\\pi\\) that tells the agent which action to take in each state in order to maximize cumulative reward.\nThis is often visualized as:\ns₀, a₀, r₀ → (s₁, a₁, r₁) → s₂, a₂, r₂ → ...\nWe define the return \\(G_t\\) as the total discounted reward:\nLet:\n\n( t {0, 1, 2, } )\n( s ): a state\n( a (s) ): an action available in state ( s )\n( r ): a scalar reward\n\nThe environment dynamics (transition model) are given by:\n\\[\np(s', r \\mid s, a) = \\text{Prob}(S_{t+1} = s',\\ R_{t+1} = r \\mid S_t = s,\\ A_t = a)\n\\]\n\n\nPolicy\n\nStochastic: ( (a s) )\nDeterministic: ( a = (s) )\n\n\n\n\nReturn\nThe return ( G_t ) is the total discounted reward from time ( t+1 ) to final time ( T ):\n\\[\nG_t = \\sum_{k = t+1}^{T} \\gamma^{k - t - 1} R_k\n\\]\nExpanded:\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{T - t - 1} R_T\n\\]\n\n\nGoal\n\\[\n\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\n\\]\nHow to determine a policy that will accumulate a lot of rewards? # Value Functions\nUsed to estimate expected returns:\n\nState-value function: \\(V_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\)\nAction-value function: \\(Q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\nRelationship: \\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#value-function",
    "href": "posts/reinforcement-learning/index.html#value-function",
    "title": "Reinforcement Learning",
    "section": "Value Function",
    "text": "Value Function\nkeep track of the average return Gt expected when following a certain policy pi at a state (s) or (state, action) (s,a) - state value: V_pi(s) - action value: Q_pi(s,a) - actually V_pi(s) = Expected[Q_pi(a,s)]\nGoal: best case scenario (optimal) V(s), Q(s,a)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sarsa-on-policy",
    "href": "posts/reinforcement-learning/index.html#sarsa-on-policy",
    "title": "Reinforcement Learning",
    "section": "SARSA (On-policy)",
    "text": "SARSA (On-policy)\nGiven a (state, action), how much better or worse is my return Gt relative to the new state and the SPECIFIC action that I ended up taking?\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n\\] BUT: there are many actions, not just one action a+1 so what action to take? We do not have access to p(s’,a’|s,a) so we do not know But we can calculate what we can expect in the next state after we take an action a ## Expected SARSA Given a (state, action), how much better or worse is my return Gt relative to the new state withh ALL actions AVERAGED OUT acording to policy probability pi? - we can calculate all the possible action at the next state and take the expected average with the policy probability (expected SARSA) \\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\sum_a \\pi(a \\mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning-off-policy",
    "href": "posts/reinforcement-learning/index.html#q-learning-off-policy",
    "title": "Reinforcement Learning",
    "section": "Q-Learning (Off-policy)",
    "text": "Q-Learning (Off-policy)\nGiven a (state, action), how much better or worse is my return Gt relative to the new state with the best action? \\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#credit-assignment-problem",
    "href": "posts/reinforcement-learning/index.html#credit-assignment-problem",
    "title": "Reinforcement Learning",
    "section": "Credit assignment problem",
    "text": "Credit assignment problem\nwithin an episode, figure out what subpath made the episode bad or good a.k.a individual action impact on the episode =&gt;"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning",
    "href": "posts/reinforcement-learning/index.html#q-learning",
    "title": "Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\nDevelop a function ( Q ) to approximate ( Q_* ), by updating:\n\\[\nQ(s_t, a_t) \\rightarrow r_t + \\gamma \\max_a Q(s_{t+1}, a)\n\\]\nRemember, the environment is random (many possible next states/rewards).\nAccording to the world model:\n\\[\np(s', r \\mid s, a)\n\\]\n(You don’t have access to this model directly.)\nTherefore, We do Q-learning with enough sample trajectories, we can average them out Equation that describes what ( Q_* ) is actually supposed to be: Result of samples averaging out \\[\nQ_*(s_t, a_t) = \\mathbb{E} \\left[ r_t + \\gamma \\max_a Q_*(s_{t+1}, a) \\right]\n\\]\n\nNote: ( [] ) = expected value = weighted average according to probabilities\n\n=&gt; That means, eventhough we do not have world model, when we do sampling, we still can gather enough information near to it"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]