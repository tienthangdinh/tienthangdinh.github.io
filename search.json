[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html",
    "href": "posts/reinforcement-learning/index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) is a subfield of machine learning concerned with how agents ought to take actions in an environment in order to maximize cumulative reward."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#markov-decision-process-mdp",
    "href": "posts/reinforcement-learning/index.html#markov-decision-process-mdp",
    "title": "Reinforcement Learning",
    "section": "Markov Decision Process (MDP)",
    "text": "Markov Decision Process (MDP)\nAn agent’s interaction with the environment is usually modeled as a Markov Decision Process (MDP):\ns₀, a₀, r₀ → (s₁, a₁, R₁) → s₂, a₂, R₂ → …\nThis sequence is called an episode, and it may or may not terminate depending on the task.\nLet:\n\n\\(t \\in \\{0, 1, 2, \\dots\\}\\)\n\\(s \\in \\mathcal{S}\\): a state\n\\(a \\in \\mathcal{A}(s)\\): an action available in state \\(s\\)\n\\(r \\in \\mathcal{R} \\subseteq \\mathbb{R}\\): a scalar reward\n\nThe environment dynamics (transition model) are given by:\n\\[\np(s', r \\mid s, a) = \\text{Prob}(S_{t+1} = s',\\ R_{t+1} = r \\mid S_t = s,\\ A_t = a)\n\\]\n\nMarkov Property\n\nThe probability of the next state depends only on the current state and action — not the full history: \\[\nP(s_{t+1} \\mid s_t, a_t) = P(s_{t+1} \\mid s_1, a_1, ..., s_t, a_t)\n\\]\n\n\n\n\nPolicy\n\nStochastic: \\(\\pi(a \\mid s)\\)\nDeterministic: \\(a = \\pi(s)\\)\n\n\n\n\nReturn\nThe return \\(G_t\\) is the total discounted reward from time \\(t+1\\) to final time \\(T\\):\n\\[\nG_t = \\sum_{k = t+1}^{T} \\gamma^{k - t - 1} R_k\n\\]\nExpanded:\n\\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\]\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{T - t - 1} R_T\n\\]\n\n\nGoal\n\\[\n\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\n\\]\n\nHow can we determine a policy that accumulates a high reward?"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "href": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "title": "Reinforcement Learning",
    "section": "The Bellman Equations",
    "text": "The Bellman Equations\nFor any policy ( ), all ( s ), and all ( a (s) ):\nState-value function:\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\, q_\\pi(s, a) \\tag{1}\n\\]\nAction-value function:\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right] \\tag{2}\n\\]\nAfter substitution as derived above:\n\nBellman Equation for \\(v_\\pi(s)\\)\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]\n\n\nBellman Equation for \\(q_\\pi(s, a)\\)\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a' \\in \\mathcal{A}(s')} \\pi(a' \\mid s') q_\\pi(s', a') \\right]\n\\]\n\n\n\nOptimal Bellman\n\\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#policy-evaluation-policy-improvement",
    "href": "posts/reinforcement-learning/index.html#policy-evaluation-policy-improvement",
    "title": "Reinforcement Learning",
    "section": "Policy Evaluation & Policy Improvement",
    "text": "Policy Evaluation & Policy Improvement\nUsing the Bellman Equation above and an initial estimate (e.g. all states are randomly initialized or uniformly negative, except the terminal state which is 0), we iteratively update:\nWe apply the Bellman equations above for every single state (for V), or (state, action) pair (for Q, which forms a finer table). Over many runs, it will slowly converge. \\[\n\\pi'(s) = \\arg\\max_a q_\\pi(s, a)\n\\]\nWhere:\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#value-function",
    "href": "posts/reinforcement-learning/index.html#value-function",
    "title": "Reinforcement Learning",
    "section": "Value Function",
    "text": "Value Function\nkeep track of the average return Gt expected when following a certain policy pi at a state (s) or (state, action) (s,a) - state value: V_pi(s) - action value: Q_pi(s,a) - actually V_pi(s) = Expected[Q_pi(a,s)]\nGoal: best case scenario (optimal) V(s), Q(s,a)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sarsa-on-policy",
    "href": "posts/reinforcement-learning/index.html#sarsa-on-policy",
    "title": "Reinforcement Learning",
    "section": "SARSA (On-policy)",
    "text": "SARSA (On-policy)\nGiven a (state, action), how much better or worse is my return Gt relative to the new state and the SPECIFIC action that I ended up taking?\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n\\] BUT: there are many actions, not just one action a+1 so what action to take? We do not have access to p(s’,a’|s,a) so we do not know But we can calculate what we can expect in the next state after we take an action a ## Expected SARSA Given a (state, action), how much better or worse is my return Gt relative to the new state withh ALL actions AVERAGED OUT acording to policy probability pi? - we can calculate all the possible action at the next state and take the expected average with the policy probability (expected SARSA) \\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\sum_a \\pi(a \\mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning-off-policy",
    "href": "posts/reinforcement-learning/index.html#q-learning-off-policy",
    "title": "Reinforcement Learning",
    "section": "Q-Learning (Off-policy)",
    "text": "Q-Learning (Off-policy)\nGiven a (state, action), how much better or worse is my return Gt relative to the new state with the best action? \\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#credit-assignment-problem",
    "href": "posts/reinforcement-learning/index.html#credit-assignment-problem",
    "title": "Reinforcement Learning",
    "section": "Credit assignment problem",
    "text": "Credit assignment problem\nwithin an episode, figure out what subpath made the episode bad or good a.k.a individual action impact on the episode =&gt;"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning",
    "href": "posts/reinforcement-learning/index.html#q-learning",
    "title": "Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\nDevelop a function \\[ Q \\] to approximate \\[ Q_* \\], by updating:\n\\[\nQ(s_t, a_t) \\rightarrow r_t + \\gamma \\max_a Q(s_{t+1}, a)\n\\]\nRemember, the environment is random (many possible next states/rewards).\nAccording to the world model:\n\\[\np(s', r \\mid s, a)\n\\]\n(You don’t have access to this model directly.)\nTherefore, We do Q-learning with enough sample trajectories, we can average them out Equation that describes what \\[ Q_* \\] is actually supposed to be: Result of samples averaging out \\[\nQ_*(s_t, a_t) = \\mathbb{E} \\left[ r_t + \\gamma \\max_a Q_*(s_{t+1}, a) \\right]\n\\]\n\nNote: \\[ \\mathbb{E}[\\cdot] \\] = expected value = weighted average according to probabilities\n\n=&gt; That means, eventhough we do not have world model, when we do sampling, we still can gather enough information near to it"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "haha",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning\n\n\n\nmachine-learning\n\nRL\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nDinh Tien Thang\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 22, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  }
]