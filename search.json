[
  {
    "objectID": "posts/pet-function1rv/index.html",
    "href": "posts/pet-function1rv/index.html",
    "title": "Functions of a Random Variable",
    "section": "",
    "text": "When we have a random variable \\(X\\) and a function \\(g(x)\\), Y is also a random variable:\n\\[Y = g(X)\\]\nThe probability of \\(Y\\) is equal to the probability of the set of all \\(X\\) values that map into \\(B\\):\n\\[P(Y \\in B) = P(X \\in g^{-1}(B))\\]\n\n\nTo find the distribution of \\(Y\\), we HAVE TO START with the CDF - aka the Probability and then differentiate to find the PDF.\n\nFind the Inverse Transform: \\[Y = g(X) \\iff X = g^{-1}(Y)\\]\nThe CDF of \\(Y\\) (important inverse transform): \\[F_Y(y) = F_X(g^{-1}(y)) = F_{X2}(g^{-1}(y)) - F_{X1}(g^{-1}(y))\\]\nThe PDF of \\(Y\\): \\[f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}F_X(g^{-1}(y)) = \\frac{d}{dy}[F_{X2}(g^{-1}(y)) - F_{X1}(g^{-1}(y))]\\]\n\n\n\n\n\n\\[Y = X^2 \\iff X = \\pm \\sqrt{y}\\]\n\n\n\n\\[F_Y(y) = P(X^2 \\le y) = P(-\\sqrt{y} \\le X \\le \\sqrt{y}) = F_X(\\sqrt{y}) - F_X(-\\sqrt{y})\\]\n\n\n\nDerivative with respect to \\(y\\) using the Chain Rule:\n\\[f_Y(y) = \\frac{d}{dy} [F_X(\\sqrt{y}) - F_X(-\\sqrt{y})]\\] \\[f_Y(y) = f_X(\\sqrt{y}) \\cdot \\frac{1}{2\\sqrt{y}} + f_X(-\\sqrt{y}) \\cdot \\frac{1}{2\\sqrt{y}}\\] \\[f_Y(y) = \\frac{1}{2\\sqrt{y}} [f_X(\\sqrt{y}) + f_X(-\\sqrt{y})], \\quad y &gt; 0\\]\n\n\n\nIf \\(X\\) is normal distributed \\(X \\sim N(0,1)\\), its density is \\(f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\\). Because it is symmetric, \\(f_X(\\sqrt{y}) = f_X(-\\sqrt{y})\\). We apply numbers on the above formula. \\[f_Y(y) = \\frac{1}{2\\sqrt{y}} \\left[ 2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-(\\sqrt{y})^2/2} \\right] = \\frac{1}{\\sqrt{2\\pi y}} e^{-y/2}\\] This is the Chi-square distribution with 1 degree of freedom.\nIf \\(X \\sim U(-1, 1)\\) uniformly distributed. \\(f_X(x) = \\frac{1}{2}\\) for \\(x \\in [-1, 1]\\). So \\(f_X(\\sqrt{y}) = 1/2 = f_X(-\\sqrt{y})\\). Plugging these in: \\[f_Y(y) = \\frac{1}{2\\sqrt{y}} \\left[ \\frac{1}{2} + \\frac{1}{2} \\right] = \\frac{1}{2\\sqrt{y}}\\] The density \\(f_Y(y)\\) is very high near \\(0\\). Because when you square numbers near zero (like \\(0.1\\) or \\(-0.1\\)), they ‚Äúclump‚Äù together into even smaller numbers (\\(0.01\\)). This ‚Äúclumping‚Äù effect creates the high density (the spike) near zero.\n\n\n\n\nWhat if at different range the function looks different? \\[Y = g(X) = \\begin{cases} X - c, & X &gt; c \\\\ 0, & -c &lt; X \\le c \\\\ X + c, & X \\le -c \\end{cases}\\]\nJust do exactly the same thing above at each those range\n\n\n\\[P(Y = 0) = P(-c &lt; X \\le c) = F_X(c) - F_X(-c)\\]\n\n\n\n\\[F_Y(y) = P(Y \\le y) = P(X - c \\le y) = P(X \\le y + c)\\] \\[F_Y(y) = F_X(y + c), \\quad y &gt; 0\\]\n\n\n\n\\[F_Y(y) = P(Y \\le y) = P(X + c \\le y) = P(X \\le y - c)\\] \\[F_Y(y) = F_X(y - c), \\quad y &lt; 0\\]\n\n\n\nTo find the PDF, we differentiate the CDF \\[F_Y(y) = F_X(y + c)\\] \\[f_Y(y) = \\frac{d}{dy}[F_X(y + c)]\\] \\[f_Y(y) = f_X(y + c) \\cdot \\frac{d}{dy}(y + c)\\] Since \\(\\frac{d}{dy}(y + c) = 1\\), we are left with: \\[f_Y(y) = f_X(y + c)\\] Therefore: \\[f_Y(y) = \\begin{cases} f_X(y + c), & y &gt; 0 \\\\ [F_X(c) - F_X(-c)]\\delta(y), & y = 0 \\\\ f_X(y - c), & y &lt; 0 \\end{cases}\\]"
  },
  {
    "objectID": "posts/pet-function1rv/index.html#but-how-to-find-this-distribution-of-y",
    "href": "posts/pet-function1rv/index.html#but-how-to-find-this-distribution-of-y",
    "title": "Functions of a Random Variable",
    "section": "",
    "text": "To find the distribution of \\(Y\\), we HAVE TO START with the CDF - aka the Probability and then differentiate to find the PDF.\n\nFind the Inverse Transform: \\[Y = g(X) \\iff X = g^{-1}(Y)\\]\nThe CDF of \\(Y\\) (important inverse transform): \\[F_Y(y) = F_X(g^{-1}(y)) = F_{X2}(g^{-1}(y)) - F_{X1}(g^{-1}(y))\\]\nThe PDF of \\(Y\\): \\[f_Y(y) = \\frac{d}{dy}F_Y(y) = \\frac{d}{dy}F_X(g^{-1}(y)) = \\frac{d}{dy}[F_{X2}(g^{-1}(y)) - F_{X1}(g^{-1}(y))]\\]\n\n\n\n\n\n\\[Y = X^2 \\iff X = \\pm \\sqrt{y}\\]\n\n\n\n\\[F_Y(y) = P(X^2 \\le y) = P(-\\sqrt{y} \\le X \\le \\sqrt{y}) = F_X(\\sqrt{y}) - F_X(-\\sqrt{y})\\]\n\n\n\nDerivative with respect to \\(y\\) using the Chain Rule:\n\\[f_Y(y) = \\frac{d}{dy} [F_X(\\sqrt{y}) - F_X(-\\sqrt{y})]\\] \\[f_Y(y) = f_X(\\sqrt{y}) \\cdot \\frac{1}{2\\sqrt{y}} + f_X(-\\sqrt{y}) \\cdot \\frac{1}{2\\sqrt{y}}\\] \\[f_Y(y) = \\frac{1}{2\\sqrt{y}} [f_X(\\sqrt{y}) + f_X(-\\sqrt{y})], \\quad y &gt; 0\\]\n\n\n\nIf \\(X\\) is normal distributed \\(X \\sim N(0,1)\\), its density is \\(f_X(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\\). Because it is symmetric, \\(f_X(\\sqrt{y}) = f_X(-\\sqrt{y})\\). We apply numbers on the above formula. \\[f_Y(y) = \\frac{1}{2\\sqrt{y}} \\left[ 2 \\cdot \\frac{1}{\\sqrt{2\\pi}} e^{-(\\sqrt{y})^2/2} \\right] = \\frac{1}{\\sqrt{2\\pi y}} e^{-y/2}\\] This is the Chi-square distribution with 1 degree of freedom.\nIf \\(X \\sim U(-1, 1)\\) uniformly distributed. \\(f_X(x) = \\frac{1}{2}\\) for \\(x \\in [-1, 1]\\). So \\(f_X(\\sqrt{y}) = 1/2 = f_X(-\\sqrt{y})\\). Plugging these in: \\[f_Y(y) = \\frac{1}{2\\sqrt{y}} \\left[ \\frac{1}{2} + \\frac{1}{2} \\right] = \\frac{1}{2\\sqrt{y}}\\] The density \\(f_Y(y)\\) is very high near \\(0\\). Because when you square numbers near zero (like \\(0.1\\) or \\(-0.1\\)), they ‚Äúclump‚Äù together into even smaller numbers (\\(0.01\\)). This ‚Äúclumping‚Äù effect creates the high density (the spike) near zero.\n\n\n\n\nWhat if at different range the function looks different? \\[Y = g(X) = \\begin{cases} X - c, & X &gt; c \\\\ 0, & -c &lt; X \\le c \\\\ X + c, & X \\le -c \\end{cases}\\]\nJust do exactly the same thing above at each those range\n\n\n\\[P(Y = 0) = P(-c &lt; X \\le c) = F_X(c) - F_X(-c)\\]\n\n\n\n\\[F_Y(y) = P(Y \\le y) = P(X - c \\le y) = P(X \\le y + c)\\] \\[F_Y(y) = F_X(y + c), \\quad y &gt; 0\\]\n\n\n\n\\[F_Y(y) = P(Y \\le y) = P(X + c \\le y) = P(X \\le y - c)\\] \\[F_Y(y) = F_X(y - c), \\quad y &lt; 0\\]\n\n\n\nTo find the PDF, we differentiate the CDF \\[F_Y(y) = F_X(y + c)\\] \\[f_Y(y) = \\frac{d}{dy}[F_X(y + c)]\\] \\[f_Y(y) = f_X(y + c) \\cdot \\frac{d}{dy}(y + c)\\] Since \\(\\frac{d}{dy}(y + c) = 1\\), we are left with: \\[f_Y(y) = f_X(y + c)\\] Therefore: \\[f_Y(y) = \\begin{cases} f_X(y + c), & y &gt; 0 \\\\ [F_X(c) - F_X(-c)]\\delta(y), & y = 0 \\\\ f_X(y - c), & y &lt; 0 \\end{cases}\\]"
  },
  {
    "objectID": "posts/pet-function1rv/index.html#firstly-for-x",
    "href": "posts/pet-function1rv/index.html#firstly-for-x",
    "title": "Functions of a Random Variable",
    "section": "Firstly, for X:",
    "text": "Firstly, for X:\n\\[\\text{Var}(X) = E[(X - \\mu)^2] = \\int_{-\\infty}^{+\\infty} (x - \\mu)^2 f_X(x)dx\\] Because \\((x - \\mu)^2 = x^2 - 2x\\mu + \\mu^2\\), we have: \\[\\text{Var}(X) = \\int_{-\\infty}^{+\\infty} x^2 f_X(x)dx - \\int_{-\\infty}^{+\\infty} 2x\\mu f_X(x)dx + \\int_{-\\infty}^{+\\infty} \\mu^2 f_X(x)dx\\] \\[\\text{Var}(X) = \\int_{-\\infty}^{+\\infty} x^2 f_X(x)dx - 2\\mu \\int_{-\\infty}^{+\\infty} x f_X(x)dx + \\mu^2 \\int_{-\\infty}^{+\\infty} f_X(x)dx\\] \\[\\text{Var}(X) = E(X^2) - 2\\mu E(X) + \\mu^2 \\cdot 1\\] \\[\\text{Var}(X) = E(X^2) - 2\\mu \\cdot \\mu + \\mu^2\\] \\[\\text{Var}(X) = E(X^2) - 2\\mu^2 + \\mu^2\\] \\[\\text{Var}(X) = E(X^2) - \\mu^2\\] \\[\\text{Var}(X) = E(X^2) - [E(X)]^2\\]\n\nExample 1: Poisson distribution \\(X \\sim P(\\lambda)\\)\n\nThe mean \\(E(X) = \\lambda\\).\nThe second moment \\(E(X^2) = \\lambda^2 + \\lambda\\).\nVariance: \\(\\sigma_X^2 = (\\lambda^2 + \\lambda) - \\lambda^2 = \\lambda\\). So for a Poisson distribution, the mean and the variance are identical."
  },
  {
    "objectID": "posts/pet-function1rv/index.html#now-for-y-gx",
    "href": "posts/pet-function1rv/index.html#now-for-y-gx",
    "title": "Functions of a Random Variable",
    "section": "Now for \\(Y = g(X)\\):",
    "text": "Now for \\(Y = g(X)\\):\n\\[\\sigma_Y^2 = Var(Y) = E[(Y - \\mu_Y)^2] = \\int_{-\\infty}^{+\\infty} (y - \\mu_Y)^2 f_Y(y) dy\\]\nBut you know what is a good news? We don‚Äôt need to calculate \\(f_Y(y)\\) at all!\n\\[\\sigma_Y^2 = Var(Y) = \\int_{-\\infty}^{+\\infty} (g(x) - \\mu_Y)^2 f_X(x) dx\\]\nThis means you can find the variance of \\(Y\\) by simply integrating the function \\((g(x) - \\mu_Y)^2\\) weighted by the original density of \\(X\\)."
  },
  {
    "objectID": "posts/cg2-modelling/index.html",
    "href": "posts/cg2-modelling/index.html",
    "title": "Advanced Modelling",
    "section": "",
    "text": "Implicit Surface\n\nDefine the term implicit surface and explain the idea with a sketch!\n\n\nf: R¬≥ ‚Üí R: f(x, y, z) = 0\nThe Interior: f(x) &lt; 0.\nThe Surface: f(x) = 0.\nThe Exterior: f(x) &gt; 0.\nThey use the following operations: levelset, CSG,\n\n\nWhat is a regular implicit function and what does the implicit function theorem say about them?\n\n\nRegular Implicit Function: An implicit function f is called regular if its gradient ‚àáf(x) does not vanish aka = 0 (no cut itself)\nImplicit Function Theorem: if f is a regular function, then the implicit surface S(f) is a manifold.\n\n\nHow can one compute the surface normal of an implicit surface?\n\n\nThe gradient vector ‚àáf(x) is always orthogonal to the surface at x: n(x) ‚àù ‚àáf(x)\n\n\nExplain the advantage of an analytic computation of the gradient of an implicit function!\n\n\nAccuracy: no numeric approximatation\nFast: no need numeric apprixmiation\n\n\nCompare levelset surfaces with implicit surfaces!\n\n\nStart with implicit surface S(f) = {x | f(x) = 0}.\nA level set is an entire family of this function: l_Œª(x) = f(x) - Œª\n\n\nHow can CSG-operations be expressed in the formalism of implicit functions?\n\n\nCSG on implicit functions (f‚ÇÅ, f‚ÇÇ, etc.), think like what would a position value be:\nf_union(x) = min(f‚ÇÅ(x), f‚ÇÇ(x)).\nf_intersection(x) = max(f‚ÇÅ(x), f‚ÇÇ(x)).\nDifference (A ¬†B): The set of points in A but not in B. This corresponds to f_difference(x) = max(f‚ÇÅ(x), -f‚ÇÇ(x)).\n\n\nName examples of implicit surface primitives!\n\n\nImplicit surface primitives can be defined in several ways:\n\nSDF that we all know (can utilize mixed Minkowski) (sphere f(x) = sqrt(x¬≤ + y¬≤ + z¬≤) - r)\nAlgebraic Surfaces: e.g.¬†Quadrics (e.g., spheres, cylinders, cones) are algebraic surfaces of degree two. (sphere f(x) = x¬≤ + y¬≤ + z¬≤ - r¬≤)\nRadial Basis Functions: Used to define ‚Äúmeta-balls‚Äù or ‚Äúblobs‚Äù where a radially symmetric potential field is centered on a set of points.\nLattice Basis: Using a spline basis over a grid to define the implicit function.\n\n\n\nGiven an image of a superquadric estimate the exponents ùëù‚ÇÅ and ùëù‚ÇÇ of the used Minkowski norms!\n\n\nExponent p‚ÇÅ controls the shape in the xy-plane (looking down the z-axis).\n\np‚ÇÅ &lt; 2: The shape is concave or ‚Äústar-like‚Äù.\np‚ÇÅ = 2: The cross-section is a perfect circle.\np‚ÇÅ &gt; 2: The cross-section becomes more ‚Äúsquare-like‚Äù.\n\nExponent p‚ÇÇ controls the shape in the rz-plane (the profile along the z-axis).\n\np‚ÇÇ &lt; 2: The shape is pointy at the poles.\np‚ÇÇ = 2: The profile is circular.\np‚ÇÇ &gt; 2: The profile becomes more ‚Äúbox-like‚Äù.\n\n\n\nName same example shapes that can be represented by a quadratic implicit surface and precisely specify implicit functions for sphere and cylinder!\n\n\nSphere/Ellipsoid (centered at origin): f(x, y, z) = x¬≤ + y¬≤ + z¬≤ - 1 = 0\nCylinder (aligned with z-axis): f(x, y, z) = x¬≤ + y¬≤ - 1 = 0\n\n\nExplain the principle of meta balls (Gaussian Splatting?)!\n\n\nMeta balls (or blobs) are an implicit modeling technique used to create organic, smoothly blended shapes. The principle is:\nA set of ‚Äúatoms‚Äù (center points c_i) are defined in space.\nEach atom generates a potential field function h_i(x) that decreases with distance from the atom‚Äôs center (e.g., a Gaussian function exp(-||x - c_i||¬≤ / r_i¬≤)).\nThe overall implicit function f(x) is defined by summing the potentials from all atoms and subtracting this sum from a flipped Gaussian value: f(x) = 1/e - Œ£ h_i(x) (look at that function in the slide you will understand)\nThe resulting implicit surface f(x) = 0 is a smooth surface that naturally blends together where the potential fields of different atoms overlap.\n\n\nExplain the Marching Cubes algorithm!\n\n\nGoal: extracts a polygonal mesh (tessellates) an implicit surface defined on a grid. The steps are:\nFor each voxel, evaluate the implicit function f at its 8 corners.\nClassify each corner as being inside (f &lt; 0) or outside (f &gt; 0). This creates an 8-bit index for the voxel (one bit per corner).\nlookup edges connecting interior with exterior using that 8bit\nFor each intersected edge, calculate the precise intersection point (the ‚Äúzero crossing‚Äù).\nConnect it\n\n\nWhat problems arise at sharp creases and corners when using Marching Cubes?\n\n\ncannot reproduce sharp creases or corners\n\n\nHow can one reconstruct sharp creases and corners with the Dual Contouring approach?\n\n\nIdea: placing additionally single representative vertex inside each grid cell that is intersected by the surface, rather than on the edges the whole time.\nFor every grid edge that has a sign change, find the zero-crossing point and the surface normal (gradient) at that point. These define a tangent plane.\nFind the optimal point p that best fits all the tangent planes using Quadric Error Metric (QEM).\n\n\nCompare Space Warping and Function Value Mapping for the manipulation of implicit surfaces!\n\n\nFunction Value Mapping: combining the scalar values from generic functions: m^F(p) = m(f‚ÇÅ(p), f‚ÇÇ(p), ‚Ä¶):\n\nCSG: Union (min), Intersection (max), Difference (max(f‚ÇÅ, -f‚ÇÇ)).\nLevelset Mapping: m(f, Œª) = f - Œª\nOutline Mapping: m(f, Œª) = f¬≤ - Œª¬≤ (creates two offset surfaces).\nBlending: Smooth approximations of min and max for CSG union and intersection.\nAdding Noise: m(f) = f + noise()\n\nSpace Warping: These deform the surface by modifying the space (input coordinate)s before they are evaluated by the function f.¬†The new function is f‚Äô(q) = f(w(q)), where w is the inverse warp.\n\nLinear Transformations: Affine transformations like rotation, scaling, and shear, represented by a matrix M. The warp is w(q) = M‚Åª¬πq.\nTaper: A non-uniform scaling along an axis.\nTwist: A rotation around an axis where the angle of rotation depends on the position along that axis.\nBend: Deforming an object along a circular arc.\n\n\n\nExplain how to transform an implicit surface with an affine transformation! How can one compute the gradient of the transformed implicit surface?\n\n\nTo transform an implicit surface f(p) = 0 with an affine transformation to q = M*p:\nFind the inverse p = w(q) = M‚Åª¬πq. (bringing the coordinate system there)\nThen apply the original function: f^w(q) = f(w(q)) = f(M‚Åª¬πq)\nThe gradient of the transformed surface is found using the chain rule: ‚àáf^w = ‚àáf(p) ‚ãÖ J_w where ‚àáf(p) is the gradient of the original function evaluated at the un-warped point p, and J_w is the Jacobian of the warp function. For a linear transformation, the Jacobian is simply the matrix itself, so: ‚àáf^w(q) = ‚àáf(M‚Åª¬πq) ‚ãÖ M‚Åª¬π\n\n\nGiven an image of a space warped implicit surface, argue whether it was generated via a Tapper, Twist or Bend transformation! we know\nExplain modified union and intersection operations that smoothly blend between implicit surfaces!\n\n\nCSG union (min) putting a sphere at outside (1, 1). =&gt; (1-f1)^2 + (1-f2)^2\nCSG intersection (max) putting a sphere inside (-1, -1)\n\n\nHow can one define the extent of the smoothing area?\n\n\naka the blending radius, can be controlled independently for each surface. Normalizing d_i = f_i / r_i\n\n\nWhat problems arise in areas where the implicit surfaces coincide? How can one conquer these problems?\n\n\nwhen 2 blended surfaces become parallel or nearly parallel, unwanted bulging artifacts can occur where the blending influences undesirably accumulate. Idea: control when parallel =&gt; no blending, but when 90 deg. =&gt; max blending: d_i ‚Üí d_i / (1 - |cos Œ∏|)\n\n\nSkeleton based with hard Implicit Surface\n\n\nGiven a point p: find the dot product between AB, and AP to get the projection (clamp between 0, 1)\nGet the height from that projection\nMinus 1 (to make the surface to 1)\n\n\nHow to do similar thing using Convolution?\n\n\nAn implicit surface generated from a skeleton (a geometric primitive like a point, line, or polygon) and a potential function (or filter kernel, h).\nImplementation: skeleton function g (which is 1 on the skeleton and 0 elsewhere) convoluted by the potential function h. The final implicit surface is a level set of this resulting scalar field: f(r) = const - (g * h)(r) = 0.\n\n\nCompare distance surfaces and convolution surfaces!\n\n\nDistance Surfaces: easy to implement but sharp\nConvolution Surfaces: smooth (can produce bulging) but more computationally complex\n\n\nHow can one evaluate a convolution surface defined over a set of primitives?\n\n\nsumming the result of each primitive skeleton\n\n\nWhich filter kernels do you know and over which can these be integrated analytically?\n\n\nGaussian\nCauchy\nInverse powers of radius (radial basis)\n\n\nDo bulging artefacts also arise for convolution surfaces and if so, how can they be cured?\n\n\nSolution:\n\nThick Skeletons: skeleton width thicker than the kernel radius\nWeighted Skeletons: Use weights that decrease at the intersections of skeleton primitives\n\n\n\n\n3D Scanning\n\nName some techniques to acquire the shape of 3D objects!\n\n\nStructured light (RGBD)\nstereo acquisition RGBD\nTomographic reconstruction from X-ray images (CT scan).\nTime-of-flight\nMagnetic Resonance Tomography (MRI).\n\n\nExplain the idea of a structured light scanner!\n\n\nknown structured pattern of light is projected (e.g., vertical stripes) onto the 3D object.\nThe projected pattern appears distorted in the image due to the object‚Äôs surface shape.\nBecause the system is calibrated, for any pixel in the camera image, we can determine which stripe\ntriangulation.\n\n\nWhat is a homography?\n\n\ntransformation 3x3 homogeneous matrix H maps points from one 2D plane to another\n\n\nHow many feature correspondences are needed to estimate a homography?\n\n\n3x3 matrix has 9 parameters but only 8 variables needed, the last one set to 1 (doesnt matter, scalable).\nSince each point-to-point correspondence provides 2 linear equations (x, y), needs 4 corresponding pairs\ncalculate using SVD\n\n\nUnder which situations can two images be brought into correspondence with a homography?\n\n\nPlanar Scene: The images are of a 3D scene where all the points of interest lie on a single plane (like a checkerboard). The camera can undergo any rotation and translation.\nPure Rotation: The camera only rotates around its optical center (pinhole) and does not translate. In this case, the 3D scene can have any arbitrary shape.\n\n\nCALIBRATION: What are the intrinsic and extrinsic camera parameters in the pinhole camera model?\n\n\nExtrinsic Parameters: camera‚Äôs position and orientation relative to the world coordinate system.\nIntrinsic Parameters: focal lengths in pixel units (sx, sy), the principal point (the pixel coordinates where the optical axis intersects the image plane) (cx, cy), and a skew parameter (h).\n\n\nHow many degrees of freedom do we have for the intrinsic and extrinsic parameters?\n\n\nExtrinsic Parameters: 3 for rotation and 3 for translation, for a total of 6 degrees of freedom.\nIntrinsic Parameters: 2 focal length at diagonal (scaling) + 2 principle point (translation) + 1 skew matrix (shear) = 5 (sx, sy, cx, cy, h).\n\n\nWhy is the pinhole model in practice not sufficient and how can it be extended?\n\n\nnon-linear distortion (if you show me formula i can explain)\n\n\nExplain the procedure for camera calibration according to Zhang! Zhang‚Äôs method is a widely used camera calibration technique:\n\n\nAcquire 3 images from different orientations.\nDetect multiples anker points of the image (mostly corners)\nfor each new image -&gt; a new homography H!!! nice!!! (hollistic intrinsic K + extrinsic [R|t])\n2 equations each image, and utilizing \\(H = K * [R|t]\\):\n\nR are orthogonal =&gt; dot products =&gt; creating inverse K \\(r_1r_2 = h_1^TK^{-T}K^{-1}h_2\\)\nR should have same size everywhere =&gt; \\(h_1^TK^{-T}K^{-1}h1 = h2^TK^{-T}K^{-1}h2\\)\n\n3 images * 2 equations each images we have 6 equation to find \\(K\\) (in slides is B)\nadd more non-linear distortion if needed\n\n\nHow many shots of the calibration plate are needed at least? Why?\n\n\n5 linear intrinsic parameter (ffcch)\nA minimum of 3 images are needed. each image = 1 homography = 2 linear constraints inintrinsic parameters derived.\n=&gt; 3 views (providing 6 constraints) are required to solve the system of equations.\n\n\nWhy are the parameters of the camera model estimated in two steps?\n\n\nLinear Estimation: first step ignores distortion =&gt; initial point\nNon-linear Refinement: include distortion using algorithm to solve\n\n\nWhat is the typical procedure to calibrate a camera?\n\n\nlike said above (2 big steps linear for intrinsic) =&gt; nonlinear for distortion\n\n\nExplain a method to calibrate a camera-projector setup!\n\n\n‚Äúinverse progress‚Äù:\nCalibrate the camera first using a standard method.\nTo establish correspondences for the projector, project gray-codes that encode the projector‚Äôs pixel columns and rows onto a calibration plane\nThe calibrated camera captures images of these patterns. This allows you to determine for each camera pixel which projector pixel is illuminating that spot.\nBy detecting the checkerboard corners in the camera image, you can find their corresponding projector coordinates.\nNow that you have correspondences between known 3D world points (the checkerboard corners) and 2D projector image points, you can calibrate the projector using the same technique as for a camera.\n\n\nExplain the term Triangulation in the domain of structured light scanning!\n\n\nstereo trigonometry\n\n\nWhat difficulty does one face with a non-linear camera-projector model for triangulation when working with stripe patterns? How can the problem be circumvented?\n\n\nprojector has distortion -&gt; also inversely distort projector pattern\n\n\nExplain advantages of a 2-camera structured light scanning setup! Adding additional camera to a standard camera-projector setup provides several advantages:\n\n\nReduces highlight problems: Specular reflections that might blind one camera can be correctly captured by the other.\nIncreases surface visibility: It helps resolve occlusions, as a surface part hidden from one camera may be visible to the other.\nIncreases precision: It provides multiple measurements of the same surface point, leading to a more robust and accurate reconstruction.\n\n\nDiscuss the scanning of dynamic 3D scenes with structured light approaches!\n\n\nHigh-speed systems: Use a synchronized high-speed camera and projector to capture the entire pattern sequence before the object moves significantly.\nDebrujin, GrayCode\n\n\nExplain and compare Line-Shift, Intensity Coding, Gray-Code and phase shift based structured light scanning!\n\n\nLine-Shift: Projects one vertical line at a time =&gt; not good, because only 1 line =&gt; many shots\nIntensity Coding: 2 images: x = L2/L1\nGray-Code: n images, see how a point change after n images (dark/white)\nPhase Shift: 3 images, see how a point change after 3 images (using continuos function )\n\n\nWhy is the Gray-Code better suited for structured light 3D scanning?\n\n\navoid color bleeding (small scattered cells)\n\n\nHow can one use de Bruijn Sequences to build a Single-Shot 3D Scanner?\n\n\nidk\n\n\nExplain the basic idea for the separation of direct from indirect illumination! How can one make decoding of bit codes more robust with direct-indirect light separation?\n\n\nIndirect: low frequency\nDirect_ high frequency\n\n\n\n3D Processing\n\nExplain what the Riemannian-Graph is and how it can be used to filter outliers!\n\n\nRiemannian-Graph: every point has outgoing edges connecting it to its k nearest neighbors.\nOutlier Filtering: This graph structure can be used to detect outliers. has outgoing but not incoming =&gt; no friends haha\nExtra (symetric Riemanian Graph, has something to do with Voronoi area, and the idea is only connect pointbetween the (similar) parallel voronoi poles)\n\n\nDescribe a method to estimate the local sampling density of a point cloud!\n\n\narea among 3-6 nearest neighbors\n\n\nExplain how to fit a least squares plane to a set of points! (also repeat details on that from CG1)\n\n\nGoal find the plane \\(p=(n_x, n_y, n_z,d)\\) that minimizes weighted sum displacement \\(f = p^TA^TWAp\\)\nWeight: Gaussian from center\nWeighted Covariance Matrix\nFind smallest eigenvalue\nsimilar to PCA but embed the tangental normal in =&gt; from max to min\n\n\nIs the least squares normal unique? No, can be n* or -n* =&gt; need orientation step.\nMotivate problems of the least squares fit in the vicinity of outliers, C‚Å∞ and C¬π discontinuities!\n\n\nThe standard L‚ÇÇ-norm (least squares) is highly sensitive discontiuinity C0 C2 + outliers\n=&gt; for example L1-norm is less sensitive\n\n\n\nThere is also the optimal weighting problem, but I dont remember\n\nHow to extend the weighted least squares normal fit to avoid smoothing out of normals in the vicinity of sharp corners and creases?\n\n\nremember bilateral filtering? this one similar\n\\(w = w_1 w_{bilateral}\\) where w1 is normal weight, w2 reduce the effect when tangent normal is different\nBUT to do this, we do not know the tangent normal, so just start with \\(w_{bilateral}=1\\) first, then start using that to keep fitting\n\n\nHow to compute a Minimum / Maximum Spanning Tree efficiently?\n\n\nGoal a tree that go through all vertices but just through the edges with minimal costs\n\n\nExplain how to compute a consistent orientation of surface normals with the help of a Minimum Spanning Tree and explain variants of defining the edge costs?\n\n\nStart with one or more normals that are known to be correctly oriented (e.g., from scanner position).\nConstruct a neighbor graph (e.g., a Riemannian graph)\nAssign a cost (low for edges at smooth surface - high creases edges)\nPropagate through the MST. For each edge traversed, orient the neighbor‚Äôs normal to be consistent with the current normal.\nflip criteria\n\nHoppe, Xie: if dot product \\(n_1 n_2 &lt; 0\\) then flip\nK√∂nig: idea is simpler curve with less curvature\n\n\n\nExplain the ICP-algorithm!\n\n\nStart with BIGGEST ASSUMPTION: oready initially coarse aligned. =&gt; alternates between 2 steps until convergence:\nFind closest Correspondences (like real physical closest - or normal matching - or even FPFH)\nTransform using minimal homography (similar to covariance matrix)\nSVD transformation for translation|rotation\nNachteil: it really needs initially coarsely aligned, for example if 2 scans flipped, then it does not work\n\n\nWhat is normal-space sampling and how can it help to improve the ICP-algorithm?\n\n\nGoal balance the set of normal vectors distributed from sampling =&gt; less biased to only flat plane fitting\n\n\nWhat possibilities do you know to define the distance measure used for the objective function of the registration optimization problem?\n\n\n2 main error metrics (distance measures) used in ICP:\nPoint-to-Point Distance: simply minimize squared Euclidean distance between corresponding points\nPoint-to-Plane Distance: minimizes the squared distance from a source point to the tangent plane at its corresponding target point (using dot product to project this chan duong cao)\n\n\nWhat variants do you know to extract corresponding point pairs for the ICP-algorithm?\n\n\nSeveral matching strategies:\nClosest-Point: standard, but when they flip not good\nNormal Shooting: Project a point along its normal to find an intersection with the other surface\nProjection: Project a point from the source onto the target mesh from the camera‚Äôs point of view (tbh honest i really do not know how this works)\n\n\nExplain and compare the surface reconstruction techniques silhouette-, space- and volume carving!\n\n\nSilhouette Carving using only 2D RGB: create a 3D voxel space, project that 3D on binary silhouettte from every perspective image, start carving\nSpace Carving using only 2D RGB: initialized silhouette carving + photo consistency over all images of that same voxel\nVolume Carving: space carving in RGBD. depth provides a much stronger initial constraint, nothing else to say, this is good!!!\n\n\nGiven images of reconstructions, find out, which result was produced by silhouette-, space- or volume carving!\n\n\nSilhouette Carving: results in convex looking\nSpace Carving: actually convex but color constraint can reduce something, but not good enough\nVolume Carving: most accurate\n\n\nExplain how to use implicit functions for surface reconstruction! Why are constraints at the sample points not sufficient?\n\n\nGoal Poisson uses 2 constraints:\n(‚àáœá ‚âà V) (V are the surface normals)\n(œá(p) = 0) constraints at surface (screened poisson)\nHow to do this? we derivate one more to get the Laplace/Poisson equation where we seem to be able to to solve bc we know the divergence inside/outside (divœá = ‚àáV)\nthen slowly make finer\n\n\n\nRotation & Articulated Objects\n\nExplain 3 representations for rotations and discuss their suitability for interpolation between rotations!\n\n\n3x3 Rotation Matrices: Not suitable for direct interpolation. Linearly interpolating between two rotation matrices and re-normalizing does not produce a constant-velocity rotation and can result in unwanted scaling/shearing artifacts.\nEuler Angles: Simple to interpolate the three angles linearly, but this is highly problematic. It does not produce the ‚Äúshortest path‚Äù rotation and is susceptible to gimbal lock\nQuaternions:Excellent for interpolation.\n\n\nDiscuss uniqueness of the different representations for rotations!\n\n\nRotation Matrices: 1 rotation 1 matrix eindeutig\nEuler Angles: (extrinsic & intrinsic)\nAxis-Angle / Quaternions: rotation by angle Œ± around axis n is identical to a rotation by angle -Œ± around axis -n.¬†(q and -q) represent the exact same 3D rotation. (similar to complex number)\n\n\nWhat is a quaternion and how can it be used to rotate a vector around an axis?\n\n\nis a 4-dimensional complex numbers, with one real part (s) and three imaginary parts (x, y, z). \\((i¬≤=j¬≤=k¬≤=ijk=-1) \\And (ijk)\\)\nTo rotate p around \\(n=ai+bj+ck\\) for \\(\\alpha\\) degree (where \\(a+b+c=1\\) normalized):\nRepresent p as a ‚Äúpure‚Äù quaternion with a zero real part: \\(p_{quat} = (0, p)\\). The rotation is performed using the quaternion multiplication formula: \\(p'_{quat} = (\\cos(\\frac{\\alpha}{2}) + sin(\\frac{\\alpha}{2})(n)) * p_{quat} * (\\cos(\\frac{\\alpha}{2}) - sin(\\frac{\\alpha}{2})(n))\\)\nBasically only rotate 3 dimension ijk out of 4 dimensions, therefore the inverse\n\n\nWhich additional transformation can be expressed by the length of a quaternion?\n\n\nunit quaternion (length = 1) -&gt; pure rotation\nnon-unit quaternion -&gt; rotation + uniform scaling\n\n\nGiven an image of a robot arm, explain the terms basis, node, joint and end effector!\n\n\nwe all know what those are\n\n\nGiven images of joint types, classify joint type and enumerate degrees of freedom!\n\n\nRevolute (1R): 1 DOF (1 rotation)\nPrismatic (1T): 1 DOF (1 translation)\nUniversal (2R):2 DOF (2 revolute together)\nSpherical (3R): 3 DOF.\nGimbal (3R): 3 DOF with Gimbal lock\n\n\nWhat is the difference between a kinematic chain and a kinematic tree?\n\n\nKinematic Chain: transofrmation base-&gt; effector\nKinematic Tree: bone hierarchy\n\n\nDiscuss the difference between representing joint transformation as a sequence of rotation and translation compared to a sequence of translation and rotation!\n\n\nofcourse not the same =&gt; ( Denavit-Hartenberg) makes it deterministic\n\n\nExplain linearization of a kinematic tree for the efficient computation of world to joint transformations!\n\n\nits a linear transformation: T(n+1) use Tn as base\njust calculate forward\n\n\nHow many parameters are used in the Denavit-Hartenberg Convention to represent bone transformations in a kinematic chain?\n\n\n4 param, compared to Euler (2+3=6), Quaternion (7)\nd_i distance along z_i to meet common perpendicular length\nŒ±_i angular offset of the 2 joint axes z_i & z_i-1\nŒ∏_i actual angular change of x_i and x_i-1 on z-1 axe\na_i common perpendicular length\nTo sum up, DH = Rot(Œ∏_i) * Trans(d_i) * Trans(a_i) * Rot(Œ±_i), where the actual rotation is Rot(Œ∏_i) & Trans(d_i)\nEVENTUALLY then it can all be presented in a matrix in lecture but I do not remember, you can show me\n\n\nWhich joint types can be represented directly in the Denavit-Hartenberg Convention?\n\n\nRevolute -&gt; joint angle Œ∏_i is variable.\nPrismatic -&gt; link offset d_i is variable.\n\n\nHow can other joint types be emulated in the Denavit-Hartenberg Convention?\n\n\ncombine them together with zero link offset\n\n\nHow can one compute the angles in the Denavite-Hartenberg Convention such that a full revolution of 360 degrees can be supported?\n\n\nTo support a full 360-degree =&gt; use arctan2(sine, cosine) =&gt; output (angle, quadrant).\nFor an angle like the link twist Œ±_i, you compute both:\nsin(Œ±_i) = ||zÃÇ_{i-1} √ó zÃÇ_i||\ncos(Œ±_i) = &lt;zÃÇ_{i-1}, zÃÇ_i&gt;\nHowever, the sine term is always positive, restricting the angle to [0, œÄ]. SO that does not actually eliminate the result we want anyway =&gt; sign of the cross product of angle by taking a dot product dot producting with a third, orthogonal vector ( xÃÇ_i).\nThe final formula becomes: Œ±_i = sgn(&lt;zÃÇ_{i-1} √ó zÃÇ_i, xÃÇi&gt;) * arctan2(||zÃÇ{i-1} √ó zÃÇi||, &lt;zÃÇ{i-1}, zÃÇ_i&gt;)\n\n\n\nSkeleton Extraction\n\nExplain the term medial axis and be able to draw it into a 2D shape!\n\n\nset of all interior points that have at least two closest points on the shape‚Äôs boundary.\nvoxelize -&gt; create Voronoi vertices -&gt; connect Voronois with each other\n\n\nWhy is the medial axis not suitable as a curve skeleton of a 2D or a 3D shape?\n\n\nSensitivity to Noise: It is extremely sensitive to small perturbations or noise on the boundary. A tiny bump on the surface can create a new, often large, branch in the medial axis, making it unstable and overly complex.\nDimensionality in 3D: For a 3D shape, the medial axis is not a 1D curve but a 2D surface, which is not skeleton\n\n\nName important properties of a curve skeleton!\n\n\n1D curve structure, even for 3D shapes.\nsame topology as the shape (e.g., the same number of loops for holes/tunnels).\nlie in the center of the object‚Äôs volume.\ninvariant to isometric transformations (like bending) of the shape.\nEvery point on the shape‚Äôs surface should be ‚Äúvisible‚Äù from at least one point on the skeleton.\ninsensitive to small amounts of noise\n\n\nExplain some techniques to compute a curve skeleton!\n\n\nErosion until 1D\nDistance Field: skeleton is maxima of this field.\nVoronoi: we all know\nCompeting Front (growing method and at each timestamp keep the center)\n\n\nExplain the competing front approach of Sharf et al.¬†and name some advantages!\n\n\nmethod for both surface reconstruction and skeleton extraction.\nPrinciple:\nAn initial deformable mesh (a ‚Äúfront‚Äù) is placed inside the point cloud.\nThis front is inflated outwards, driven by a distance field, to reconstruct the shape.\nAs the front expands, it may need to split to pass through narrow channels or merge when different parts of the front meet.\nThe skeleton is traced as the center of these evolving fronts. Junctions in the skeleton are formed where fronts split or merge.\nAdvantages: robust, shape good, topology\n\n\n\nRigging\n\nExplain the term rigging in the domain of character animation!\n\n\nin animation:\nRigging: building an internal skeleton and fitting it inside the 3D mesh.\nSkinning: binding the mesh‚Äôs vertices to the bones of the skeleton.\n\n\nWhat are input and output to the Pinocchio automated rigging approach! Input: 3D mesh + skeleton Output: embedded skeleton\nGive an overview of how the Skeleton is positioned inside the input polygonal mesh!\n\n\nDiscretization: The system first computes a medial axis graph.\nEmbedding initialization: simplified version of the skeleton is optimally placed within the graph using an A* search algorithm guided by a penalty function.\nEmbedding Refinement: The full skeleton is then placed based on the coarse result, and its position is fine-tuned using a continuous gradient-descent optimization on a simplified penalty function.\n\n\nName some terms of the energy function used to optimize the skeleton and explain how the authors adapted the energy function to a set of good and bad rigging examples!\n\n\nCenter Distance: Penalizes bones for being far from the medial axis.\nShort Bones / Wrong Directions: Penalizes the skeleton for having bone lengths or orientations that differ significantly from the input skeleton‚Äôs proportions.\nFeet: A specific term to ensure the feet are placed at the bottom of the model.\nLimb Sharing: Penalizes cases where multiple limbs are embedded into the same part of the mesh.\nsupervised learning from bad & good examples\n\n\nExplain the idea of the RigMesh-Approach\n\n\nintegrate rigging into a sketch-based modeling workflow. Traditionally, rigging is a separate, final step performed after modeling is complete. If the model is changed, the rig must be redone. RigMesh makes the rigging process incremental. As a user draws and edits the shape of a character using sketches, the system automatically and continuously updates the internal skeleton and skinning weights =&gt; faster change\n\n\nGive some sources for mesh animations! A mesh animation (also called a vertex cache or point cache animation) is a sequence of meshes where the vertex positions change over time. Sources for this data include:\n\n\nPhysics simulations (e.g., cloth or soft bodies).\nMotion capture data applied to a mesh.\nKeyframe animation created by an artist.\n\n\nHow can one approximate a mesh animation with a skinned mesh representation?\n\n\nThe goal is to find a set of bones, bone transformations for each frame, and a set of vertex weights that, when applied to a single base mesh, best reproduces the entire input animation sequence. This is a decomposition problem. It seeks to compress the large amount of data from the mesh animation into a much more compact and controllable skinned mesh representation.\n\n\nExplain the principle steps of the ‚ÄúSkinning Mesh Animations‚Äù (SMA) approach! The SMA approach automatically extracts a skinned mesh from a given mesh animation. The main steps are:\n\n\nclustering triangles with similar transformation along the frames\ntransfer that transformation to Bone: For each frame and each identified bone (cluster), a average rigid transformation is calculated to assign to that bone.\n\n\nWhat are the feature vectors used for clustering the mesh triangles into bone clusters?\n\n\nhomogeneous transoformation matrix 3x3 or a 9D parameter vector\n\n\nHow are the vertex weights computed in the SMA approach?\n\n\nsolving a constrained least squares problem.\nBone Influence Selection: First, for each vertex, the b most influential bones are identified (where b is a small user-defined number). This is done greedily by finding the set of b bones that minimizes the animation reconstruction error for that vertex.\nLeast Squares Fit: With the influencing bones known, a linear least squares problem is solved to find the optimal weights. This fit is often constrained (e.g., using non-negative least squares) to avoid overfitting and ensure plausible weights.\n\n\nWhere is room for improvement in the SMA approach that was used in ‚ÄûFast&efficient skinning of animated meshes‚Äú?\n\n\nThe ‚ÄúFast & Efficient Skinning of Animated Meshes‚Äù (F&ESAM) paper improves upon the general idea by addressing its computational complexity. The main improvement is solving the problem in a reduced trajectory space.\nInstead of working with the trajectories of all m vertices over k frames (a very large matrix A), the method first approximates the entire animation using a much smaller set of d basis trajectories (where d &lt;&lt; m). The matrix decomposition problem is then solved in this much smaller, lower-dimensional space. This yields a slightly larger number of bones (d) than a full SVD would, but it can be solved in a significantly shorter amount of time."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html",
    "href": "posts/ml4r-bayesian-optimization/index.html",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "\\(P(f | \\mathcal{D}) \\propto P(\\mathcal{D} | f) P(f)\\) is a real multiplication of probability densities. However, when dealing with functions and multivariate Gaussians, this multiplication is implicitly handled by the properties of joint and conditional Gaussian distributions.\n\n\nThe key to the GP‚Äôs tractability is that while it‚Äôs a distribution over infinite-dimensional functions, any finite collection of function values drawn from a GP jointly follow a multivariate Gaussian distribution.\nConsider our observed input points \\(\\mathbf{X}_t = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_t\\}\\) and a new test input point \\(\\mathbf{x}^*\\). Under the GP prior, the vector of true function values at these points, \\(\\mathbf{f}_t = [f(\\mathbf{x}_1), \\dots, f(\\mathbf{x}_t)]^T\\) and unobserved \\(f^* = f(\\mathbf{x}^*)\\), has a joint multivariate Gaussian distribution:\n\\[\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}, \\begin{pmatrix} \\mathbf{K}_t & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix} \\right)\\]\nHere:\n\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) is the prior mean at observed points.\n\\(\\mathbf{K}_t\\) is the \\(t \\times t\\) covariance matrix of the observed points, where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_* = [k(\\mathbf{x}^*, \\mathbf{x}_1), \\dots, k(\\mathbf{x}^*, \\mathbf{x}_t)]^T\\) is the \\(t \\times 1\\) vector of covariances between the test point and observed points.\n\\(k(\\mathbf{x}^*, \\mathbf{x}^*)\\) is the prior variance at the test point itself.\n\nThis joint prior distribution for \\(\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix}\\) represents the \\(P(f)\\) term in Bayes‚Äô Theorem. We do not know what that exactly is, but it does tell us about the plausible relationships between function values at observed and unobserved locations before we see any data.\n\n\n\nWe observe the data \\(\\mathcal{D}_t = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^t\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\). This can be written in vector form as:\n\\[\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\]\nwhere \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 \\mathbf{I})\\).\nThe likelihood \\(P(\\mathcal{D}_t | f)\\) (or more precisely, \\(P(\\mathbf{y}_t | \\mathbf{f}_t)\\)) is a Gaussian centered at the true function values \\(\\mathbf{f}_t\\):\n\\[P(\\mathbf{y}_t | \\mathbf{f}_t) = \\mathcal{N}(\\mathbf{y}_t | \\mathbf{f}_t, \\sigma_n^2 \\mathbf{I})\\]\nThis explicitly defines the \\(P(\\mathcal{D}|f)\\) term (using \\(\\mathbf{f}_t\\) as the ‚Äúspecific function‚Äù part for the observed data).\n\n\n\nNow, the ‚Äúmultiplication‚Äù that yields the posterior GP is achieved by a fundamental property of multivariate Gaussian distributions:\nIf you have two random variables (or vectors of variables) \\(A\\) and \\(B\\) that are jointly Gaussian (note that \\(\\Sigma = Cov\\)):\n\\[\\begin{pmatrix} A \\\\ B \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_A \\\\ \\boldsymbol{\\mu}_B \\end{pmatrix}, \\begin{pmatrix} \\boldsymbol{\\Sigma}_{AA} & \\boldsymbol{\\Sigma}_{AB} \\\\ \\boldsymbol{\\Sigma}_{BA} & \\boldsymbol{\\Sigma}_{BB} \\end{pmatrix} \\right)\\]\nThen, the conditional distribution of \\(B\\) given \\(A\\) (i.e., \\(P(B|A)\\)) is also Gaussian with:\n\nConditional Mean: \\(\\boldsymbol{\\mu}_{B|A} = \\boldsymbol{\\mu}_B + \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} (\\mathbf{A} - \\boldsymbol{\\mu}_A)\\)\nConditional Covariance: \\(\\boldsymbol{\\Sigma}_{B|A} = \\boldsymbol{\\Sigma}_{BB} - \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} \\boldsymbol{\\Sigma}_{AB}\\)\n\nHow this maps to GPs:\n\nForm the Joint Distribution of \\((\\mathbf{y}_t, f^*)\\) under the Prior: We need the joint prior distribution of our observed outputs \\(\\mathbf{y}_t\\) and our unobserved test point function value \\(f^*\\). We know \\(\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\). Since \\(\\mathbf{f}_t\\) and \\(f^*\\) are jointly Gaussian (from the GP prior) and \\(\\boldsymbol{\\epsilon}\\) is Gaussian (noise), their sum is also jointly Gaussian. The joint prior mean of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is \\(\\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}\\). The joint prior covariance of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is:\n\\[\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\right) = \\begin{pmatrix} \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I} & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix}\\]\n(Here, the \\(\\sigma_n^2 \\mathbf{I}\\) term comes from the noise added to \\(\\mathbf{f}_t\\) in the \\(\\mathbf{y}_t\\) block).\nConditioning (Applying Bayes‚Äô Rule): Now, we have observed \\(\\mathbf{y}_t\\). We want to find the posterior distribution of \\(f^*\\) given \\(\\mathbf{y}_t\\), which is \\(P(f^* | \\mathbf{y}_t)\\). Using the conditional Gaussian formulas, let:\n\n\\(A = \\mathbf{y}_t\\) (our observed data)\n\\(B = f^*\\) (the function value we want to predict)\n\\(\\boldsymbol{\\mu}_A = \\mathbf{m}_t\\)\n\\(\\boldsymbol{\\mu}_B = m(\\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AA} = \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\)\n\\(\\boldsymbol{\\Sigma}_{BB} = k(\\mathbf{x}^*, \\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AB} = \\mathbf{k}_*\\)\n\\(\\boldsymbol{\\Sigma}_{BA} = \\mathbf{k}_*^T\\)\n\nPlugging these into the conditional mean and covariance formulas gives exactly the GP posterior predictive mean and variance:\nPosterior Mean \\(\\mu_t(\\mathbf{x}^*)\\):\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\]\nPosterior Variance \\(\\sigma_t^2(\\mathbf{x}^*)\\):\n\\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\n\nIn essence:\nThe ‚Äúmultiplication‚Äù \\(P(\\mathcal{D} | f) P(f)\\) is handled internally by the mathematical properties of Gaussian distributions. The GP framework sets up a joint Gaussian prior over all relevant function values (observed and unobserved). The likelihood then specifies how our observed data \\(\\mathbf{y}_t\\) relates to the true function values \\(\\mathbf{f}_t\\). By conditioning this joint prior on the observed data \\(\\mathbf{y}_t\\), we directly derive the exact posterior distribution for the unobserved function values \\(f^*\\), which turns out to also be Gaussian with the mean and variance formulas."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#the-gp-prior-pf-joint-gaussian-over-all-relevant-points",
    "href": "posts/ml4r-bayesian-optimization/index.html#the-gp-prior-pf-joint-gaussian-over-all-relevant-points",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "The key to the GP‚Äôs tractability is that while it‚Äôs a distribution over infinite-dimensional functions, any finite collection of function values drawn from a GP jointly follow a multivariate Gaussian distribution.\nConsider our observed input points \\(\\mathbf{X}_t = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_t\\}\\) and a new test input point \\(\\mathbf{x}^*\\). Under the GP prior, the vector of true function values at these points, \\(\\mathbf{f}_t = [f(\\mathbf{x}_1), \\dots, f(\\mathbf{x}_t)]^T\\) and unobserved \\(f^* = f(\\mathbf{x}^*)\\), has a joint multivariate Gaussian distribution:\n\\[\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}, \\begin{pmatrix} \\mathbf{K}_t & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix} \\right)\\]\nHere:\n\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) is the prior mean at observed points.\n\\(\\mathbf{K}_t\\) is the \\(t \\times t\\) covariance matrix of the observed points, where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_* = [k(\\mathbf{x}^*, \\mathbf{x}_1), \\dots, k(\\mathbf{x}^*, \\mathbf{x}_t)]^T\\) is the \\(t \\times 1\\) vector of covariances between the test point and observed points.\n\\(k(\\mathbf{x}^*, \\mathbf{x}^*)\\) is the prior variance at the test point itself.\n\nThis joint prior distribution for \\(\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix}\\) represents the \\(P(f)\\) term in Bayes‚Äô Theorem. We do not know what that exactly is, but it does tell us about the plausible relationships between function values at observed and unobserved locations before we see any data."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#the-likelihood-pmathcald-f-adding-gaussian-noise",
    "href": "posts/ml4r-bayesian-optimization/index.html#the-likelihood-pmathcald-f-adding-gaussian-noise",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "We observe the data \\(\\mathcal{D}_t = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^t\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\). This can be written in vector form as:\n\\[\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\]\nwhere \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 \\mathbf{I})\\).\nThe likelihood \\(P(\\mathcal{D}_t | f)\\) (or more precisely, \\(P(\\mathbf{y}_t | \\mathbf{f}_t)\\)) is a Gaussian centered at the true function values \\(\\mathbf{f}_t\\):\n\\[P(\\mathbf{y}_t | \\mathbf{f}_t) = \\mathcal{N}(\\mathbf{y}_t | \\mathbf{f}_t, \\sigma_n^2 \\mathbf{I})\\]\nThis explicitly defines the \\(P(\\mathcal{D}|f)\\) term (using \\(\\mathbf{f}_t\\) as the ‚Äúspecific function‚Äù part for the observed data)."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#the-multiplication-and-conditioning-to-get-the-posterior-pf-mathcald",
    "href": "posts/ml4r-bayesian-optimization/index.html#the-multiplication-and-conditioning-to-get-the-posterior-pf-mathcald",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "Now, the ‚Äúmultiplication‚Äù that yields the posterior GP is achieved by a fundamental property of multivariate Gaussian distributions:\nIf you have two random variables (or vectors of variables) \\(A\\) and \\(B\\) that are jointly Gaussian (note that \\(\\Sigma = Cov\\)):\n\\[\\begin{pmatrix} A \\\\ B \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_A \\\\ \\boldsymbol{\\mu}_B \\end{pmatrix}, \\begin{pmatrix} \\boldsymbol{\\Sigma}_{AA} & \\boldsymbol{\\Sigma}_{AB} \\\\ \\boldsymbol{\\Sigma}_{BA} & \\boldsymbol{\\Sigma}_{BB} \\end{pmatrix} \\right)\\]\nThen, the conditional distribution of \\(B\\) given \\(A\\) (i.e., \\(P(B|A)\\)) is also Gaussian with:\n\nConditional Mean: \\(\\boldsymbol{\\mu}_{B|A} = \\boldsymbol{\\mu}_B + \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} (\\mathbf{A} - \\boldsymbol{\\mu}_A)\\)\nConditional Covariance: \\(\\boldsymbol{\\Sigma}_{B|A} = \\boldsymbol{\\Sigma}_{BB} - \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} \\boldsymbol{\\Sigma}_{AB}\\)\n\nHow this maps to GPs:\n\nForm the Joint Distribution of \\((\\mathbf{y}_t, f^*)\\) under the Prior: We need the joint prior distribution of our observed outputs \\(\\mathbf{y}_t\\) and our unobserved test point function value \\(f^*\\). We know \\(\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\). Since \\(\\mathbf{f}_t\\) and \\(f^*\\) are jointly Gaussian (from the GP prior) and \\(\\boldsymbol{\\epsilon}\\) is Gaussian (noise), their sum is also jointly Gaussian. The joint prior mean of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is \\(\\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}\\). The joint prior covariance of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is:\n\\[\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\right) = \\begin{pmatrix} \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I} & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix}\\]\n(Here, the \\(\\sigma_n^2 \\mathbf{I}\\) term comes from the noise added to \\(\\mathbf{f}_t\\) in the \\(\\mathbf{y}_t\\) block).\nConditioning (Applying Bayes‚Äô Rule): Now, we have observed \\(\\mathbf{y}_t\\). We want to find the posterior distribution of \\(f^*\\) given \\(\\mathbf{y}_t\\), which is \\(P(f^* | \\mathbf{y}_t)\\). Using the conditional Gaussian formulas, let:\n\n\\(A = \\mathbf{y}_t\\) (our observed data)\n\\(B = f^*\\) (the function value we want to predict)\n\\(\\boldsymbol{\\mu}_A = \\mathbf{m}_t\\)\n\\(\\boldsymbol{\\mu}_B = m(\\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AA} = \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\)\n\\(\\boldsymbol{\\Sigma}_{BB} = k(\\mathbf{x}^*, \\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AB} = \\mathbf{k}_*\\)\n\\(\\boldsymbol{\\Sigma}_{BA} = \\mathbf{k}_*^T\\)\n\nPlugging these into the conditional mean and covariance formulas gives exactly the GP posterior predictive mean and variance:\nPosterior Mean \\(\\mu_t(\\mathbf{x}^*)\\):\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\]\nPosterior Variance \\(\\sigma_t^2(\\mathbf{x}^*)\\):\n\\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\n\nIn essence:\nThe ‚Äúmultiplication‚Äù \\(P(\\mathcal{D} | f) P(f)\\) is handled internally by the mathematical properties of Gaussian distributions. The GP framework sets up a joint Gaussian prior over all relevant function values (observed and unobserved). The likelihood then specifies how our observed data \\(\\mathbf{y}_t\\) relates to the true function values \\(\\mathbf{f}_t\\). By conditioning this joint prior on the observed data \\(\\mathbf{y}_t\\), we directly derive the exact posterior distribution for the unobserved function values \\(f^*\\), which turns out to also be Gaussian with the mean and variance formulas."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#the-gaussian-process-gp-as-the-surrogate-model",
    "href": "posts/ml4r-bayesian-optimization/index.html#the-gaussian-process-gp-as-the-surrogate-model",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "1. The Gaussian Process (GP) as the Surrogate Model",
    "text": "1. The Gaussian Process (GP) as the Surrogate Model\nAs established, the Gaussian Process models our unknown objective function \\(f(\\mathbf{x})\\) as a probability distribution over functions:\n\\[f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))\\]\n\n\\(m(\\mathbf{x})\\): Mean function (often assumed to be \\(m(\\mathbf{x})=0\\) or the mean of observed data for simplicity).\n\\(k(\\mathbf{x}, \\mathbf{x}')\\): Kernel (covariance) function, defining similarity between function values at different points. A common choice is the Squared Exponential (RBF) kernel:\n\\[k(\\mathbf{x}, \\mathbf{x}') = \\sigma_f^2 \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}'\\|^2}{2l^2}\\right)\\]\nwhere \\(\\sigma_f^2\\) is the signal variance (amplitude) and \\(l\\) is the length-scale.\n\nGiven a set of \\(t\\) observed data points \\(\\mathcal{D}_t = \\{(\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_t, y_t)\\}\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) (with additive Gaussian noise \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\)), the posterior predictive distribution for a new point \\(\\mathbf{x}^*\\) is Gaussian:\n\\[f(\\mathbf{x}^*) | \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\mathbf{x}^*), \\sigma_t^2(\\mathbf{x}^*))\\]\nThe predictive mean \\(\\mu_t(\\mathbf{x}^*)\\) and variance \\(\\sigma_t^2(\\mathbf{x}^*)\\) are given by:\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\] \\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\nWhere:\n\n\\(\\mathbf{y}_t = [y_1, \\dots, y_t]^T\\) (vector of observed values)\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) (mean function evaluated at observed points)\n\\(\\mathbf{K}_t\\): \\(t \\times t\\) covariance matrix where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_*\\): \\(t \\times 1\\) vector where \\([\\mathbf{k}_*]_i = k(\\mathbf{x}^*, \\mathbf{x}_i)\\).\n\\(\\mathbf{I}\\): Identity matrix.\n\\(\\sigma_n^2\\): Noise variance."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#bayesian-optimization-iteration-using-an-acquisition-function",
    "href": "posts/ml4r-bayesian-optimization/index.html#bayesian-optimization-iteration-using-an-acquisition-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "2. Bayesian Optimization Iteration using an Acquisition Function",
    "text": "2. Bayesian Optimization Iteration using an Acquisition Function\nThe goal of Bayesian Optimization is to find \\(\\mathbf{x}^* = \\arg\\max_{\\mathbf{x} \\in \\mathcal{X}} f(\\mathbf{x})\\), where \\(\\mathcal{X}\\) is the search domain.\nThe iterative process involves:\n\nUpdate GP: Use the current dataset \\(\\mathcal{D}_t\\) to compute the posterior mean \\(\\mu_t(\\mathbf{x})\\) and variance \\(\\sigma_t^2(\\mathbf{x})\\) for the entire search space \\(\\mathcal{X}\\).\nMaximize Acquisition Function: Select the next point \\(\\mathbf{x}_{next}\\) by maximizing an acquisition function \\(a(\\mathbf{x})\\), which intelligently balances exploration (sampling in uncertain regions) and exploitation (sampling in promising regions). We‚Äôll use Expected Improvement (EI) for our example:\n\\[\\text{EI}(\\mathbf{x}) = \\mathbb{E}[\\max(0, f(\\mathbf{x}) - y_{\\text{max}}^*)]\\]\nWhere \\(y_{\\text{max}}^* = \\max_{i=1 \\dots t} y_i\\) is the current best observed value.\nThe analytical form of EI (assuming \\(\\sigma_t(\\mathbf{x}) &gt; 0\\)) is:\n\\[\\text{EI}(\\mathbf{x}) = \\sigma_t(\\mathbf{x}) \\left[\\phi(Z) + Z\\Phi(Z)\\right]\\]\nwhere \\(Z = \\frac{\\mu_t(\\mathbf{x}) - y_{\\text{max}}^*}{\\sigma_t(\\mathbf{x})}\\). If \\(\\sigma_t(\\mathbf{x}) = 0\\), then \\(\\text{EI}(\\mathbf{x}) = 0\\). \\(\\phi(\\cdot)\\) is the PDF and \\(\\Phi(\\cdot)\\) is the CDF of the standard normal distribution.\nEvaluate True Function: Obtain \\(y_{next} = f(\\mathbf{x}_{next})\\).\nAdd to Data: \\(\\mathcal{D}_{t+1} = \\mathcal{D}_t \\cup \\{(\\mathbf{x}_{next}, y_{next})\\}\\)."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#numerical-example-optimizing-a-simple-1d-function",
    "href": "posts/ml4r-bayesian-optimization/index.html#numerical-example-optimizing-a-simple-1d-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "3. Numerical Example: Optimizing a Simple 1D Function",
    "text": "3. Numerical Example: Optimizing a Simple 1D Function\nLet‚Äôs use a very simple 1D objective function \\(f(x) = -(x-2)^2 + 4\\) over the domain \\(x \\in [0,4]\\). The true maximum is at \\(x=2\\) with \\(f(2)=4\\).\nGP Hyperparameters (Fixed for Simplicity):\n\nMean function \\(m(x)=0\\).\nSquared Exponential Kernel: \\(\\sigma_f^2=1.0\\), \\(l=1.0\\). So, \\(k(x,x') = 1.0 \\cdot \\exp\\left(-\\frac{(x-x')^2}{2 \\cdot 1.0^2}\\right) = \\exp\\left(-\\frac{(x-x')^2}{2}\\right)\\).\nNoise variance \\(\\sigma_n^2=0.01\\).\n\nInitial Data Points (\\(\\mathcal{D}_2\\)): Let‚Äôs say we randomly selected two points and evaluated the true function (with no noise for simplicity in the example, so \\(\\epsilon_i=0\\)):\n\n\\(x_1=1.0 \\Rightarrow y_1 = -(1.0-2)^2 + 4 = -(-1)^2 + 4 = 3.0\\)\n\\(x_2=3.0 \\Rightarrow y_2 = -(3.0-2)^2 + 4 = -(1)^2 + 4 = 3.0\\)\n\nSo, our initial dataset is \\(\\mathcal{D}_2 = \\{(1.0, 3.0), (3.0, 3.0)\\}\\). Current best observed value: \\(y_{\\text{max}}^* = 3.0\\).\n\nIteration 1: Find the Next Point to Evaluate\nStep 1: Compute \\(\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\) and its inverse\nFirst, calculate the kernel matrix \\(\\mathbf{K}_2\\) for \\(x_1=1.0\\) and \\(x_2=3.0\\):\n\n\\(k(x_1,x_1) = \\exp\\left(-\\frac{(1-1)^2}{2}\\right) = \\exp(0) = 1.0\\)\n\\(k(x_1,x_2) = \\exp\\left(-\\frac{(1-3)^2}{2}\\right) = \\exp\\left(-\\frac{(-2)^2}{2}\\right) = \\exp\\left(-\\frac{4}{2}\\right) = \\exp(-2) \\approx 0.1353\\)\n\\(k(x_2,x_1) = k(x_1,x_2) \\approx 0.1353\\)\n\\(k(x_2,x_2) = \\exp\\left(-\\frac{(3-3)^2}{2}\\right) = \\exp(0) = 1.0\\)\n\nSo, \\(\\mathbf{K}_2 = \\begin{bmatrix} 1.0 & 0.1353 \\\\ 0.1353 & 1.0 \\end{bmatrix}\\)\nNow add the noise variance \\(\\sigma_n^2 \\mathbf{I} = 0.01 \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix}\\): \\(\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I} = \\begin{bmatrix} 1.01 & 0.1353 \\\\ 0.1353 & 1.01 \\end{bmatrix}\\)\nCalculate the inverse \\((\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1}\\): Determinant \\(= (1.01 \\times 1.01) - (0.1353 \\times 0.1353) = 1.0201 - 0.0183 \\approx 1.0018\\) Inverse \\(\\approx \\frac{1}{1.0018} \\begin{bmatrix} 1.01 & -0.1353 \\\\ -0.1353 & 1.01 \\end{bmatrix} \\approx \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix}\\)\nStep 2: Calculate \\(\\mu_t(x^*)\\) and \\(\\sigma_t^2(x^*)\\) for candidate points\nLet‚Äôs pick a few candidate points \\(x^*\\) to evaluate our GP at:\n\n\\(x_A^* = 0.5\\)\n\\(x_B^* = 2.0\\) (Near the true optimum, but previously unobserved)\n\\(x_C^* = 3.5\\)\n\nFor each \\(x^*\\), we need \\(\\mathbf{k}_* = [k(x^*,x_1), k(x^*,x_2)]^T\\):\nFor \\(x_A^* = 0.5\\):\n\n\\(k(0.5,1.0) = \\exp\\left(-\\frac{(0.5-1.0)^2}{2}\\right) = \\exp\\left(-\\frac{(-0.5)^2}{2}\\right) = \\exp(-0.125) \\approx 0.8825\\)\n\\(k(0.5,3.0) = \\exp\\left(-\\frac{(0.5-3.0)^2}{2}\\right) = \\exp\\left(-\\frac{(-2.5)^2}{2}\\right) = \\exp(-3.125) \\approx 0.0440\\) So, \\(\\mathbf{k}_* = \\begin{bmatrix} 0.8825 \\\\ 0.0440 \\end{bmatrix}\\)\n\nNow compute \\(\\mu_t(0.5)\\) and \\(\\sigma_t^2(0.5)\\): (\\(\\mathbf{y}_t - \\mathbf{m}_t = \\mathbf{y}_t = [3.0, 3.0]^T\\) since \\(m(x)=0\\))\n\\(\\mu_t(0.5) = \\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_t\\) (since \\(\\mathbf{m}_t=0\\)) \\(\\mu_t(0.5) \\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix} \\begin{bmatrix} 3.0 \\\\ 3.0 \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} (1.0082 \\times 3) + (-0.1351 \\times 3) \\\\ (-0.1351 \\times 3) + (1.0082 \\times 3) \\end{bmatrix} = \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 2.6193 \\\\ 2.6193 \\end{bmatrix}\\) \\(\\approx (0.8825 \\times 2.6193) + (0.0440 \\times 2.6193) \\approx 2.3106 + 0.1151 \\approx \\mathbf{2.4257}\\)\n\\(\\sigma_t^2(0.5) = k(0.5,0.5) - \\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\) \\(k(0.5,0.5) = 1.0\\) \\(\\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_* \\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix} \\begin{bmatrix} 0.8825 \\\\ 0.0440 \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} (1.0082 \\times 0.8825) + (-0.1351 \\times 0.0440) \\\\ (-0.1351 \\times 0.8825) + (1.0082 \\times 0.0440) \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 0.8837 \\\\ -0.0762 \\end{bmatrix} \\approx (0.8825 \\times 0.8837) + (0.0440 \\times -0.0762) \\approx 0.7797 - 0.0033 \\approx 0.7764\\) \\(\\sigma_t^2(0.5) \\approx 1.0 - 0.7764 = \\mathbf{0.2236}\\)\nLet‚Äôs summarize for our candidates (using a more precise calculator for speed for \\(x_B^*\\) and \\(x_C^*\\)):\n\nAt \\(x_A^* = 0.5\\): \\(\\mu_t(0.5) \\approx 2.425\\) \\(\\sigma_t(0.5) = \\sqrt{0.2236} \\approx 0.473\\)\nAt \\(x_B^* = 2.0\\): (This point is exactly in the middle of our two observed points, so we expect high uncertainty as it‚Äôs unobserved but also perhaps a good mean due to interpolation) \\(\\mu_t(2.0) \\approx 3.0\\) \\(\\sigma_t(2.0) \\approx 0.995\\) (High uncertainty because it‚Äôs far from observed data in terms of kernel distance, but interpolated mean is high)\nAt \\(x_C^* = 3.5\\): \\(\\mu_t(3.5) \\approx 2.425\\) \\(\\sigma_t(3.5) \\approx 0.473\\)\n\nStep 3: Calculate Expected Improvement (EI) for candidate points\nCurrent best \\(y_{\\text{max}}^* = 3.0\\). We will use \\(\\xi=0\\) (the default for simple EI, meaning no exploration-exploitation trade-off parameter).\nFor \\(x_A^* = 0.5\\):\n\\(Z = \\frac{\\mu_t(0.5) - y_{\\text{max}}^*}{\\sigma_t(0.5)} = \\frac{2.425 - 3.0}{0.473} = \\frac{-0.575}{0.473} \\approx -1.215\\) \\(\\phi(-1.215) \\approx 0.1804\\) (PDF value)\n\\(\\Phi(-1.215) \\approx 0.1122\\) (CDF value)\n\\(\\text{EI}(0.5) = 0.473 [0.1804 + (-1.215) \\cdot 0.1122] = 0.473 [0.1804 - 0.1364] = 0.473 [0.044] \\approx \\mathbf{0.0208}\\)\nFor \\(x_B^* = 2.0\\):\n\\(Z = \\frac{\\mu_t(2.0) - y_{\\text{max}}^*}{\\sigma_t(2.0)} = \\frac{3.0 - 3.0}{0.995} = 0\\) \\(\\phi(0) \\approx 0.3989\\) \\(\\Phi(0) = 0.5\\) \\(\\text{EI}(2.0) = 0.995 [0.3989 + 0 \\cdot 0.5] = 0.995 [0.3989] \\approx \\mathbf{0.3964}\\)\nFor \\(x_C^* = 3.5\\):\n\\(Z = \\frac{\\mu_t(3.5) - y_{\\text{max}}^*}{\\sigma_t(3.5)} = \\frac{2.425 - 3.0}{0.473} \\approx -1.215\\) \\(\\text{EI}(3.5) \\approx \\mathbf{0.0208}\\) (same as \\(x_A^*\\) due to symmetry in this specific example setup)\nStep 4: Identify \\(\\mathbf{x}_{next}\\)\nComparing the EI values:\n\n\\(\\text{EI}(0.5) \\approx 0.0208\\)\n\\(\\text{EI}(2.0) \\approx 0.3964\\)\n\\(\\text{EI}(3.5) \\approx 0.0208\\)\n\nThe point \\(\\mathbf{x}_{next}=\\mathbf{2.0}\\) has the highest Expected Improvement. This makes sense: it‚Äôs centrally located relative to the observed points, and while its predicted mean is only 3.0 (same as observed), its uncertainty is very high, suggesting a high potential for improvement.\nStep 5: Evaluate the true function at \\(\\mathbf{x}_{next}\\) and update data\nWe evaluate \\(f(2.0) = -(2.0-2)^2 + 4 = 4.0\\). Our updated dataset becomes \\(\\mathcal{D}_3 = \\{(1.0, 3.0), (3.0, 3.0), (2.0, 4.0)\\}\\). The new best observed value \\(y_{\\text{max}}^* = 4.0\\).\n\n\nIteration 2 (Conceptual)\nWith the new point \\((2.0, 4.0)\\), the GP model would be updated. The uncertainty around \\(x=2.0\\) would drastically decrease, as we now know its value precisely (or with very low noise). The acquisition function would then be maximized again. Given that \\(y_{\\text{max}}^*\\) is now 4.0 (the true optimum), the EI will be very low at \\(x=2.0\\). The algorithm would likely explore regions further away from \\(x=2.0\\) to ensure no other maxima exist, or converge as no significant improvement is expected elsewhere."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#formulation",
    "href": "posts/ml4r-bayesian-optimization/index.html#formulation",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Formulation",
    "text": "Formulation\nBayes‚Äô Theorem:\n\\[P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)}\\]\nWhere:\n\n\\(P(\\theta | D)\\): Posterior (Our updated belief in the parameters \\(\\theta\\) after seeing the Data \\(D\\)). This is what we want to find in Bayesian inference.\n\\(P(D | \\theta)\\): Likelihood (The probability of observing the Data \\(D\\) given the parameters \\(\\theta\\)). This is the core term that MLE maximizes.\n\\(P(\\theta)\\): Prior Probability (Our initial belief in the parameters \\(\\theta\\) before seeing any data).\n\\(P(D)\\): Evidence / Marginal Likelihood (The total probability of the data, across all possible parameters. This is a normalizing constant).\n\n\nMaximum Likelihood Estimation (MLE)\nMLE is purely data-driven (no belief about parameters needed). It asks: ‚ÄúGiven this data, what are the parameters that make this data most probable?‚Äù\n\\[\\theta_{MLE} = \\arg \\max_{\\theta} P(D | \\theta)\\]\n\n\nMaximum A Posteriori (MAP) Estimation\nMAP is the inverse one, working on posterior:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(\\theta | D)\\]\nApplying Bayes‚Äô Theorem:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} \\frac{P(D | \\theta) P(\\theta)}{P(D)}\\]\nSince \\(P(D)\\) is a constant with respect to \\(\\theta\\) ( doesn‚Äôt depend on \\(\\theta\\)):\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(D | \\theta) P(\\theta)\\]\nKey characteristic: MAP is a blend of data and prior beliefs. It asks: ‚ÄúGiven this data and my prior beliefs about the parameters, what are the parameters that are most probable?‚Äù\n\n\nMLE is a Special Case of MAP\n\\(P(\\theta)\\) is constant, because parameter space is uniformly distributed:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(D | \\theta)\\]\nWow, MAP is reduced to MLE when parameters are uniformly distributed\nIn simple terms:\n\nMLE: ‚ÄúWhat parameters best explain the data I observed?‚Äù \\(\\rightarrow P(D | \\theta)\\)\nMAP: ‚ÄúWhat parameters are most plausible overall, considering both the data and what I already believed before seeing the data?‚Äù \\(\\rightarrow P(D | \\theta) \\times P(\\theta)\\)\nBayes‚Äô Theorem (Full Inference): ‚ÄúWhat‚Äôs the complete updated probability distribution for my parameters, given everything I know?‚Äù"
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#naive-bayes-classifier---the-naive-assumption-conditional-independence",
    "href": "posts/ml4r-bayesian-optimization/index.html#naive-bayes-classifier---the-naive-assumption-conditional-independence",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Naive Bayes Classifier - The ‚ÄúNaive‚Äù Assumption: Conditional Independence",
    "text": "Naive Bayes Classifier - The ‚ÄúNaive‚Äù Assumption: Conditional Independence\nRecall Bayes‚Äô Theorem for a hypothesis \\(H\\) (which is our class \\(C\\)) and evidence \\(E\\) (which are our features \\(F_1, F_2, \\dots, F_n\\)):\n\\[P(C | F_1, F_2, \\dots, F_n) = \\frac{P(F_1, F_2, \\dots, F_n | C) \\cdot P(C)}{P(F_1, F_2, \\dots, F_n)}\\]\nComputing the joint probability of correlated evidences likelihood term \\(P(F_1, F_2, \\dots, F_n | C)\\) can be very complex.\nThe ‚ÄúNaive‚Äù assumption: conditional independence, the joint likelihood = the product of individual feature likelihoods:\n\\[P(F_1, F_2, \\dots, F_n | C) \\approx \\prod_{i=1}^n P(F_i | C)\\]\nThus, we reduced the Naive Bayes Classifier to finding \\(C\\) that maximizes the posterior probability:\n\\(C_{predicted} = \\arg \\max_{C} \\left( P(C) \\cdot \\prod_{i=1}^n P(F_i | C) \\right)\\)\nJust the same as above, the denominator \\(P(E) = \\sum_i P(E|H_i)P(H_i) = P(F_1, F_2, \\dots, F_n)\\) is omitted during prediction because it‚Äôs a constant for all classes, acting only as a normalizer so that the sum of all posterior probabilities for all possible hypotheses equals 1."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#covariance-between-two-random-variables-1d",
    "href": "posts/ml4r-bayesian-optimization/index.html#covariance-between-two-random-variables-1d",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Covariance between Two Random Variables (1D)",
    "text": "Covariance between Two Random Variables (1D)\nLet‚Äôs start with the simplest case: two scalar random variables \\(X\\) and \\(Y\\). The covariance between \\(X\\) and \\(Y\\), denoted \\(Cov(X, Y)\\) or \\(\\sigma_{XY}\\), measures the degree to which they vary together.\nThe formal definition of covariance is:\n\\[Cov(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\]\nWhere:\n\n\n\\(\\mathbb{E}[X]\\) is the expected value (mean) of a vector that contains some samples but only taking the \\(X\\) dimension.\n\n\n\\(\\mathbb{E}[Y]\\) is the expected value (mean) of a vector that contains the samples but only taking the \\(Y\\) dimension.\n\n\nDot product between vectors \\((X - \\mathbb{E}[X]) and (Y - \\mathbb{E}[Y])\\)\n\n\nThe outer operation \\(\\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\) basically just eventually normalized by dividing the number of samples\n\n\nIntuition:\n\nIf \\(X\\) mostly distributed above its mean AND \\(Y\\) is also above its mean, the product \\((X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\) will often be positive.\nIf \\(X\\) mostly distributed above its mean AND \\(Y\\) is below its mean (and vice-versa), the product will often be negative.\nOtherwise the distributed \\(X\\) and \\(y\\) cancel out on average =&gt; zero covariance only implies no linear relationship; variables can still have a non-linear relationship."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#covariance-of-a-single-random-variable-with-itself-variance",
    "href": "posts/ml4r-bayesian-optimization/index.html#covariance-of-a-single-random-variable-with-itself-variance",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Covariance of a Single Random Variable with Itself (Variance)",
    "text": "Covariance of a Single Random Variable with Itself (Variance)\nVariance measures how much a single random variable deviates from its mean, or its spread, \\(Cov(X, X)\\). This gives us the variance of \\(X\\), denoted \\(Var(X)\\) or \\(\\sigma_X^2\\):\n\\[Cov(X, X) = \\mathbb{E}[(X - \\mathbb{E}[X])(X - \\mathbb{E}[X])] = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = Var(X)\\]\nCalculate the same way as above, basically a dot product of a vector \\((X - \\mathbb{E}[X])\\) with itself, therefore it will be sth like this:\n\\[Var(X) = (Xsample_1 - meanX)¬≤ + (Xsample_2 - meanX)¬≤ + ... + (Xsample_n - meanX)¬≤ = {a positive or negative number}\\]\nIntuition:\n\nmean \\(E[X]\\) could be in the middle, but when most samples are mostly distributed on negative or positive side, then the Medium will be on that side\nremember we are dealing with only one dimensional (scalar) variables"
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#the-covariance-matrix-for-multiple-random-variables-multivariate-case",
    "href": "posts/ml4r-bayesian-optimization/index.html#the-covariance-matrix-for-multiple-random-variables-multivariate-case",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "The Covariance Matrix for Multiple Random Variables (Multivariate Case)",
    "text": "The Covariance Matrix for Multiple Random Variables (Multivariate Case)\nNow, let‚Äôs extend both concepts above to multiple random variables. Suppose we have a random vector \\(\\mathbf{X}\\) consisting of \\(n\\) random variables:\n\\[\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}\\]\nThe covariance matrix \\(\\mathbf{\\Sigma}\\), is an \\(n \\times n\\) matrix where each element \\(\\mathbf{\\Sigma}_{ij}\\) represents the covariance between the \\(i\\)-th random variable \\(X_i\\) and the \\(j\\)-th random variable \\(X_j\\):\n\\[\\mathbf{\\Sigma}_{ij} = Cov(X_i, X_j)\\]\nMore formally, the covariance matrix \\(\\mathbf{\\Sigma}\\) is defined as:\n\\[\\mathbf{\\Sigma} = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])^T]\\]\nLet‚Äôs explicitly write out the elements of a \\(3 \\times 3\\) covariance matrix for a random vector \\(\\mathbf{X} = [X_1, X_2, X_3]^T\\):\n\\[\n\\mathbf{\\Sigma} = \\begin{bmatrix}\nVar(X_1) & Cov(X_1, X_2) & Cov(X_1, X_3) \\\\\nCov(X_2, X_1) & Var(X_2) & Cov(X_2, X_3) \\\\\nCov(X_3, X_1) & Cov(X_3, X_2) & Var(X_3)\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#intuition-of-the-covariance-matrix-spread-and-relationships",
    "href": "posts/ml4r-bayesian-optimization/index.html#intuition-of-the-covariance-matrix-spread-and-relationships",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Intuition of the Covariance Matrix: Spread and Relationships",
    "text": "Intuition of the Covariance Matrix: Spread and Relationships\n\nDiagonal Elements: The variances of each individual random variable a.k.a the individual spread: \\[\\mathbf{\\Sigma}_{ii} = Cov(X_i, X_i) = Var(X_i)\\]\nOff-Diagonal Elements: \\[\\mathbf{\\Sigma}_{ij} = Cov(X_i, X_j) \\quad \\text{for } i \\neq j\\] These tell us how pairs of variables move together.\n\nIf \\(\\mathbf{\\Sigma}_{ij} &gt; 0\\), \\(X_i\\) and \\(X_j\\) tend to increase or decrease together.\nIf \\(\\mathbf{\\Sigma}_{ij} &lt; 0\\), \\(X_i\\) tends to increase when \\(X_j\\) decreases, and vice-versa.\nIf \\(\\mathbf{\\Sigma}_{ij} \\approx 0\\), there is little to no linear relationship between \\(X_i\\) and \\(X_j\\).\n\nSymmetry: The covariance matrix is always symmetric, \\(\\mathbf{\\Sigma}_{ij} = \\mathbf{\\Sigma}_{ji}\\) because \\(Cov(X_i, X_j) = Cov(X_j, X_i)\\), you know how.\n\nIn the next chapter about Gaussian Processes, we will slowly build the kernel function \\(k(x, x')\\) directly defines the elements of this covariance matrix for function values at different input points. This matrix say how much a point x is impacted by x‚Äô."
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#the-linear-model-and-gaussian-noise-assumption",
    "href": "posts/ml4r-bayesian-optimization/index.html#the-linear-model-and-gaussian-noise-assumption",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "The Linear Model and Gaussian Noise Assumption",
    "text": "The Linear Model and Gaussian Noise Assumption\nConsider a simple linear regression model where we try to predict an output \\(y_i\\) based on input features \\(\\mathbf{x}_i\\):\n\\[y_i = \\mathbf{x}_i^T \\boldsymbol{\\beta} + \\epsilon_i\\]\nWhere:\n\n\\(y_i\\) is the \\(i\\)-th observed output.\n\\(\\mathbf{x}_i^T\\) representing the features for the \\(i\\)-th observation.\n\\(\\boldsymbol{\\beta}\\) is the vector of unknown regression coefficients (parameters) we want to estimate.\n\\(\\epsilon_i\\) is the error or noise term for the \\(i\\)-th observation.\n\nCrucial assumption for this derivation is that these error terms \\(\\epsilon_i\\) are independently and identically distributed (i.i.d.) according to a Gaussian (Normal) distribution with a mean of zero and a constant variance \\(\\sigma^2\\):\n\\[\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\]\nBecause \\(\\mathbf{x}_i^T \\boldsymbol{\\beta}\\) is a fixed (non-random) quantity for a given \\(\\mathbf{x}_i\\), this assumption about the error implies that \\(y_i\\) itself is also normally distributed:\n\\[y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2 \\sim \\mathcal{N}(\\mathbf{x}_i^T \\boldsymbol{\\beta}, \\sigma^2)\\]\nThis means the probability density function (PDF) for a single observation \\(y_i\\) is:\n\\[p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\]"
  },
  {
    "objectID": "posts/ml4r-bayesian-optimization/index.html#likelihood-function",
    "href": "posts/ml4r-bayesian-optimization/index.html#likelihood-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nImportant Definition: For a dataset of \\(N\\) independent observations \\((\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_N, y_N)\\), the likelihood function \\(L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y})\\) is the product of the individual PDFs (due to the independence assumption):\n\\[L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\prod_{i=1}^N p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2)\\]\n\\[L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\]\nGoal Maximum Likelihood Estimation (MLE):\n\nGiven datast \\((\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_N, y_N)\\)\n=&gt; Find \\(\\boldsymbol{\\beta}\\) (and \\(\\sigma^2\\)) that maximize this likelihood function.\n\n\nDerivation\nConverts products into sums, simplifying differentiation. Since the logarithm is a monotonically increasing function, maximizing \\(\\ln L\\) is equivalent to maximizing \\(L\\):\n\\[\\ln L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\ln \\left( \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right) \\right)\\]\nUsing logarithm properties (\\(\\ln(ab) = \\ln a + \\ln b\\) and \\(\\ln(a^b) = b \\ln a\\)):\n\\[\\ln L = \\sum_{i=1}^N \\left[ \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\ln\\left(\\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\right) \\right]\\]\n\\[\\ln L = \\sum_{i=1}^N \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2} \\right]\\]\n\\[\\ln L = -N \\cdot \\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\]\n\n\nMaximizing this function with respect to \\(\\boldsymbol{\\beta}\\)\nTo do that, easy, we take the partial derivative with respect to \\(\\boldsymbol{\\beta}\\) and set it to zero.\nInterestingly, \\(-N \\cdot \\frac{1}{2}\\ln(2\\pi\\sigma^2)\\), does not depend on \\(\\boldsymbol{\\beta}\\). Therefore, when maximizing \\(\\ln L\\) with respect to \\(\\boldsymbol{\\beta}\\), we only need to consider the second term:\n\\[\\text{maximize} \\left( - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2} \\right)\\]\nand voila!: \\[\\text{minimize} \\left( \\sum_{i=1}^N (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 \\right)\\]"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html",
    "href": "posts/ml4r-reinforcement-learning/index.html",
    "title": "Model-free Reinforcement Learning",
    "section": "",
    "text": "An agent‚Äôs interaction with the environment is usually modeled as a Markov Decision Process (MDP):\n\\(s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2,...\\)\nwhere:\n\n\\(t \\in \\{0, 1, 2, \\dots\\}\\)\n\\(s \\in \\mathcal{S}\\): a state\n\\(a \\in \\mathcal{A}(s)\\): an action available in state \\(s\\)\n\\(r \\in \\mathcal{R} \\subseteq \\mathbb{R}\\): a scalar reward\n\nThe environment dynamics (transition model) are given by:\n\\[\np(s', r \\mid s, a) = \\text{Prob}(S_{t+1} = s',\\ R_{t+1} = r \\mid S_t = s,\\ A_t = a)\n\\]\n\n\n\nThe probability of the next state depends only on the current state and action ‚Äî not the full history: \\[\nP(s_{t+1} \\mid s_t, a_t) = P(s_{t+1} \\mid s_1, a_1, ..., s_t, a_t)\n\\]\n\n\n\n\n\nStochastic: \\(\\pi(a \\mid s)\\)\nDeterministic: \\(a = \\pi(s)\\)\n\n\n\n\nThe return \\(G_t\\) is the total discounted reward from time \\(t+1\\) to final time \\(T\\):\n\\[\nG_t = \\sum_{k = t+1}^{T} \\gamma^{k - t - 1} R_k\n\\]\nExpanded:\n\\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\]\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{T - t - 1} R_T\n\\]\n\n\n\n\\[\n\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\n\\]\nHow can we determine a policy that accumulates a high reward?"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#value-functions",
    "href": "posts/ml4r-reinforcement-learning/index.html#value-functions",
    "title": "Model-free Reinforcement Learning",
    "section": "Value Functions",
    "text": "Value Functions\nVery similar to return \\(\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\\).\nValue functions are used to estimate expected returns:\n\nState-value function: \\(V_\\pi(s_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\)\nAction-value function: \\(Q_\\pi(s_t, a_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\nRelationship: \\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) and \\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)\nDerivation of this relationship:\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) (semantically true because)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} \\mid s_t = s, a_t = a]\\) (because \\(\\mathbb{E}[G_{t+1} \\mid s, a] = \\mathbb{E}[V_\\pi(S_{t+1}) \\mid s, a]\\))\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a)[r + \\gamma V_\\pi(s')]\\)"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#the-bellman-equations",
    "href": "posts/ml4r-reinforcement-learning/index.html#the-bellman-equations",
    "title": "Model-free Reinforcement Learning",
    "section": "The Bellman Equations",
    "text": "The Bellman Equations\nFor any policy \\(\\pi\\), all \\(s \\in \\mathcal{S}\\), and all \\(a \\in \\mathcal{A}(s)\\):\nState-value function:\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\, q_\\pi(s, a) \\tag{1}\n\\]\nAction-value function:\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right] \\tag{2}\n\\]\nAfter substitution as derived above:\n\nBellman Equation for \\(v_\\pi(s)\\)\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]\n\n\nBellman Equation for \\(q_\\pi(s, a)\\)\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a' \\in \\mathcal{A}(s')} \\pi(a' \\mid s') q_\\pi(s', a') \\right]\n\\]\n\n\nOptimal Bellman\n\\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "href": "posts/ml4r-reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "title": "Model-free Reinforcement Learning",
    "section": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement",
    "text": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement\nUsing the Bellman Equation above and an initial estimate (e.g.¬†all states are randomly initialized or uniformly negative, except the terminal state which is 0), we iteratively update:\nWe apply the Bellman equations above for every single state (for V), or (state, action) pair (for Q, which forms a finer table). Over many runs, it will slowly converge. \\[\n\\pi'(s) = \\arg\\max_a q_\\pi(s, a)\n\\] Where: \\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#monte-carlo-state-value",
    "href": "posts/ml4r-reinforcement-learning/index.html#monte-carlo-state-value",
    "title": "Model-free Reinforcement Learning",
    "section": "Monte Carlo (State Value)",
    "text": "Monte Carlo (State Value)\nGoal: Given samples under \\(\\pi\\), estimate \\(q_\\pi\\).\n\nWe can express \\(q_\\pi\\)-estimation as \\(v_\\pi\\)-estimation. Imagine a new problem where: \\[\nS_t^{\\text{new}} = (S_t, A_t)\n\\]\n\nAny evaluation algorithm estimating \\(v(s^{\\text{new}})\\) would be estimating \\(q_\\pi(s, a)\\).\nSo basically what happening is that we do not have a policy anymore, instead, policy is now a part of the environment. But why do we want to do that? Because: - This is a generalization anyway, something applied to state value function V, also applied for action value function Q. - It simplifies our analysis, reduces the problem to a simpler problem Markov Reward Process (MRP), which is MDP without actions (s1,r1,s2,r2,s3,r3,‚Ä¶)\nOK, but still, how to do it?\nStart with a random value function: \\(V(s) = \\frac{1}{|S|}\\)\nCollect sampling trajectories \\(M\\) trajectory samples: \\[\ns_0^m \\quad r_1^m \\quad s_1^m \\quad \\cdots \\quad s_{T_m}^m \\qquad m = 1, \\ldots, M\n\\]\nThe Goal: Use averages to approximate \\(v_\\pi(s)\\): \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s] \\approx \\frac{1}{C(s)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[s_\\tau^m = s] \\, g_\\tau^m\n\\] where: \\[\n\\mathbb{I}[s_\\tau^m = s] =\n\\begin{cases}\n1 & \\text{if } s_\\tau^m = s \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] \\[\ng_\\tau^m = R_{t+1}^m + \\gamma R_{t+2}^m + \\gamma^2 R_{t+3}^m + \\dots + \\gamma^{T - t - 1} R_T^m\n\\]\nFor every sample trajectory \\(m\\), at any step \\(\\tau\\) in that trajectory, check if the state \\(g_\\tau^m\\) of that step is the \\(s\\) we are interested in, then include its return \\(g_\\tau^m\\) in the sum, then normalize by \\(C(s)\\), the total number of times state \\(s\\) was visited.\nAt this moment I just realized that: the state will get higher return, if its nearer to the beginning of a trajectory, if u dont believe, have a look at \\(g_\\tau^m\\) again ^^.\nBtw, to calculate return \\(g_\\tau^m\\), maybe you already know, we have to calculate from the terminate state first \\(R_T^m\\), where we know if the reward \\(R_T^m\\) is 0 or 1 (reached the goal or not), then slowly trace backward with addding \\(\\gamma\\)\nAnd to make sure you understand it, \\(v_\\pi(s)\\) is just like \\(G\\), but \\(G\\) is mostly binded to the trajectory and a policy, therefore the function \\(v_\\pi(s)\\) is actually \\(G\\)!!!\nHow to get to that Goal? to apply after the \\(m\\)-th sample: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha \\left( g_t^m - V(s_t^m) \\right)\n\\]\n‚Ä¶ then it will slowly converge to the Goal above ‚Ä¶\nBUT, how do we extend this to update our action ?"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#monte-carlo-action-value",
    "href": "posts/ml4r-reinforcement-learning/index.html#monte-carlo-action-value",
    "title": "Model-free Reinforcement Learning",
    "section": "Monte Carlo (Action Value)",
    "text": "Monte Carlo (Action Value)\nSince we also have a deterministic set of action \\(a \\in \\mathcal{A}(s)\\), therefore we can extend the state value above to action value like this, it is equivalent:\nStart also with \\(Q(s,a) = \\frac{1}{|SxA|}\\) or just simply 0\nBasically it just create a finer Q-table.\nThe Goal \\[\nQ_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a] \\approx \\frac{1}{C(s, a)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[(s,a)_\\tau^m = (s,a)] \\, g_\\tau^m\n\\]\nHow to get to that goal? \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha \\left( g_t^m - Q(s_t^m, a_t^m) \\right)\n\\]\n‚Ä¶ Then it will slowly converge the Goal above ‚Ä¶\nThen we just argmax over action at each state, thats how we get optimal action."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#sum-up-constant-Œ±-mc-algorithm-pi-approx-pi",
    "href": "posts/ml4r-reinforcement-learning/index.html#sum-up-constant-Œ±-mc-algorithm-pi-approx-pi",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: Constant-Œ± MC Algorithm \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Constant-Œ± MC Algorithm \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(\\epsilon &gt; 0\\), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some \\(\\epsilon\\)-soft policy\n\n\\(Q(s,a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S},\\ a \\in \\mathcal{A}(s)\\) (like a random Q-Table hehe)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample a trajectory under policy \\(\\pi\\):\n\\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\)\nFor (literally EACH - EVERY SINGLE) \\(t = 0, \\dots, T_m - 1\\):\n\nCompute return (the best way is just to calculate backwards then slowly add \\(\\gamma\\) like Gonkee ^^):\n\\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nUpdate Q-value:\n\\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\n\\]\n\nUpdate policy:\n\\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\)\n\nWhere \\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\) is specified as follows: \\[\na^* \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a) \\quad \\text{(ties broken arbitrarily)}\n\\]\nFor all \\(a \\in \\mathcal{A}(s_t^m)\\): (this means to balance the policy to avoid a local optimal) \\[\n\\pi(a|s_t^m) \\leftarrow\n\\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a = a^* \\\\\n\\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a \\neq a^*\n\\end{cases}\n\\]\n(\\(|\\mathcal{A}(s_t^m)| = \\text{number of actions in } \\mathcal{A}(s_t^m)\\))\nthen back to the loop For \\(m = 1, \\dots, M\\) again and again ‚Ä¶"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#off-policy",
    "href": "posts/ml4r-reinforcement-learning/index.html#off-policy",
    "title": "Model-free Reinforcement Learning",
    "section": "Off-Policy",
    "text": "Off-Policy\nThe problem is, as seen in the Blackjack example, lack of sample leads to local-optimal bias model\n=&gt; Goal: more variance\nSample a trajectory under a different policy \\(b\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\). But the rest of the algorithm stays the same.\nOK, but how to make sampling policy \\(b\\) effect the main behavior policy \\(\\pi\\)?\n\nRelationship between sampling policy \\(b\\) vs main behavior policy \\(\\pi\\)?\nWe want: \\[\nq_\\pi(s, a) = \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]\n\\]\nSampled data under \\(b\\) means this is what we actually estimated: \\[\n\\mathbb{E}_b[G_t|S_t = s, A_t = a]\n\\]\nTherefore we use Importance Sampling to bring them to \\(\\pi\\): \\[\nq_\\pi(s, a) = \\mathbb{E}_b\\left[\\frac{p_\\pi(G_t)}{p_b(G_t)}G_t|S_t = s, A_t = a\\right]\n\\] where \\(\\rho\\) is the importance sampling ratio: \\[\n\\frac{p_\\pi(G_t)}{p_b(G_t)} = \\rho = \\prod_{\\tau=t+1}^{T-1} \\frac{\\pi(A_\\tau|S_\\tau)}{b(A_\\tau|S_\\tau)}\n\\]"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "href": "posts/ml4r-reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(b\\) (behavior policy), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some policy\n\\(Q(s, a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\) (also the random Q-Table above hehe)\n\nFor \\(m = 1, \\dots, M\\):\nUnder \\(b\\) sample: \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\)\n\nFor \\(t = 0, \\dots, T_m - 1\\):\n\n\\(\\rho_t^m \\leftarrow \\prod_{\\tau=t+1}^{T_m-1} \\frac{\\pi(a_\\tau^m|s_\\tau^m)}{b(a_\\tau^m|s_\\tau^m)}\\) (or 1 if \\(t+1 &gt; T_m-1\\))\nCompute return: \\(g_t^m \\leftarrow \\rho_t^m(r_{t+1}^m + \\gamma r_{t+2}^m + \\dots)\\)\nUpdate Q-Value: \\(Q(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\\)\nUpdate policy: \\(\\pi(s_t^m) \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a)\\) (ties broken arbitrarily)\n\nNote that at Update Policy: we do not need the \\(\\pi\\)-greedy as above, because now using behavior-policy \\(b\\), we could already diverse out for a more global view\n\nBUT, off policy MC has too much variance, therefore the next technique ‚Ä¶ Temporal Difference\nBefore we continue, let‚Äôs see where is exactly the point of model-free MC learning:"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "href": "posts/ml4r-reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "title": "Model-free Reinforcement Learning",
    "section": "n-step Temporal Difference Learning",
    "text": "n-step Temporal Difference Learning\nRecall from the MC approach: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\nwhere: \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nn-STEP TD: Replace the target, \\(g_t^m\\), with: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n V(s_{t+n}^m)\n\\]\nwhere \\(V(s_{t+n}^m)\\) is actually no different than \\(g_{t+n}^m\\), but instead of waiting for the trajectory to finish the episode then calculate backward, we just need to wait for \\(n\\) steps to BOOTSTRAPPING the existing \\(V(s_{t+n})\\) calculated from older trajectories, think a little bit, it means the same thing with \\(g_{t+n}\\) (accumulated return). If \\(n = \\infty\\): TD is identical to MC."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#why-is-td-better",
    "href": "posts/ml4r-reinforcement-learning/index.html#why-is-td-better",
    "title": "Model-free Reinforcement Learning",
    "section": "Why is TD better?",
    "text": "Why is TD better?\n\nMarkov property: The RL environment has Markov property, that means the future depends only on the current state, not the history. And TD works based on that design, the update is just between neighboring states. Not like waiting for the whole trajectory like MC, \\(g_t^m\\) needs backward calculation for the whole trajectory =&gt; strongly history based. \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\n\nFor example in TD(0) the use of \\(V(s_{t+1}^m)\\) is very Markov property: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(r_{t+1}^m + \\gamma V(s_{t+1}^m) - V(s_t^m))\n\\]\n\nReduced Variance: The effect of MC is calculating accumulated result at each single trajectory, then averaging then in the end, this makes the result very specific to the sampled trajectories. But TD directly add the already smoothed \\(+ \\gamma^n V(s_{t+n}^m)\\)\nOnline-learning we all know what that means"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#what-does-larger-n-means",
    "href": "posts/ml4r-reinforcement-learning/index.html#what-does-larger-n-means",
    "title": "Model-free Reinforcement Learning",
    "section": "What does larger n means?",
    "text": "What does larger n means?\nIncrease the bias = utilize more from the knowledge from this trajectory, instead of keep learning from the average.\nE.g., for a single episode with TD(8):\n\nAll states from \\(S_0\\) up to \\(S_{k-8}\\) (if \\(k \\ge 8\\)): Will be updated using a full 8-step return, bootstrapping from \\(V(S_{t+8})\\) =&gt; very average in the beginning\nThe last 7 states (\\(S_{k-7}, \\dots, S_{k-1}\\)): Will be updated using a return that effectively ‚Äúruns out of steps‚Äù before 8. Their targets will be a mix of actual rewards leading to the terminal state, like Monte Carlo =&gt; direct reward in the end"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#sum-up-on-policy-temporal-difference-n-step-sarsa",
    "href": "posts/ml4r-reinforcement-learning/index.html#sum-up-on-policy-temporal-difference-n-step-sarsa",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: On-Policy Temporal Difference: n-step Sarsa",
    "text": "Sum up: On-Policy Temporal Difference: n-step Sarsa\nModel-free control \\(\\rightarrow\\) use \\(Q(s, a)\\), not \\(V(s)\\).\nRedefine: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n Q(s_{t+n}^m, a_{t+n}^m)\n\\]\nUpdate rule: \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_{t:t+n}^m - Q(s_t^m, a_t^m))\n\\]"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#q-learning",
    "href": "posts/ml4r-reinforcement-learning/index.html#q-learning",
    "title": "Model-free Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\n1-step TD Control‚Äî-target adjustment‚Äî-&gt; Q-Learning (off-policy).\nInstead of using \\(r_{t+1}^m + \\gamma Q(s_{t+1}^m, a_{t+1}^m)\\) (which is used in SARSA and relies on the next action taken by the policy), Q-Learning uses: \\[\nr_{t+1}^m + \\gamma \\max_{a} Q(s_{t+1}^m, a)\n\\]\nThe \\(\\max\\) operator means, regardless of which action the behavior policy \\(b\\) actually took, this target is formed by the best possible action from the next state \\(s_{t+1}^m\\) =&gt; Q-Learning an off-policy.\nTo describe what actually happens, it is like this: 1-step TD (SARSA-like): \\[\n\\dots s_0^m, \\underset{\\uparrow}{\\underline{a_0^m}}, r_1^m, s_1^m, \\underset{\\uparrow}{\\underline{a_1^m}}, r_2^m, s_2^m, \\underset{\\uparrow}{\\underline{a_2^m}}, r_3^m, s_3^m, \\dots\n\\] Updates for \\(Q(s_t, a_t)\\) occur after observing \\(s_{t+1}, a_{t+1}\\), using the target \\(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\) where \\(a_{t+1}\\) is the action taken by the current policy.\n1-step Q-Learning: \\[\n\\dots  \\underset{\\uparrow}{\\underline{s_0^m}},a_0^m, r_1^m, \\underset{\\uparrow}{\\underline{s_1^m}},a_1^m, r_2^m, \\underset{\\uparrow}{\\underline{s_2^m}},a_2^m, r_3^m, \\dots\n\\] Updates for \\(Q(s_t, a_t)\\) occur after observing \\(s_{t+1}\\), using the target \\(r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a)\\), where action \\(a_{t+1}\\) is observed but not used in forming the target for \\(Q(s_t, a_t)\\)."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#expected-sarsa",
    "href": "posts/ml4r-reinforcement-learning/index.html#expected-sarsa",
    "title": "Model-free Reinforcement Learning",
    "section": "Expected Sarsa",
    "text": "Expected Sarsa\n1-step Q-Learning ‚Äî‚Äì \\(\\max\\) operator-&gt;average operator‚Äî&gt; Expected Sarsa \\[\nr_{t+1}^m + \\gamma \\max_{a} Q(s_{t+1}^m, a)\n\\] to using an expectation over all possible actions, weighted by the policy \\(\\pi\\): \\[\nr_{t+1}^m + \\gamma \\sum_{a} Q(s_{t+1}^m, a) \\pi(a|s_{t+1}^m)\n\\]\nAs presented (when the policy \\(\\pi\\) used in the target is the same as the behavior policy generating the data), this is an on-policy method.\nBut to make it off-policy, just need \\(\\text{policy generating the trajectory} \\neq \\pi \\text{ in target}\\)"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#compare",
    "href": "posts/ml4r-reinforcement-learning/index.html#compare",
    "title": "Model-free Reinforcement Learning",
    "section": "Compare",
    "text": "Compare\n\nSarsa has longer path, because it just took the action it actually took, which was most of the time exploratory, the one with \\(\\epsilon \\text{exploration}\\) policy that balance out\nQ-Learning does not use this \\(\\epsilon \\text{exploration}\\) policy, it uses \\(\\max\\) operator\nExpected Sarsa use weighted average, so yeah, always a safe choice."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#summary-of-td",
    "href": "posts/ml4r-reinforcement-learning/index.html#summary-of-td",
    "title": "Model-free Reinforcement Learning",
    "section": "Summary of TD",
    "text": "Summary of TD\nGoal of Q-Learning is updating Q-Table to optimal where: \\[\nQ_*(s_t, a_t) = \\mathbb{E} \\left[ r_t + \\gamma \\max_a Q_*(s_{t+1}, a) \\right]\n\\]\nAlso called Bellman Optimality Equation: every subsequence of a optimal sequence must also be optimal\nLearning Q-values:\n\nSARSA:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n\\]\nExpected SARSA:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\sum_a \\pi(a \\mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\nQ-Learning:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\n\nLearning V-values:\n\\[\nV(s_t) \\leftarrow V(s_t) + \\alpha \\left[ r_t + \\gamma V(s_{t+1}) - V(s_t) \\right]\n\\]\nWhere \\(\\alpha\\) is the learning rate."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#on-policy-evaluation-with-function-approximation",
    "href": "posts/ml4r-reinforcement-learning/index.html#on-policy-evaluation-with-function-approximation",
    "title": "Model-free Reinforcement Learning",
    "section": "On-Policy Evaluation with Function Approximation",
    "text": "On-Policy Evaluation with Function Approximation\nGoal remains to approximate the state-value function \\(v_\\pi(s)\\). Data generated from a given fixed policy \\(\\pi\\).\nWe now learn a parameterized function \\(\\hat{v}(s, \\mathbf{w})\\), where:\n\n\\(\\mathbf{w} \\in \\mathbb{R}^d\\) is a vector of parameters\na state \\(s\\) as input\n\nWe learn \\(w\\) and hope that \\(v_\\pi(s) \\approx \\hat{v}(s, \\mathbf{w})\\)\nSince \\(d \\ll |\\mathcal{S}|\\), any change to \\(\\mathbf{w}\\) can simultaneously change \\(\\hat{v}(s, \\mathbf{w})\\) for many (or all) states \\(s\\). Different from tabular methods, where an update to \\(V(s)\\) for a specific state \\(s\\) affects only that state‚Äôs value."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#how-to-represent-mathbfxs",
    "href": "posts/ml4r-reinforcement-learning/index.html#how-to-represent-mathbfxs",
    "title": "Model-free Reinforcement Learning",
    "section": "How to represent \\(\\mathbf{x}(s)\\)?",
    "text": "How to represent \\(\\mathbf{x}(s)\\)?\nExample 1: two simple features for given image state \\(s\\):\n\n\\(x_1(s)\\): The average of all pixel values in the image.\n\\(x_2(s)\\): The standard deviation of all pixel values in the image.\n\n=&gt; feature vector \\(\\mathbf{x}(s) = \\begin{bmatrix} x_1(s) \\\\ x_2(s) \\end{bmatrix}\\)\nWith these features, we can construct a linear value function to approximate \\(v_\\pi(s)\\): \\[\n\\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s)^T \\mathbf{w}\n\\]\nExample 2: Mountain Car below with a dynamic model of velocity \\(dx\\) and position \\(x\\)\nExample 3: Proto Points and Radius Basis Function will be discussed in next chapter Policy Gradient Method"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#goal-how-to-get-mathbfw",
    "href": "posts/ml4r-reinforcement-learning/index.html#goal-how-to-get-mathbfw",
    "title": "Model-free Reinforcement Learning",
    "section": "Goal: How to get \\(\\mathbf{w}\\)?",
    "text": "Goal: How to get \\(\\mathbf{w}\\)?\nThe ‚Äòbest‚Äô \\(\\mathbf{w}\\) minimizes: \\[\n\\overline{VE}(\\mathbf{w}) = \\sum_{s \\in \\mathcal{S}} \\mu(s)[v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]^2\n\\] where \\(\\mu(\\cdot)\\) is a distribution over states (frequency of visiting each state).\n\nWe observe a surrogate for \\(v_\\pi(S_t)\\): \\(U_t\\): Since we don‚Äôt know \\(v_\\pi(S_t)\\) exactly, we use a sample-based estimate or target, \\(U_t\\), as a stand-in. This \\(U_t\\) could be the Monte Carlo return \\(G_t\\), or an n-step TD target \\(g_{t:t+n}\\), or a 1-step TD target \\((R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}))\\).\n\nUpdate Rule we don‚Äôt have direct access to \\(v_\\pi(s)\\) for all states =&gt; Stochastic Gradient Descent (SGD) for updating our parameters \\(\\mathbf{w}\\): \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\] Where:\n\n\\(\\alpha\\) is the learning rate (step size).\n\\(\\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right]\\) is the TD error (or prediction error) based on our sample \\(U_t\\).\n\\(\\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\\) is the gradient of our estimated value function with respect to its parameters \\(\\mathbf{w}\\), evaluated at state \\(S_t\\). This gradient tells us how to adjust \\(\\mathbf{w}\\) to change \\(\\hat{v}(S_t, \\mathbf{w})\\) in the desired direction."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#how-to-obtain-the-target-u_t",
    "href": "posts/ml4r-reinforcement-learning/index.html#how-to-obtain-the-target-u_t",
    "title": "Model-free Reinforcement Learning",
    "section": "How to Obtain the Target \\(U_t\\)",
    "text": "How to Obtain the Target \\(U_t\\)\nIn the Stochastic Gradient Descent update rule: \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\] The term \\(U_t\\) serves as our sample-based target for the true value \\(v_\\pi(S_t)\\). Since \\(v_\\pi(S_t)\\) is unknown, we must derive \\(U_t\\) from our observed experience. The choice of \\(U_t\\) determines whether our method leans towards Monte Carlo or Temporal Difference approaches:\n1. Monte Carlo Target: If the target \\(U_t\\) is the full Monte Carlo return from state \\(S_t\\) to the end of the episode, then we are using a Gradient Monte Carlo method: \\[\nU_t = G_t\n\\] where \\(G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1}R_T\\) is the total discounted return observed from time step \\(t\\) until the terminal state \\(T\\).\n\nCharacteristics:\n\nUnbiased: If \\(G_t\\) is an unbiased estimate of \\(v_\\pi(S_t)\\) (which it is, on average), then using it as \\(U_t\\) can lead to the parameters \\(\\mathbf{w}\\) converging to a local optimum of the Mean Squared Value Error (\\(\\overline{VE}(\\mathbf{w})\\)).\nHigh Variance: \\(G_t\\) can be noisy due to the sum of many random rewards.\nRequires complete episodes: We must wait until the episode ends to compute \\(G_t\\).\n\n\n2. Temporal Difference (TD) Target: If the target \\(U_t\\) is derived using bootstrapping (i.e., using an estimate of the value of a future state), then we are using a Semi-Gradient TD method. The most common is the 1-step TD target: \\[\nU_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\n\\] Here, \\(R_{t+1}\\) is the actual reward observed, and \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\) is our current estimate of the value of the next state \\(S_{t+1}\\). For this specific update, \\(\\mathbf{w}\\) in \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\) is usually the online network‚Äôs weights, not a target network‚Äôs weights in this basic formulation.\n\nCharacteristics:\n\nBiased: especially in the beginning with crappy initialized value \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\).\nLower Variance: It typically has lower variance than the Monte Carlo target because it depends on only one actual reward and then uses a smoothed estimate for the rest of the future.\nOnline Learning: Updates can be performed after each single time step, without waiting for the end of an episode.\nSemi-Gradient: Since \\(U_t\\) depends on \\(\\mathbf{w}\\), our update rule is not a true gradient step. The gradient \\(\\nabla_{\\mathbf{w}} L(\\mathbf{w})\\) for the loss \\(\\left( (R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})) - \\hat{v}(S_t, \\mathbf{w}) \\right)^2\\) would actually involve the derivative of \\(U_t\\) (\\(\\hat{v}(S_{t+1}, \\mathbf{w})\\)) with respect to \\(\\mathbf{w}\\). =&gt; semi-gradient means: \\(\\nabla_{\\mathbf{w}} \\hat{v}(S_{t+1}, \\mathbf{w})\\) is omitted for simplicity and stability, only taking \\(\\nabla_{\\mathbf{w}} \\hat{v}(S_{t}, \\mathbf{w})\\).\nNo Guarantee of Convergence (to Global Optimum): Because it‚Äôs not a true gradient of the overall \\(\\overline{VE}(\\mathbf{w})\\), we generally don‚Äôt guarantee convergence to the global optimum of the Mean Squared Value Error, even if the optimal \\(\\mathbf{w}\\) is unique. However, for linear function approximation, it can still converge to a local optimum. For non-linear approximators like neural networks, theoretical guarantees are weaker, but these methods still perform very well in practice.\n\n\nThe update rule remains: \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation",
    "href": "posts/ml4r-reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation",
    "title": "Model-free Reinforcement Learning",
    "section": "Extension to On-Policy Control with Function Approximation",
    "text": "Extension to On-Policy Control with Function Approximation\nSo far is just policy evaluation (approximating \\(v_\\pi(s)\\)). Now extend directly to control problems (finding an optimal policy), typically by approximating the action-value function \\(q_\\pi(s,a)\\) or \\(q_*(s,a)\\). \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{q}(S_t, A_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})\n\\] Here, \\(\\hat{q}(S_t, A_t, \\mathbf{w})\\) is our function approximator‚Äôs estimate of the action-value for the state-action pair \\((S_t, A_t)\\) using parameters \\(\\mathbf{w}\\). The term \\(\\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})\\) is the gradient of this estimate with respect to \\(\\mathbf{w}\\).\nFor Semi-gradient 1-step Sarsa, the target \\(U_t\\) is defined as: \\[\nU_t = R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\mathbf{w})\n\\]\nSince both the action being evaluated (\\(A_t\\)) and the action used to construct the target (\\(A_{t+1}\\)) are chosen according to the same behavior policy (which is actively being improved based on \\(\\hat{q}\\)), this method is on-policy. The agent learns the value of the policy it is currently following, and this policy itself is improved through action selection methods (e.g., \\(\\epsilon\\)-greedy) based on the learned \\(\\hat{q}\\) values.\nExample: Linear Action-Value Function for the Mountain Car Task\nTo illustrate how function approximation can be used for action-value functions, let‚Äôs consider a scenario like The Mountain Car Task. We approximate the action-value function \\(\\hat{q}(s, a, \\mathbf{w})\\) using a linear function approximator instead of a Q-Table: \\[\n\\hat{q}(s, a, \\mathbf{w}) = \\begin{cases}\n    \\mathbf{w}_{-1}^T \\mathbf{x}(s) & \\text{if } a = -1 \\\\\n    \\mathbf{w}_{0}^T \\mathbf{x}(s) & \\text{if } a = 0 \\\\\n    \\mathbf{w}_{1}^T \\mathbf{x}(s) & \\text{if } a = 1\n\\end{cases}\n\\] Where:\n\naction \\(a\\) (-1, 0 -1)\n\\(\\mathbf{x}(s)\\) is the feature representation of the state \\(s\\) (length 120 decoded from position & velocity)\n\\(\\mathbf{w}_{-1}\\), \\(\\mathbf{w}_{0}\\), and \\(\\mathbf{w}_{1}\\) are distinct weight vectors, each corresponding to one of the possible actions.\nThe overall parameter vector \\(\\mathbf{w}\\) for the entire function approximator would be the concatenation of these individual action-specific weight vectors (i.e., \\(\\mathbf{w} = [\\mathbf{w}_{-1}, \\mathbf{w}_{0}, \\mathbf{w}_{1}]\\))."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation-1",
    "href": "posts/ml4r-reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation-1",
    "title": "Model-free Reinforcement Learning",
    "section": "Extension to On-Policy Control with Function Approximation",
    "text": "Extension to On-Policy Control with Function Approximation\nWe all know what that is, just that when we combine three things together:\n\noff-policy\nfunction approximation\nbootstrapping\n\nwe will have problem with convergence, which may be solved by the next topic ‚Ä¶"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#how-to-represent-xs",
    "href": "posts/ml4r-reinforcement-learning/index.html#how-to-represent-xs",
    "title": "Model-free Reinforcement Learning",
    "section": "How to represent \\(x(s)\\)?",
    "text": "How to represent \\(x(s)\\)?\n\nProto Points and Radial Basis Functions (RBFs)\ninstead of having a neural network directly compute an output, the state is first transformed into a set of features by measuring its ‚Äúsimilarity‚Äù or ‚Äúproximity‚Äù to a predefined set of ‚Äúproto points‚Äù or ‚Äúcenters.‚Äù\n\nDefine Proto Points (Centers):\nFirst, you select a number of ‚Äúproto points‚Äù or ‚Äúcenters‚Äù in your state space. Let‚Äôs call these centers \\(c_1, c_2, \\dots, c_k\\). These centers are essentially fixed, representative points in your environment‚Äôs state space. They could be chosen: manually or randomly\n\n\nDefine Basis Functions (e.g., Radial Basis Functions):\nFor each center \\(c_j\\), you define a basis function \\(\\phi_j(s)\\). A common choice is a Gaussian Radial Basis Function: \\[\n\\phi_j(s) = \\exp \\left( -\\frac{\\|s - c_j\\|^2}{2\\sigma_j^2} \\right)\n\\]\nwhere:\n\n\\(\\|s - c_j\\|^2\\) is the squared Euclidean distance between the current state \\(s\\) and the center \\(c_j\\).\n\\(\\sigma_j^2\\) is a variance or width parameter for that basis function, controlling how broad its ‚Äúinfluence‚Äù is.\n\nWhat does \\(\\phi_j(s)\\) mean? It‚Äôs a measure of how ‚Äúsimilar‚Äù or ‚Äúclose‚Äù the current state \\(s\\) is to the center \\(c_j\\). It peaks at 1 when \\(s = c_j\\) and decays to 0 as \\(s\\) moves away from \\(c_j\\).\n\n\nConstruct the Feature Vector:\nFor any given state \\(s\\), you compute the value of each basis function: \\[\n\\mathbf{x}(s) = \\begin{bmatrix} \\phi_1(s) \\\\ \\phi_2(s) \\\\ \\vdots \\\\ \\phi_k(s) \\end{bmatrix}\n\\] This \\(\\mathbf{x}(s)\\) is your feature vector, where each feature represents the activation of a ‚Äúproto point.‚Äù\n\n\nLinear Function Approximation:\nNow, these features are used in a linear function approximator. The parameters you learn are the weights associated with each of these basis functions.\n\nFor a value function: \\[\n  \\hat{V}(s, \\mathbf{w}) = \\sum_{j=1}^k w_j \\phi_j(s) = \\mathbf{w}^T \\mathbf{x}(s)\n  \\] Here, \\(\\mathbf{w}\\) is the vector \\([w_1, \\dots, w_k]\\), and \\(w_j\\) is the weight for the \\(j\\)-th proto point‚Äôs influence.\nFor a policy (e.g., logits in a softmax): For each action \\(a\\), you‚Äôd have a separate weight vector \\(\\mathbf{\\theta}_a\\), and the logits for the policy could be: \\[\n  h(s,a,\\mathbf{\\theta}_a) = \\mathbf{\\theta}_a^T \\mathbf{x}(s)\n  \\] Then, \\(\\pi(a|s,\\mathbf{\\theta}) = \\text{softmax}(h(s,a,\\mathbf{\\theta}_a))\\).\n\n\n\n\nExample: Policy Gradient with Proto Points (RBFs) for Mountain Car\nImagine our Mountain Car environment again. The state \\(s\\) is (position, velocity), which are continuous. We want to learn a policy \\(\\pi(a|s, \\mathbf{\\theta})\\) directly.\n1. Define Proto Points (Centers) in State Space:\nLet‚Äôs say we define \\(K=4\\) proto points (centers) \\(c_j\\) in our 2D (position, velocity) state space. For simplicity, let‚Äôs pick them:\n\n\\(c_1 = (-0.5, 0.0)\\) (Mid-left, still)\n\\(c_2 = (0.0, 0.0)\\) (Center, still)\n\\(c_3 = (0.5, 0.0)\\) (Mid-right, still)\n\\(c_4 = (-0.2, 0.05)\\) (Slightly left, moving right)\n\nWe also define a width \\(\\sigma^2\\) for all RBFs (or separate \\(\\sigma_j^2\\) values).\n2. Create the State Feature Vector \\(\\mathbf{x}(s)\\) using RBFs:\nFor any state \\(s = (\\text{pos}, \\text{vel})\\), we calculate its similarity to each of these 4 proto points using a Gaussian RBF. Our feature vector \\(\\mathbf{x}(s)\\) will have 4 dimensions:\n\\[\n\\mathbf{x}(s) = \\begin{bmatrix}\n\\phi_1(s) = \\exp \\left( -\\frac{\\|s - c_1\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_2(s) = \\exp \\left( -\\frac{\\|s - c_2\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_3(s) = \\exp \\left( -\\frac{\\|s - c_3\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_4(s) = \\exp \\left( -\\frac{\\|s - c_4\\|^2}{2\\sigma^2} \\right)\n\\end{bmatrix}\n\\] This \\(\\mathbf{x}(s)\\) now serves as our ‚Äúmeaningful‚Äù representation of the state, telling us how similar \\(s\\) is to certain key points in the environment.\n3. Define the Policy \\(\\pi(a|s, \\mathbf{\\theta})\\) using these Features:\nFor each action \\(a \\in \\{-1, 0, 1\\}\\), we define a linear combination of these features to get a ‚Äúscore‚Äù or ‚Äúlogit‚Äù for that action.\nLet \\(\\mathbf{\\theta}_{-1}\\), \\(\\mathbf{\\theta}_{0}\\), \\(\\mathbf{\\theta}_{1}\\) be our learnable parameter vectors (weights), each of size \\(K=4\\). The total policy parameters \\(\\mathbf{\\theta}\\) would be the concatenation of these three vectors.\nThe logits for each action are: * \\(h(s, a=-1, \\mathbf{\\theta}) = \\mathbf{\\theta}_{-1}^T \\mathbf{x}(s)\\) * \\(h(s, a=0, \\mathbf{\\theta}) = \\mathbf{\\theta}_{0}^T \\mathbf{x}(s)\\) * \\(h(s, a=1, \\mathbf{\\theta}) = \\mathbf{\\theta}_{1}^T \\mathbf{x}(s)\\)\nAnd the policy probabilities are then given by the softmax function: \\[\\pi(a|s, \\mathbf{\\theta}) = \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_{a'} e^{h(s,a',\\mathbf{\\theta})}}\\]\n4. Update the Policy Parameters (\\(\\mathbf{\\theta}\\)) using Policy Gradients (REINFORCE):\nWhen we collect a trajectory and compute returns \\(G_t\\), the REINFORCE update rule applies: \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha G_t \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t|s_t, \\mathbf{\\theta})\\]\nLet‚Äôs look at a specific component of this update, say for \\(\\mathbf{\\theta}_{-1}\\): \\[\\mathbf{\\theta}_{-1} \\leftarrow \\mathbf{\\theta}_{-1} + \\alpha G_t \\nabla_{\\mathbf{\\theta}_{-1}} \\ln \\pi(a_t|s_t, \\mathbf{\\theta})\\]\nIf \\(a_t = -1\\), and \\(G_t\\) is high, the gradient \\(\\nabla_{\\mathbf{\\theta}_{-1}} \\ln \\pi(-1|s_t, \\mathbf{\\theta})\\) will push \\(\\mathbf{\\theta}_{-1}\\) to increase the score for action -1 when in state \\(s_t\\). Since \\(h(s, a, \\mathbf{\\theta}_a) = \\mathbf{\\theta}_a^T \\mathbf{x}(s)\\), the gradient with respect to \\(\\mathbf{\\theta}_a\\) is simply \\(\\mathbf{x}(s)\\). So, the update to \\(\\mathbf{\\theta}_{-1}\\) will be proportional to \\(\\mathbf{x}(s_t)\\). This means:\n\nIf \\(s_t\\) is very similar to \\(c_1\\) (so \\(\\phi_1(s_t)\\) is high), then \\(w_{-1,1}\\) (the weight for \\(c_1\\) and action -1) will be adjusted significantly.\nIf \\(s_t\\) is far from \\(c_1\\) (so \\(\\phi_1(s_t)\\) is near zero), then \\(w_{-1,1}\\) will be adjusted very little by this particular sample.\n\nThis means the learning process adjusts the weights for each proto point for each action based on the returns received.\n\n\nAre Probabilities Fixed When Starting with Proto Points?\nNo, the probabilities of actions are not fixed when you start with proto points.\n\nFixed: The proto points \\(c_j\\) themselves are fixed in the state space, and the basis functions \\(\\phi_j(s)\\) are fixed (their shape and location are determined at the start).\nLearned: However, the parameters \\(\\mathbf{\\theta}\\) (the weights associated with each proto point for each action) are learnable.\n\nThe idea of relying action parameter \\(\\theta\\) on fixed state representation \\(x(s)\\) is because action \\(a\\) depends on the state \\((a|s)\\), so we need to fix states representation first, use it as a basis so that action parameter can learn."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#a-monte-carlo-style-policy-gradient-algorithm",
    "href": "posts/ml4r-reinforcement-learning/index.html#a-monte-carlo-style-policy-gradient-algorithm",
    "title": "Model-free Reinforcement Learning",
    "section": "A Monte Carlo Style Policy Gradient Algorithm",
    "text": "A Monte Carlo Style Policy Gradient Algorithm\nThe core idea is to increase the probability of actions that lead to high returns and decrease the probability of actions that lead to low returns.\nInitialize:\n\nFunctional form for the policy: \\(\\pi(a|s, \\mathbf{\\theta})\\) (e.g., a neural network that outputs action probabilities given a state, parameterized by \\(\\mathbf{\\theta}\\)).\nInitial parameters: \\(\\mathbf{\\theta}\\) (e.g., randomly initialized weights for a neural network).\nStep size (learning rate): \\(\\alpha\\)\n\nAlgorithm:\nFor \\(m = 1, \\dots, M\\) (for each episode):\n\nSample a trajectory under the current policy \\(\\pi(\\cdot|\\cdot, \\mathbf{\\theta})\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\) (where \\(T_m\\) is the length of the episode).\nFor \\(t = 0, \\dots, T_m - 1\\) (for each time step in the trajectory):\n\nCompute the return from time \\(t\\): \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{T_m - t - 1} r_{T_m}^m\\) (This is the total discounted reward from \\(t+1\\) until the end of the episode).\nUpdate the policy parameters: \\[\n  \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t g_t^m \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\n  \\]\n\n\nExplanation of the Update Rule:\n\n\\(\\alpha\\): The learning rate, controlling the step size of the update.\n\\(\\gamma^t\\): The discount factor raised to the power of \\(t\\). This term accounts for the discounting of future rewards, ensuring that immediate rewards have a stronger influence on early actions.\n\\(g_t^m\\): Scaling stepsize with the Monte Carlo return from time step \\(t\\). e.g.¬†If return high =&gt; big step size, if return negative =&gt; step backward\n\\(\\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\): This is the gradient of the log-probability of the action taken.\n\n\\(\\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\): The logarithm of the probability that the policy \\(\\pi\\) would choose action \\(a_t^m\\) in state \\(s_t^m\\) with current parameters \\(\\mathbf{\\theta}\\). This is very important!!! It is a normalizer for cases like a random policy \\(\\pi\\) pick a positive (but low return) action too often, leading to pushing the probability too much, so by taking the gradient of \\(ln\\) of it, it limit down it, (and also the opposite for high reward actions but the behavior policy did not pick as many time to push enough higher). These situations usually happen with initialization of that lower reward already too high, and then add up that the policy random pick it too often pushing it even further.\n\\(\\nabla_{\\mathbf{\\theta}}\\): The gradient operator with respect to the policy parameters \\(\\mathbf{\\theta}\\). This term tells us how to change \\(\\mathbf{\\theta}\\) to increase the log-probability of taking action \\(a_t^m\\) in state \\(s_t^m\\).\n\n\nIntuition:\nThe update rule essentially says: if action \\(a_t^m\\) taken in state \\(s_t^m\\) leads to a high return (\\(g_t^m\\) is large and positive), then increase the probability of taking that action in that state. If it leads to a low (or negative) return, decrease its probability. The \\(\\gamma^t\\) term ensures that actions taken earlier in the trajectory, which influence more of the subsequent rewards, are given appropriate credit.\nREINFORCE is a fundamental algorithm that demonstrates the direct optimization of a policy, laying the groundwork for more advanced policy gradient methods."
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#variance-problem",
    "href": "posts/ml4r-reinforcement-learning/index.html#variance-problem",
    "title": "Model-free Reinforcement Learning",
    "section": "Variance Problem",
    "text": "Variance Problem\nReturn \\(g_t^m\\) rewards the action when it is positive, and punish the action when it is negative. But some cases all actions can be positive, just some are less positive than other =&gt; it should not be encouraged.\nSolution: With baseline (e.g., \\(V_\\pi(s,w)\\)) =&gt; then we have \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t (g_t^m - b(s_t^m)) \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\] so that:\n\nif \\(g_t^m &gt; V_\\pi(s,w)\\), the scaling factor is positive =&gt; increase probability of action \\(a\\)\nif \\(g_t^m &lt; V_\\pi(s,w)\\), the scaling factor is negative =&gt; decrease probability of action \\(a\\)"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#sum-up-reinforce-gradient-policy-with-baseline",
    "href": "posts/ml4r-reinforcement-learning/index.html#sum-up-reinforce-gradient-policy-with-baseline",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: REINFORCE Gradient Policy with Baseline",
    "text": "Sum up: REINFORCE Gradient Policy with Baseline\nTo specify upfront:\n\nFunctional forms \\(\\pi(a|s, \\mathbf{\\theta})\\), \\(\\hat{v}(s, \\mathbf{w})\\)\nInitial \\(\\mathbf{\\theta}, \\mathbf{w}\\)\nStep sizes \\(\\alpha^{\\theta}, \\alpha^{w}\\)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample: \\(s_0^m, a_0^m, r_1^m \\dots a_{T_m-1}^m, r_{T_m}^m\\)\nFor \\(t = 0, \\dots, T_m - 1\\): \\[g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\]\n\n\\[\\delta \\leftarrow g_t^m - \\hat{v}(s_t^m, \\mathbf{w})\\]\n\\[\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^w \\delta \\nabla_{\\mathbf{w}} \\hat{v}(s_t^m, \\mathbf{w})\\]\n\\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^{\\theta} \\gamma^t \\delta \\nabla \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\]"
  },
  {
    "objectID": "posts/ml4r-reinforcement-learning/index.html#proximal-policy-optimization-ppo",
    "href": "posts/ml4r-reinforcement-learning/index.html#proximal-policy-optimization-ppo",
    "title": "Model-free Reinforcement Learning",
    "section": "Proximal Policy Optimization (PPO)",
    "text": "Proximal Policy Optimization (PPO)\nIt‚Äôs an on-policy, actor-critic method. Its main innovation lies in its clipped surrogate objective function, which aims to take the largest possible improvement step on a policy without causing a performance collapse.\nThe PPO policy objective to maximize is: \\[L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)\\right]\\]"
  },
  {
    "objectID": "posts/cg1-polygonalmesh/index.html",
    "href": "posts/cg1-polygonalmesh/index.html",
    "title": "Polygonal Mesh",
    "section": "",
    "text": "mesh ùëÄ is defined as a tripel (ùëâ, ùê∏, ùêπ)\nTopological type using betti numbers\nincident: different type, adjacent: same type\nvalence (given a vertex: number of incident edge/traingles?)\ndegree (given a face: number of incident edges?)\n\\(\\sum valences = 2edges = \\sum degrees\\) (because mostly one edge is shared by 2 vertices and 2 faces)\nwinding order is used for backface culling (when we add into the program)\n\n\n\nv -1 -1 1\nv 1 -1 1\nv -1 1 1\n...\nvt u1 v1\nvt u2 v2\nvt u3 v3\n...\nvn -1 0 0\nvn 1 0 0\nvn 0 -1 0\n...\nf 1/1/1 3/3/1 7/?/1 5/?/1\nf 2/2/2 6/6/2 8//2 4//2\n...\n\ndeep in hardware could be saved as:\n\narray: very fast for linear traversal, add and delete slower\ndouble linked-list: add and delete faster, but linear traversal slower\n\n\n\n\n\nGoal idea: resembling 2D plane everywhere on the surface:\n\neach edge is incident to 1 or 2 faces (if less: no face, if more: no clear surface definition, if 1: boundary)\neach edge is incident to 2 vertices (no rounded edge with its face in itself)\neach face is bounded by 1 loop over incident edges (if not: its a hole)\neach vertex: the incident faces and edges form a single ‚Äúfan‚Äù which is either open (on the boundary) or a closed disk (in the interior of the mesh), but not multiple fans.\n\nDS are mostly restricted on this because:\n\nReliably deterministic in operation (e.g.¬†each edge only has 1,2 faces at 2 sides which create a clear surface for traversal) =&gt; avoid ambigioiusity\nNaturally it is able to model most of 3D shapes\n\n\n\n\n\nclassify all corners in convex or concave (already knew from internal angles)\nrepeat n-3 times: (n loop)\n\none ear-cut: iterate all convex corners (corner hear means vertex and the edges at two sides): (n loop)\n\ncheck, if corner is an ear (if it contains no concave inside the border line)\nif yes, cut-off ear, reclassify adjacent corners, break this loop to go to the next eat-cut\n\n\n\n\n\n\n\nTriangulated using Ear-Cutting\n\\(2e = 3f\\) or \\(\\frac{3}{2} f = e\\) bc each faces has 3 edges, but each edge is used double\nEuler formula: \\(v-e+f=X\\), for X very small =&gt; \\(v = \\frac{1}{3} e = \\frac{1}{2} f\\)\nvalence: \\(\\sum valences = 2e = 6v\\)\n\n\n\n\n\n\n\n\nFor triangle: \\(\\hat{n} = (p_1 - p_0) \\times (p_2 - p_0)\\) and \\(A = \\frac{1}{2} \\left| p_0 \\times p_1 + p_1 \\times p_2 + p_2 \\times p_0 \\right|\\)\nFor Polygon: \\(\\hat{n} \\propto (p_0 \\times p_1) + (p_1 \\times p_2) + \\ldots + (p_{d-1} \\times p_0)\\) and \\(A = \\frac{1}{2} \\left| (p_0 \\times p_1) + (p_1 \\times p_2) + \\ldots + (p_{d-1} \\times p_0) \\right|\\)\n\nGeneral idea for Area:\n\nChoose a reference point, typically the origin (0, 0).\nIterate through each directed edges (p_i, p_j) of the polygon, calculate the signed area of the triangle it forms with the origin using the z-component of the cross product: \\(\\Delta A = \\frac{1}{2} (x_i y_j - x_j y_i)\\). (It is represented by a normal pointing inside in the winding order aka right-hand rule, some shorter, some longer will sum up to the correct direction)\nSum all these signed areas \\(A = \\sum \\Delta A\\). Concave (negative area) and Convex (positive area)\n\nAdvantage:\n\nNo need ear-cutting: O(n¬≤)\nTaking care of concave triangles\n\n\n\n\nIdea comes from Divergence Theorem: sum of all the ‚Äúsources‚Äù and ‚Äúsinks‚Äù within a volume equal to the total amount of ‚Äústuff‚Äù flowing out (or in) through the surface boundary of that volume. \\(\\iint_S \\mathbf{F} \\cdot d\\mathbf{S} = \\iiint_V (\\nabla \\cdot \\mathbf{F}) dV\\)\nWhere:\n\n\\(\\mathbf{F}\\) is the vector field.\n\\(S\\) is a closed surface.\n\\(V\\) is the volume enclosed by \\(S\\).\n\\(d\\mathbf{S}\\) is an infinitesimal outward normal vector element of the surface.\n\\(\\nabla \\cdot \\mathbf{F}\\) is the divergence of the vector field \\(\\mathbf{F}\\).\n\nIn simpler terms:\n\nFlux (\\(\\iint_S \\mathbf{F} \\cdot d\\mathbf{S}\\)): This represents the ‚Äúflow‚Äù of the vector field out of the enclosed volume through its boundary surface. Imagine a fluid flowing; the flux would be the amount of fluid passing through the surface per unit time.\nDivergence (\\(\\nabla \\cdot \\mathbf{F}\\)): This measures the ‚Äúsource‚Äù or ‚Äúsink‚Äù density of the vector field at each point within the volume. A positive divergence indicates a source (where the field is originating or expanding), and a negative divergence indicates a sink (where the field is converging or disappearing).\n\nNow we apply that to Volume, each tetrahedron volume created by each triangle \\(\\frac{1}{6} \\mathbf{p}_i \\cdot (\\mathbf{p}_j \\times \\mathbf{p}_k)\\) (also called triple product)\nThen the total volume \\(V\\) is sum of tetrahedron volume: \\(V = \\frac{1}{6} \\sum_{(ijk) \\in T} [\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k]\\)\n\n\n\nLet‚Äôs start with the volume contribution of a tetrahedron formed by the origin and a triangle \\((\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k)\\):\n\\(V_{ijk} = \\frac{1}{6} \\mathbf{p}_k \\cdot (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\n\\(\\frac{\\partial V_{ijk}}{\\partial \\mathbf{p}_k} = \\frac{\\partial}{\\partial \\mathbf{p}_k} \\left( \\frac{1}{6} \\mathbf{p}_i \\cdot (\\mathbf{p}_j \\times \\mathbf{p}_k) \\right)\\) (volume with respect to the coordinates of a specific vertex \\(\\frac{\\partial V}{\\partial \\mathbf{p}_k}\\))\n\\(\\frac{\\partial V_{ijk}}{\\partial \\mathbf{p}_k} = \\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\nUntil now what this means?\n\nIt means much the volume of that specific tetrahedron changes if we move vertex \\(\\mathbf{p}_k\\) by a small amount.\nThe result, \\(\\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\), is a vector. This vector is normal to the plane formed by \\(\\mathbf{p}_i\\) and \\(\\mathbf{p}_j\\), and its magnitude is related to the area of the triangle formed by \\(\\mathbf{p}_i\\), \\(\\mathbf{p}_j\\), and the origin.\n\nFor the total volume \\(V\\), the gradient with respect to a vertex \\(\\mathbf{p}_k\\) is the sum of the gradients of all tetrahedra one-ring around \\(\\mathbf{p}_k\\):\n\\(\\nabla_{\\mathbf{p}_k} V = \\sum_{\\text{one-ring } (\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k)} \\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\nThis gradient indicates the direction in which moving the vertex would most effectively increase the total mesh volume.\n\n\n\n\\(\\hat{n}_v \\propto \\sum_i w_i \\hat{n}_i\\)\nImplementation is interestingly reversed: because faces and vertices are jointly incident to each other\nnormals.resize(positions.size()); // init normals array with length equal to number of vertices\nforeach (fi in meshfaces) // iterate over all faces\n  nml = get_face_normal(fi); // get the normal for this face\n  foreach (corner ci in face fi) // iterate over all vertices of this face\n    normals[vi(ci)] += nml *compute_corner_weight(ci); // add the weighted normal to the corresponding normal array index\n\n\n\n\\(c_v = \\frac{1}{n_{onering}} \\sum_i p_i\\) Implementaton also a little reversed, hell yeah, because all vertices are adjacent to each other\ncentroids.resize(positions.size()); //init\nvalences.resize(positions.size());\nforeach (fi in meshfaces) //for each face\n  foreach (halfedge vi,vj in face fi) // iterate three halfedges (vi,vj)\n    centroids[vi] += positions[vj];\n    valences[vi] += 1;\n\n\n\n\n\n\n\nShells are connected components\nBoundary Loops are border/edges\n\n\n\n\nThis DS can be used to find shells three operations: * initialize UF-DS such that each element forms its own subset * union(element1, element2): Merges the sets containing the two elements. * find(element): Returns a unique identifier for the set that the element belongs to.\nAlgorithm to find shells and count shells\nint nr_comp = vertices.size(); \nUF.init(vertices.size()); // init to component/set per vertex\nforeach (fi) // iterate face\n  foreach (halfedge vi,vj in face fi) //iterate HE\n  if (UF.find(vi) != UF.find(vj))\n    UF.union(vi,vj); //unite\n    nr_comp -= 1; //reduce comp by 1"
  },
  {
    "objectID": "posts/cg1-polygonalmesh/index.html#basics",
    "href": "posts/cg1-polygonalmesh/index.html#basics",
    "title": "Polygonal Mesh",
    "section": "",
    "text": "mesh ùëÄ is defined as a tripel (ùëâ, ùê∏, ùêπ)\nTopological type using betti numbers\nincident: different type, adjacent: same type\nvalence (given a vertex: number of incident edge/traingles?)\ndegree (given a face: number of incident edges?)\n\\(\\sum valences = 2edges = \\sum degrees\\) (because mostly one edge is shared by 2 vertices and 2 faces)\nwinding order is used for backface culling (when we add into the program)\n\n\n\nv -1 -1 1\nv 1 -1 1\nv -1 1 1\n...\nvt u1 v1\nvt u2 v2\nvt u3 v3\n...\nvn -1 0 0\nvn 1 0 0\nvn 0 -1 0\n...\nf 1/1/1 3/3/1 7/?/1 5/?/1\nf 2/2/2 6/6/2 8//2 4//2\n...\n\ndeep in hardware could be saved as:\n\narray: very fast for linear traversal, add and delete slower\ndouble linked-list: add and delete faster, but linear traversal slower\n\n\n\n\n\nGoal idea: resembling 2D plane everywhere on the surface:\n\neach edge is incident to 1 or 2 faces (if less: no face, if more: no clear surface definition, if 1: boundary)\neach edge is incident to 2 vertices (no rounded edge with its face in itself)\neach face is bounded by 1 loop over incident edges (if not: its a hole)\neach vertex: the incident faces and edges form a single ‚Äúfan‚Äù which is either open (on the boundary) or a closed disk (in the interior of the mesh), but not multiple fans.\n\nDS are mostly restricted on this because:\n\nReliably deterministic in operation (e.g.¬†each edge only has 1,2 faces at 2 sides which create a clear surface for traversal) =&gt; avoid ambigioiusity\nNaturally it is able to model most of 3D shapes\n\n\n\n\n\nclassify all corners in convex or concave (already knew from internal angles)\nrepeat n-3 times: (n loop)\n\none ear-cut: iterate all convex corners (corner hear means vertex and the edges at two sides): (n loop)\n\ncheck, if corner is an ear (if it contains no concave inside the border line)\nif yes, cut-off ear, reclassify adjacent corners, break this loop to go to the next eat-cut\n\n\n\n\n\n\n\nTriangulated using Ear-Cutting\n\\(2e = 3f\\) or \\(\\frac{3}{2} f = e\\) bc each faces has 3 edges, but each edge is used double\nEuler formula: \\(v-e+f=X\\), for X very small =&gt; \\(v = \\frac{1}{3} e = \\frac{1}{2} f\\)\nvalence: \\(\\sum valences = 2e = 6v\\)"
  },
  {
    "objectID": "posts/cg1-polygonalmesh/index.html#geometry-analysis",
    "href": "posts/cg1-polygonalmesh/index.html#geometry-analysis",
    "title": "Polygonal Mesh",
    "section": "",
    "text": "For triangle: \\(\\hat{n} = (p_1 - p_0) \\times (p_2 - p_0)\\) and \\(A = \\frac{1}{2} \\left| p_0 \\times p_1 + p_1 \\times p_2 + p_2 \\times p_0 \\right|\\)\nFor Polygon: \\(\\hat{n} \\propto (p_0 \\times p_1) + (p_1 \\times p_2) + \\ldots + (p_{d-1} \\times p_0)\\) and \\(A = \\frac{1}{2} \\left| (p_0 \\times p_1) + (p_1 \\times p_2) + \\ldots + (p_{d-1} \\times p_0) \\right|\\)\n\nGeneral idea for Area:\n\nChoose a reference point, typically the origin (0, 0).\nIterate through each directed edges (p_i, p_j) of the polygon, calculate the signed area of the triangle it forms with the origin using the z-component of the cross product: \\(\\Delta A = \\frac{1}{2} (x_i y_j - x_j y_i)\\). (It is represented by a normal pointing inside in the winding order aka right-hand rule, some shorter, some longer will sum up to the correct direction)\nSum all these signed areas \\(A = \\sum \\Delta A\\). Concave (negative area) and Convex (positive area)\n\nAdvantage:\n\nNo need ear-cutting: O(n¬≤)\nTaking care of concave triangles\n\n\n\n\nIdea comes from Divergence Theorem: sum of all the ‚Äúsources‚Äù and ‚Äúsinks‚Äù within a volume equal to the total amount of ‚Äústuff‚Äù flowing out (or in) through the surface boundary of that volume. \\(\\iint_S \\mathbf{F} \\cdot d\\mathbf{S} = \\iiint_V (\\nabla \\cdot \\mathbf{F}) dV\\)\nWhere:\n\n\\(\\mathbf{F}\\) is the vector field.\n\\(S\\) is a closed surface.\n\\(V\\) is the volume enclosed by \\(S\\).\n\\(d\\mathbf{S}\\) is an infinitesimal outward normal vector element of the surface.\n\\(\\nabla \\cdot \\mathbf{F}\\) is the divergence of the vector field \\(\\mathbf{F}\\).\n\nIn simpler terms:\n\nFlux (\\(\\iint_S \\mathbf{F} \\cdot d\\mathbf{S}\\)): This represents the ‚Äúflow‚Äù of the vector field out of the enclosed volume through its boundary surface. Imagine a fluid flowing; the flux would be the amount of fluid passing through the surface per unit time.\nDivergence (\\(\\nabla \\cdot \\mathbf{F}\\)): This measures the ‚Äúsource‚Äù or ‚Äúsink‚Äù density of the vector field at each point within the volume. A positive divergence indicates a source (where the field is originating or expanding), and a negative divergence indicates a sink (where the field is converging or disappearing).\n\nNow we apply that to Volume, each tetrahedron volume created by each triangle \\(\\frac{1}{6} \\mathbf{p}_i \\cdot (\\mathbf{p}_j \\times \\mathbf{p}_k)\\) (also called triple product)\nThen the total volume \\(V\\) is sum of tetrahedron volume: \\(V = \\frac{1}{6} \\sum_{(ijk) \\in T} [\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k]\\)\n\n\n\nLet‚Äôs start with the volume contribution of a tetrahedron formed by the origin and a triangle \\((\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k)\\):\n\\(V_{ijk} = \\frac{1}{6} \\mathbf{p}_k \\cdot (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\n\\(\\frac{\\partial V_{ijk}}{\\partial \\mathbf{p}_k} = \\frac{\\partial}{\\partial \\mathbf{p}_k} \\left( \\frac{1}{6} \\mathbf{p}_i \\cdot (\\mathbf{p}_j \\times \\mathbf{p}_k) \\right)\\) (volume with respect to the coordinates of a specific vertex \\(\\frac{\\partial V}{\\partial \\mathbf{p}_k}\\))\n\\(\\frac{\\partial V_{ijk}}{\\partial \\mathbf{p}_k} = \\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\nUntil now what this means?\n\nIt means much the volume of that specific tetrahedron changes if we move vertex \\(\\mathbf{p}_k\\) by a small amount.\nThe result, \\(\\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\), is a vector. This vector is normal to the plane formed by \\(\\mathbf{p}_i\\) and \\(\\mathbf{p}_j\\), and its magnitude is related to the area of the triangle formed by \\(\\mathbf{p}_i\\), \\(\\mathbf{p}_j\\), and the origin.\n\nFor the total volume \\(V\\), the gradient with respect to a vertex \\(\\mathbf{p}_k\\) is the sum of the gradients of all tetrahedra one-ring around \\(\\mathbf{p}_k\\):\n\\(\\nabla_{\\mathbf{p}_k} V = \\sum_{\\text{one-ring } (\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k)} \\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\nThis gradient indicates the direction in which moving the vertex would most effectively increase the total mesh volume.\n\n\n\n\\(\\hat{n}_v \\propto \\sum_i w_i \\hat{n}_i\\)\nImplementation is interestingly reversed: because faces and vertices are jointly incident to each other\nnormals.resize(positions.size()); // init normals array with length equal to number of vertices\nforeach (fi in meshfaces) // iterate over all faces\n  nml = get_face_normal(fi); // get the normal for this face\n  foreach (corner ci in face fi) // iterate over all vertices of this face\n    normals[vi(ci)] += nml *compute_corner_weight(ci); // add the weighted normal to the corresponding normal array index\n\n\n\n\\(c_v = \\frac{1}{n_{onering}} \\sum_i p_i\\) Implementaton also a little reversed, hell yeah, because all vertices are adjacent to each other\ncentroids.resize(positions.size()); //init\nvalences.resize(positions.size());\nforeach (fi in meshfaces) //for each face\n  foreach (halfedge vi,vj in face fi) // iterate three halfedges (vi,vj)\n    centroids[vi] += positions[vj];\n    valences[vi] += 1;"
  },
  {
    "objectID": "posts/cg1-polygonalmesh/index.html#connectivity-analysis",
    "href": "posts/cg1-polygonalmesh/index.html#connectivity-analysis",
    "title": "Polygonal Mesh",
    "section": "",
    "text": "Shells are connected components\nBoundary Loops are border/edges\n\n\n\n\nThis DS can be used to find shells three operations: * initialize UF-DS such that each element forms its own subset * union(element1, element2): Merges the sets containing the two elements. * find(element): Returns a unique identifier for the set that the element belongs to.\nAlgorithm to find shells and count shells\nint nr_comp = vertices.size(); \nUF.init(vertices.size()); // init to component/set per vertex\nforeach (fi) // iterate face\n  foreach (halfedge vi,vj in face fi) //iterate HE\n  if (UF.find(vi) != UF.find(vj))\n    UF.union(vi,vj); //unite\n    nr_comp -= 1; //reduce comp by 1"
  },
  {
    "objectID": "posts/cg1-polygonalmesh/index.html#ds-construction",
    "href": "posts/cg1-polygonalmesh/index.html#ds-construction",
    "title": "Polygonal Mesh",
    "section": "DS Construction",
    "text": "DS Construction\n\nAdjacent relationship: next, prev, inv\nIncident relationship: origin, loop/face, edge\n\nstruct vertex {\n  vec3d position;\n  int halfedge;  //stores only 1 HE, the rest can be found by inv(next) like below\n};\nstruct halfedge {\n  int origin,ni,ti; (pointer-based trade off sequential access vs single access vs memory)\n  int next,inv,li;\n};\nstruct loop {\n  int halfedge; //stores only 1 HE, the rest can be found by next\n  bool is_border;\n};\nstruct halfedge_mesh {\nvector&lt;vertex&gt; vertices;\nvector&lt;vec3d&gt; normals;\nvector&lt;vec2d&gt; texCoords;\nvector&lt;halfedge&gt; halfedges;\nvector&lt;loop&gt; loops;\n};\n\nSo, from an OBJ. file, what of these relationship can be directly inferred?\n\nactually everything when reading the face: origin, next, loop BUT EXCEPT FOR inv, that neads to find what face adjacent to current face\n\n\n\nVertex Circulator:\nint h0 = inv(vertexHalfEdge(vi)); //usually start with the inverse\nint hi = h0;\ndo {\n  useNeighbor(origin(hi));\n  hi = inv(next(hi)); //always inv(next)\n} while (hi != h0); //until back to beginning\n\n\nInverse Matching\n\nGoal: finding the inv pointer for every halfedge in the mesh by placing them next to each other\nIdea: Easy:\n\nSort internally v1 -&gt; v2 vs v2 -&gt; v1 in format smaller-&gt;bigger\nSort the indices along the smaller column\nEnd effect:\n\n\nHE exists twice are matched, they are internal\nHE exists once are boundary\nHE exists more than twice are non-manifold\n\nThis way we can extract Boundary Loop by for each unmatched HE, define all the inverse, and the inverse keep traversing next until it come back to the beginning"
  },
  {
    "objectID": "posts/cg1-polygonalmesh/index.html#mesh-analysis",
    "href": "posts/cg1-polygonalmesh/index.html#mesh-analysis",
    "title": "Polygonal Mesh",
    "section": "Mesh Analysis",
    "text": "Mesh Analysis\n\nDesign Pattern for Design Mesh Processing Algorithm\n\nCirculator (around vertex, face, vertex in face)\nTagging (mark processed elements or store id/flag)\nRegion Growing (triangle strip, compressed segmentation)\n\n\n\nWhy use Triangle Strips for Rendering?\n\nTransfer: only need to transfer n+2 vertices for a strip of n triangles (the first one all 3, after that only 1 for each new triangle)\nAccess: for triangle mesh we all know \\(f = 2v\\), triangle strip can utilize this optimum, but without strip, each face individually require separate 3 times access to their vertices. So \\(f = 2.3v\\)\nDownside? The algorithm is not always perfect (its just greedy)\n\n\n\nHow to compute Triangle Strip?\nOutput Goal: strip index per face 1. Sample some seed 2. For each seed, generate a stripification 3. Here is how to stripification: * 2 types of HE, (even - odd) using parity (0-1) depending on how the origin aligned. * Forward: prev(inv(vi)) or next(inv(vi)) * Backward: ‚Ä¶ This run alternatively‚Ä¶change parity 0-1-0-1-0-1 Until Border or Comeback to beginning 4. Choose the longest strip from seed 5. Repeat with new set of seeds\n\n\nHow to do Orientability Check?\nRegion Growing: Start at a seed to queue 1. Take the first face from queue 2. Check if orientable? 1. check if all neighboring processed faces have consistent orientation 2. If no, give up. But if yes, swap next and inv 3. Add all neighbors to queue"
  },
  {
    "objectID": "posts/cg1-polygonalmesh/index.html#manifold-if-can-always-find-a-disk-or-a-half-disk",
    "href": "posts/cg1-polygonalmesh/index.html#manifold-if-can-always-find-a-disk-or-a-half-disk",
    "title": "Polygonal Mesh",
    "section": "2-Manifold if can always find a disk or a half disk",
    "text": "2-Manifold if can always find a disk or a half disk"
  },
  {
    "objectID": "posts/cg1-polygonalmesh/index.html#understanding-the-euler-characteristic",
    "href": "posts/cg1-polygonalmesh/index.html#understanding-the-euler-characteristic",
    "title": "Polygonal Mesh",
    "section": "Understanding the Euler Characteristic",
    "text": "Understanding the Euler Characteristic\n\n1. Basic Definition\nFor any closed (borderless) surface S subdivided into vertices (V), edges (E), and faces (F):\n\\[\n\\chi(S) = V - E + F\n\\]\nKey Property: - œá is a topological invariant - it depends only on the surface‚Äôs ‚Äúshape‚Äù (topology), not on how we subdivide it.\n\n\n2. Examples\n\n\n\nSurface\nV - E + F\nœá\n\n\n\n\nSphere\n8-12+6 = 2 (cube)  4-6+4 = 2 (tetrahedron)\n2\n\n\nTorus\n16-32+16 = 0\n0\n\n\nDouble Torus\n8-16+8 = 0\n0 ?\n\n\n\n\n\n3. Topological Meaning\nThe Euler characteristic can be expressed via Betti numbers (counting topological features):\n\\[\n\\chi = h_0 - h_1 + h_2\n\\]\n\n\\(h_0\\): 0-Dimensional Holes (Connected Components)\n\nCounts the number of separate pieces in the surface\nFor a single connected surface: \\(h_0 = 1\\)\nExample: Both a sphere and torus have \\(h_0 = 1\\)\n\n\n\n\\(h_1\\): 1-Dimensional Holes (Non-Trivial Loops)\n\nCounts the maximum number of independent closed curves that:\n\nCannot be shrunk to a point\nDon‚Äôt split the surface when cut\n\nSphere: \\(h_1 = 0\\) (all loops can shrink)\nTorus: \\(h_1 = 2\\) (meridian and longitude loops)\nDouble Torus: \\(h_1 = 4\\)\n\n\n\n\\(h_2\\): Number of orientable shells\n\n\n\nSurface\n\\(h_0\\)\n\\(h_1\\)\n\\(h_2\\)\n\\(\\chi\\)\n\n\n\n\nSphere\n1\n0\n1\n2\n\n\nTorus\n1\n2\n1\n0\n\n\nDisk\n1\n0\n0\n1\n\n\n\n\n\n\n5. Geometric Interpretation\n\nPositive \\(\\chi\\): Sphere-like topology\nZero \\(\\chi\\): Torus-like topology\nNegative \\(\\chi\\): Higher-genus surfaces (multiple handles)\n\n\n\n5. Practical Applications\n\nMesh Processing:\n\nVerify mesh integrity (e.g., œá=2 for watertight meshes)\nDetect topological changes during simplification\n\nPhysics:\n\nGauss-Bonnet theorem relates œá to total curvature\nEssential in string theory and cosmology\n\nComputer Graphics:\n\nTexture mapping requires œá=2 for spherical topology\nCharacter modeling (ear/handles change topology)"
  },
  {
    "objectID": "posts/ml4r-control/index.html",
    "href": "posts/ml4r-control/index.html",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "A PID controller is a feedback control loop that continuously calculates an ‚Äúerror‚Äù value \\(e(t)\\) as the difference between a desired setpoint \\(r(t)\\) and a measured process variable \\(y(t)\\):\n\\[e(t) = r(t) - y(t)\\]\nBased on this error, the PID controller generates a control output \\(u(t)\\) by combining three distinct terms:\n\nProportional Term (\\(P\\)-term): Accounts for the current error.\nIntegral Term (\\(I\\)-term): Accounts for the accumulation of past errors.\nDerivative Term (\\(D\\)-term): Accounts for the rate of change of the error.\n\nCombining these, the continuous-time PID control law is given by:\n\\[u(t) = K_p e(t) + K_i \\int e(t) dt + K_d \\frac{de(t)}{dt}\\]\nWhere:\n\n\\(u(t)\\) is the controller‚Äôs output.\n\\(e(t)\\) is the error at time \\(t\\).\n\\(K_p\\) is the proportional gain.\n\\(K_i\\) is the integral gain.\n\\(K_d\\) is the derivative gain.\n\n\n\n\n\n\n\\[u_P(t) = K_p e(t)\\]\n\nPresent: If the error is large, the controller acts strongly; if the error is small, it acts weakly. =&gt; quickly drive the system towards the setpoint.\nGradual Loss Problem: There are some system where the output just naturally decays over time (like heat loss from a room, or friction in a motor). A simplified model could be, notice that for the output \\(y\\) to be maintained at a constant setpoint \\(r\\) (i.e., \\(\\frac{dy}{dt}=0\\)), the control input \\(u\\) must provide a continuous effort to compensate: \\(u_{required} = \\frac{ay}{b}\\). This system is the core problem:\n\\[\\frac{dy}{dt} = -ay + bu\\]\nSo what we require is that for this type of model to be at steady state, \\(\\frac{dy}{dt}=0 \\implies ay_{ss} = bu_{ss}\\).\nBut the problem is here!!! With P-control, we can only have \\(u_{ss} = K_p (r - y_{ss})\\) that kinda only acts based on the last timestep error. \\[ay_{ss} = bK_p (r - y_{ss})\\] \\[ay_{ss} = bK_pr - bK_py_{ss}\\] \\[y_{ss} = \\frac{K_p r}{(K_p + a/b)}\\]\nSince \\(a,b,K_p\\) are positive, \\(y_{ss}\\) will always be less than \\(r\\), meaning there will always be a non-zero steady-state error: \\(e_{ss} = r - y_{ss} \\ne 0\\).\nTherefore In this time of time-decaying systems, a sole P-Term simply cannot provide a sustained, non-zero output, simply because it only acts with the current error, and never act for the upcoming decay\n\n\n\n\n\\[u_I(t) = K_i \\int e(t) dt\\]\n\nCompensate the gradual Loss: Probably now you know what to do‚Ä¶ we push a little more, in such time-decaying system, we need continuous effort to to maintain the setpoint. It does this by continuously accumulating errors over time.\nAnalogy: You‚Äôre driving at 99 km/h when the limit is 100 km/h (small error). The P-term might give only a tiny gas pedal press. But you know, car on the street is exactly this type of time-decaying system (\\(\\frac{dy}{dt} = -ay + bu\\)) =&gt; To maintain 99 km/h for a long time, the I-term ‚Äúnotices‚Äù this persistent deficit and gradually pushes the gas pedal a little harder and holds it there until you finally reach 100 km/h.\nDrawback: The integral term can make the system slower to respond and potentially cause overshoot or oscillations if its gain \\(K_i\\) is set too high, because it‚Äôs reacting to past errors, not current or future ones.\nWhen does this accumulated stop? I would say almost never, because we have a time-decaying system, so we always need it.\nBUT, of course sometimes we want do stop overshooting it, therefore we have another term down here‚Ä¶\n\n\n\n\n\\[u_D(t) = K_d \\frac{de(t)}{dt}\\]\n\nAnticipation and Damping :This is really nice.\n\nIf the error is rapidly increasing (either negative or positive quantitatively), the D-term will counteract it quickly.\nIf the error is rapidly decreasing (meaning the system is approaching the setpoint quickly), the D-term will reduce the control action to prevent overshoot.\n\nAnalogy: You see a sharp turn (error changing rapidly) approaching in your car. You start braking before the turn to slow down smoothly and avoid overshooting the curve. Or, you‚Äôre speeding towards the 100 km/h limit; as you get closer, the D-term will gradually ease off the gas, preventing you from overshooting.\nBenefits: Reduces overshoot, reduces oscillations, and improves the transient response (how quickly and smoothly the system reaches the setpoint).\nDrawback: The D-term is very sensitive to noise in the measurement signal. Rapid changes in noisy signals can lead to large, jerky control actions."
  },
  {
    "objectID": "posts/ml4r-control/index.html#mathematical-formulation",
    "href": "posts/ml4r-control/index.html#mathematical-formulation",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "A PID controller is a feedback control loop that continuously calculates an ‚Äúerror‚Äù value \\(e(t)\\) as the difference between a desired setpoint \\(r(t)\\) and a measured process variable \\(y(t)\\):\n\\[e(t) = r(t) - y(t)\\]\nBased on this error, the PID controller generates a control output \\(u(t)\\) by combining three distinct terms:\n\nProportional Term (\\(P\\)-term): Accounts for the current error.\nIntegral Term (\\(I\\)-term): Accounts for the accumulation of past errors.\nDerivative Term (\\(D\\)-term): Accounts for the rate of change of the error.\n\nCombining these, the continuous-time PID control law is given by:\n\\[u(t) = K_p e(t) + K_i \\int e(t) dt + K_d \\frac{de(t)}{dt}\\]\nWhere:\n\n\\(u(t)\\) is the controller‚Äôs output.\n\\(e(t)\\) is the error at time \\(t\\).\n\\(K_p\\) is the proportional gain.\n\\(K_i\\) is the integral gain.\n\\(K_d\\) is the derivative gain."
  },
  {
    "objectID": "posts/ml4r-control/index.html#but-why-p-i-and-d-why-not-just-use-the-current-error",
    "href": "posts/ml4r-control/index.html#but-why-p-i-and-d-why-not-just-use-the-current-error",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "\\[u_P(t) = K_p e(t)\\]\n\nPresent: If the error is large, the controller acts strongly; if the error is small, it acts weakly. =&gt; quickly drive the system towards the setpoint.\nGradual Loss Problem: There are some system where the output just naturally decays over time (like heat loss from a room, or friction in a motor). A simplified model could be, notice that for the output \\(y\\) to be maintained at a constant setpoint \\(r\\) (i.e., \\(\\frac{dy}{dt}=0\\)), the control input \\(u\\) must provide a continuous effort to compensate: \\(u_{required} = \\frac{ay}{b}\\). This system is the core problem:\n\\[\\frac{dy}{dt} = -ay + bu\\]\nSo what we require is that for this type of model to be at steady state, \\(\\frac{dy}{dt}=0 \\implies ay_{ss} = bu_{ss}\\).\nBut the problem is here!!! With P-control, we can only have \\(u_{ss} = K_p (r - y_{ss})\\) that kinda only acts based on the last timestep error. \\[ay_{ss} = bK_p (r - y_{ss})\\] \\[ay_{ss} = bK_pr - bK_py_{ss}\\] \\[y_{ss} = \\frac{K_p r}{(K_p + a/b)}\\]\nSince \\(a,b,K_p\\) are positive, \\(y_{ss}\\) will always be less than \\(r\\), meaning there will always be a non-zero steady-state error: \\(e_{ss} = r - y_{ss} \\ne 0\\).\nTherefore In this time of time-decaying systems, a sole P-Term simply cannot provide a sustained, non-zero output, simply because it only acts with the current error, and never act for the upcoming decay\n\n\n\n\n\\[u_I(t) = K_i \\int e(t) dt\\]\n\nCompensate the gradual Loss: Probably now you know what to do‚Ä¶ we push a little more, in such time-decaying system, we need continuous effort to to maintain the setpoint. It does this by continuously accumulating errors over time.\nAnalogy: You‚Äôre driving at 99 km/h when the limit is 100 km/h (small error). The P-term might give only a tiny gas pedal press. But you know, car on the street is exactly this type of time-decaying system (\\(\\frac{dy}{dt} = -ay + bu\\)) =&gt; To maintain 99 km/h for a long time, the I-term ‚Äúnotices‚Äù this persistent deficit and gradually pushes the gas pedal a little harder and holds it there until you finally reach 100 km/h.\nDrawback: The integral term can make the system slower to respond and potentially cause overshoot or oscillations if its gain \\(K_i\\) is set too high, because it‚Äôs reacting to past errors, not current or future ones.\nWhen does this accumulated stop? I would say almost never, because we have a time-decaying system, so we always need it.\nBUT, of course sometimes we want do stop overshooting it, therefore we have another term down here‚Ä¶\n\n\n\n\n\\[u_D(t) = K_d \\frac{de(t)}{dt}\\]\n\nAnticipation and Damping :This is really nice.\n\nIf the error is rapidly increasing (either negative or positive quantitatively), the D-term will counteract it quickly.\nIf the error is rapidly decreasing (meaning the system is approaching the setpoint quickly), the D-term will reduce the control action to prevent overshoot.\n\nAnalogy: You see a sharp turn (error changing rapidly) approaching in your car. You start braking before the turn to slow down smoothly and avoid overshooting the curve. Or, you‚Äôre speeding towards the 100 km/h limit; as you get closer, the D-term will gradually ease off the gas, preventing you from overshooting.\nBenefits: Reduces overshoot, reduces oscillations, and improves the transient response (how quickly and smoothly the system reaches the setpoint).\nDrawback: The D-term is very sensitive to noise in the measurement signal. Rapid changes in noisy signals can lead to large, jerky control actions."
  },
  {
    "objectID": "posts/ml4r-control/index.html#example-lti-system-the-mass-spring-damper",
    "href": "posts/ml4r-control/index.html#example-lti-system-the-mass-spring-damper",
    "title": "From Control to Model-based Learning",
    "section": "Example LTI System: The Mass-Spring-Damper",
    "text": "Example LTI System: The Mass-Spring-Damper\nSystem Description: This is a common second-order system. Consider a mass \\(m\\) (kg) connected to a spring with stiffness \\(k\\) (N/m) and a damper with damping coefficient \\(b\\) (Ns/m). An external force \\(F(t)\\) (N) is applied to the mass, causing a displacement \\(y(t)\\) (m) from its equilibrium position.\n1. Governing Differential Equation: Applying Newton‚Äôs Second Law (\\(\\sum F = m \\ddot{y}\\)) to the mass:\n\nApplied force: \\(+F(t)\\)\nSpring force (restoring): \\(-k y(t)\\)\nDamping force (opposing velocity): \\(-b \\dot{y}(t)\\)\n\nSo, we have now a second-order linear ordinary differential equation: \\[m\\ddot{y}(t) + b\\dot{y}(t) + k y(t) = F(t)\\] \\[m\\ddot{y}(t) = F(t) - b\\dot{y}(t) - k y(t)\\]\n2. Converting to State-Space Form: To convert this second-order equation into the first-order state-space form, we define state variables. A common choice is to pick the position and velocity as states:\n\n\\(x_1(t) = y(t)\\) (the position of the mass)\n\\(x_2(t) = \\dot{y}(t)\\) (the velocity of the mass)\n\nNow, we need to express the derivatives of these state variables in terms of the states themselves and the input \\(F(t)\\). \\[\\dot{x_1}(t) = \\dot{y}(t) = x_2(t)\\] \\[\\ddot{y}(t) = \\frac{1}{m} F(t) - \\frac{b}{m}\\dot{y}(t) - \\frac{k}{m}y(t)\\] Substituting our state variables (\\(y(t)=x_1(t)\\) and \\(\\dot{y}(t)=x_2(t)\\)) and our input \\(u(t) = F(t)\\): \\[\\dot{x_2}(t) = -\\frac{k}{m}x_1(t) - \\frac{b}{m}x_2(t) + \\frac{1}{m}u(t)\\]\n3. Writing in Matrix Form: Now, we can assemble these first-order equations into the state-space matrix form \\(\\dot{\\mathbf{x}}(t)=\\mathbf{A}\\mathbf{x}(t)+\\mathbf{B}\\mathbf{u}(t)\\):\n\\[\\begin{pmatrix} \\dot{x_1}(t) \\\\ \\dot{x_2}(t) \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -\\frac{k}{m} & -\\frac{b}{m} \\end{pmatrix} \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{1}{m} \\end{pmatrix} u(t)\\]\nFrom this, we can identify the statematrix \\(\\mathbf{A}\\) and input matrix \\(\\mathbf{B}\\): \\[\\mathbf{A} = \\begin{pmatrix} 0 & 1 \\\\ -\\frac{k}{m} & -\\frac{b}{m} \\end{pmatrix}, \\mathbf{B} = \\begin{pmatrix} 0 \\\\ \\frac{1}{m} \\end{pmatrix}\\]"
  },
  {
    "objectID": "posts/ml4r-control/index.html#formulate-into-integral-of-terms-using-x-and-u",
    "href": "posts/ml4r-control/index.html#formulate-into-integral-of-terms-using-x-and-u",
    "title": "From Control to Model-based Learning",
    "section": "Formulate into integral of terms using x and u",
    "text": "Formulate into integral of terms using x and u\nThe formal LQR problem is to find the optimal control input \\(\\mathbf{u}^*(t)\\) that minimizes the cost function \\(J\\), subject to the system dynamics:\n\\[\\text{Minimize } J = \\int_0^\\infty (\\mathbf{x}^T(t)\\mathbf{Q}\\mathbf{x}(t) + \\mathbf{u}^T(t)\\mathbf{R}\\mathbf{u}(t)) dt\\] \\[\\text{Subject to: } \\dot{\\mathbf{x}}(t) = \\mathbf{A}\\mathbf{x}(t) + \\mathbf{B}\\mathbf{u}(t)\\]\nOur goal is now, choosing the weighting matrices \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\), we can tune the controller to prioritize different aspects of performance:\n\nLarger \\(\\mathbf{Q}\\): driving states to zero quickly\nLarger \\(\\mathbf{R}\\): minimizing control effort\n\nYou know what? LQR is a linear state-feedback control system, so it people from long time ago has found out it also satisfies this form:\n\\[\\mathbf{u}(t) = -\\mathbf{K}\\mathbf{x}(t)\\]\nwell, we do not know what \\(K\\) is, we need to find \\(P\\) to calculate \\(K\\):\n\\[\\mathbf{K} = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\]\nBut \\(\\mathbf{P} \\in \\mathbb{R}^{n \\times n}\\) is also sth that we need to find.\nPeople long time ago just started by adding and subtracting \\(\\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0\\) from \\(J\\) to see what they can explore from here:\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 - \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty (\\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u}) dt\\]\nSubstituting the integral form of \\(-\\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 = \\int_0^\\infty ( \\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}))dt\\):\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) + \\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u} \\right) dt\\]\nNow we derive \\(\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x})\\). Since \\(\\mathbf{P}\\) is a constant, symmetric matrix (\\(\\mathbf{P} = \\mathbf{P}^T\\)):\n\\[\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) = \\dot{\\mathbf{x}}^T\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\dot{\\mathbf{x}}\\]\nNow, substitute \\(\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u}\\) into this expression:\n\\[\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) = (\\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u})^T\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}(\\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u})\\] \\[\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) = (\\mathbf{x}^T\\mathbf{A}^T + \\mathbf{u}^T\\mathbf{B}^T)\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{A}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u}\\] \\[\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) = \\mathbf{x}^T\\mathbf{A}^T\\mathbf{P}\\mathbf{x} + \\mathbf{u}^T\\mathbf{B}^T\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{A}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u}\\]\nSubstitute this back into the expression for \\(J\\):\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\mathbf{x}^T\\mathbf{A}^T\\mathbf{P}\\mathbf{x} + \\mathbf{u}^T\\mathbf{B}^T\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{A}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u} + \\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u} \\right) dt\\]\nNow, let‚Äôs group the terms. We can gather terms involving \\(\\mathbf{x}^T (\\cdot) \\mathbf{x}\\) and terms involving \\(\\mathbf{u}\\). Note that \\(\\mathbf{u}^T\\mathbf{B}^T\\mathbf{P}\\mathbf{x}\\) and \\(\\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u}\\) are scalars and transposes of each other (and since \\(\\mathbf{P}=\\mathbf{P}^T\\)), they are equal. So their sum is \\(2\\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u}\\).\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\mathbf{x}^T(\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q})\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u} + 2\\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u} \\right) dt\\]\nNow, we want to rewrite the terms dependent on \\(\\mathbf{u}\\) into a perfect square like:\n\\[(a + b)¬≤ = a¬≤ + 2ab + b¬≤\\] \\[a¬≤ + 2ab = (a + b)¬≤ - b¬≤\\]\nwe somehow figuredout it looks like this, I also cannot derive it how, but thats the result:\n\\[\\mathbf{u}^T\\mathbf{R}\\mathbf{u} + 2\\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u} = (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})^T \\mathbf{R} (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}) - \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}\\]\nSubstitute this back into the expression for \\(J\\):\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\mathbf{x}^T(\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q})\\mathbf{x} + (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})^T \\mathbf{R} (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}) - \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x} \\right) dt\\]\n\nImportant part\nFinally, group the \\(\\mathbf{x}^T(\\cdot)\\mathbf{x}\\) terms AGAIN, we did this twice aigoo:\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\mathbf{x}^T(\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P})\\mathbf{x} + (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})^T \\mathbf{R} (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}) \\right) dt\\]\nTo minimize \\(J\\), we need to make the integral as small as possible. Let‚Äôs analyze the terms within the integral:\n\nThe term \\(\\mathbf{x}^T(\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P})\\mathbf{x}\\) depends only on the state \\(\\mathbf{x}\\), which is a consequence of the control.\nThe term \\((\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})^T \\mathbf{R} (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})\\) is a quadratic form involving \\(\\mathbf{u}\\). Since \\(\\mathbf{R}\\) is a positive definite matrix, this term is always greater than or equal to zero.\n\nTo minimize \\(J\\), we must choose \\(\\mathbf{u}\\) such that the second term in the integral is zero (its minimum possible value). This occurs when:\n\\[\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x} = \\mathbf{0}\\]\nImagine we already have a optimal control: \\[\\mathbf{u}^*(t) = -\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}(t)\\]\nImagine a specific case at a convergence, this \\(\\mathbf{u}^*(t)\\) term is only 0, only when \\(\\mathbf{x}\\) must also be zero. Therefore, interestingly:\n\\[\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P} = \\mathbf{0}\\]\nThis is the famous Algebraic Riccati Equation (ARE) which we can solve numerically.\nFInally Back to the Top: Now Calculate \\(\\mathbf{K}\\) using the obtained \\(\\mathbf{P}\\) to get optimal control \\(\\mathbf{u}(t)\\):\n\\[\\mathbf{K} = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\] \\[\\mathbf{u}(t) = -\\mathbf{K}\\mathbf{x}(t)\\]\nThis means the controller takes the current state \\(\\mathbf{x}(t)\\), multiplies it by the pre-computed gain matrix \\(\\mathbf{K}\\), and applies this as the control input. The negative sign indicates feedback (driving the state towards zero)."
  },
  {
    "objectID": "posts/ml4r-control/index.html#derivation-of-linear-optimal-control-u--kx-and-quadratic-value-function-vxxt-px",
    "href": "posts/ml4r-control/index.html#derivation-of-linear-optimal-control-u--kx-and-quadratic-value-function-vxxt-px",
    "title": "From Control to Model-based Learning",
    "section": "Derivation of Linear Optimal Control (\\(u^* = -Kx\\)) and Quadratic Value Function (\\(V(x)=x^T Px\\))",
    "text": "Derivation of Linear Optimal Control (\\(u^* = -Kx\\)) and Quadratic Value Function (\\(V(x)=x^T Px\\))\n\nBellman Optimality -&gt; The Value Function (Cost-to-Go):\nif a path from point A to point C is optimal, then any segment of that path (e.g., from point B to point C, where B is on the path) must also be optimal from point B. This motivated the transformation of cost function J to the value function below.\nThe Cost Function (J): Our problem statement is to find \\(u(t)\\) that minimizes: \\[J = \\int_{0}^{\\infty} (x^T(\\tau)Qx(\\tau) + u^T(\\tau)Ru(\\tau))d\\tau\\]\nThis is the objective.\nThe Value Function (V) is Defined in Terms of J: The value function V(x(t),t)$, as the minimum possible future cost from the current state \\(x(t)\\) at time \\(t\\) to the end of the control horizon (which is \\(\\infty\\) for infinite-horizon LQR). \\[V(x(t),t) = \\min_{u(\\tau),\\tau \\ge t} \\int_{t}^{\\infty} (x^T(\\tau)Qx(\\tau) + u^T(\\tau)Ru(\\tau))d\\tau\\]\nSo, \\(V(x(t),t)\\) is literally the minimum value of a section of the integral \\(J\\).\nTime-Invariance of \\(V(x)\\): For an LTI system with an infinite horizon and constant cost weights, the optimal cost-to-go function \\(V\\) will eventually reach a steady-state. This means it will no longer explicitly depend on time \\(t\\). Therefore, \\(\\frac{\\partial V}{\\partial t} = 0\\). \\[-\\frac{\\partial V}{\\partial t} = \\min_{u} \\left[ x^T Q x + u^T R u + \\left(\\frac{\\partial V}{\\partial x}\\right)^T (Ax + Bu) \\right]\\]\n\\[0 = \\min_{u} \\left[ x^T Q x + u^T R u + \\frac{\\partial V}{\\partial t} + \\left(\\frac{\\partial V}{\\partial x}\\right)^T (Ax + Bu) \\right]\\]\n\n\nDeriving \\(u^* = -Kx\\)\nLet‚Äôs focus on the term inside the \\(\\min_{u}\\) operator. This is a function of \\(u\\). To find the \\(u\\) that minimizes it, we take the partial derivative with respect to \\(u\\) and set it to zero.\nLet \\(g(u) = x^T Q x + u^T R u + \\left(\\frac{\\partial V}{\\partial x}\\right)^T (Ax + Bu)\\). We‚Äôre minimizing \\(g(u)\\) with respect to \\(u\\). Only terms involving \\(u\\) are relevant:\n\\[g(u) = u^T R u + \\left(\\frac{\\partial V}{\\partial x}\\right)^T Bu\\]\nTaking the derivative with respect to \\(u\\):\n\\[\\frac{\\partial g}{\\partial u} = 2Ru + B^T \\frac{\\partial V}{\\partial x}\\]\nSet to zero to find the optimal \\(u^*\\):\n\\[2Ru^* + B^T \\frac{\\partial V}{\\partial x} = 0\\] \\[u^* = -\\frac{1}{2} R^{-1} B^T \\frac{\\partial V}{\\partial x}\\]\nThis is the crucial step: The optimal control is found to be a linear function of the gradient of the value function!\n\n\nDeriving \\(V(x)=x^T Px\\)\nAt this point, we have \\(u^*\\) expressed in terms of \\(\\frac{\\partial V}{\\partial x}\\). Now we need to solve for \\(V(x)\\). This is where the specific structure of the LQR problem (linear dynamics, quadratic cost) becomes paramount.\nSince the problem is Linear-Quadratic, it is a known property from optimal control theory that the optimal value function \\(V(x)\\) will be a quadratic form of the state. This is not just a guess, but a deduction based on the inherent structure of LQ problems.\n\nWhy Quadratic? If \\(V(x)\\) were linear, its second derivative would be zero, which wouldn‚Äôt match the quadratic terms in the HJB. If it were higher order, the derivatives would lead to more complex non-linear equations, which would contradict the simplicity and linearity that arise from the problem. The quadratic form \\(x^T Px\\) is the lowest-order non-trivial form that is consistent with the problem‚Äôs structure.\nSymmetry: \\(P\\) is typically chosen to be symmetric (\\(P=P^T\\)) because \\(x^T Px = x^T P^T x\\). Any asymmetric part of \\(P\\) cancels out in the quadratic form, so we enforce symmetry for uniqueness and consistency.\n\nSo, we propose (or infer) the form:\n\\[V(x) = x^T Px\\]\nwhere \\(P\\) is a symmetric positive definite matrix.\nNow, calculate the gradient of \\(V(x)\\) with respect to \\(x\\):\n\\[\\frac{\\partial V}{\\partial x} = 2Px\\]\n\n\nDeriving ARE\nSubstitute \\(\\frac{\\partial V}{\\partial x} = 2Px\\) back into the expression for \\(u^*\\):\n\\[u^* = -\\frac{1}{2} R^{-1} B^T (2Px)\\] \\[u^* = -R^{-1} B^T Px\\]\nThis is our desired linear state feedback law! Here, \\(K = R^{-1} B^T P\\).\nNow, substitute \\(u^*\\) and \\(\\frac{\\partial V}{\\partial x}\\) back into the simplified HJB equation:\n\\[0 = x^T Q x + (-R^{-1} B^T Px)^T R (-R^{-1} B^T Px) + (2Px)^T (Ax + B(-R^{-1} B^T Px))\\] \\[0 = x^T Q x + x^T P^T (B^T)^T (R^{-1})^T R R^{-1} B^T Px + 2x^T P^T (Ax - BR^{-1} B^T Px)\\] Since \\(P = P^T\\) and \\(R\\) is symmetric (\\(R=R^T\\)), \\(R^{-1}\\) is also symmetric (\\((R^{-1})^T = R^{-1}\\)). Also, \\((B^T)^T = B\\).\n\\[0 = x^T Q x + x^T P B R^{-1} B^T Px + 2x^T PAx - 2x^T P B R^{-1} B^T Px\\] \\[0 = x^T Q x + 2x^T PAx - x^T P B R^{-1} B^T Px\\]\nRecognizing that \\(2x^T PAx = x^T PAx + x^T A^T P^T x = x^T (A^T P + PA)x\\) (since \\(P\\) is symmetric):\n\\[0 = x^T (Q + A^T P + PA - P B R^{-1} B^T P)x\\]\nFor this equation to hold for any state \\(x\\), the matrix in the parenthesis must be identically zero.\n\\[A^T P + PA - P B R^{-1} B^T P + Q = 0\\]\nThis is the Algebraic Riccati Equation (ARE)."
  },
  {
    "objectID": "posts/ml4r-control/index.html#transformation-to-a-standard-quadratic-program-qp",
    "href": "posts/ml4r-control/index.html#transformation-to-a-standard-quadratic-program-qp",
    "title": "From Control to Model-based Learning",
    "section": "Transformation to a Standard Quadratic Program (QP)",
    "text": "Transformation to a Standard Quadratic Program (QP)\nThe goal is to eliminate the state variables \\(x(k+i|k)\\) from the optimization problem, leaving only the control input sequence \\(U_k\\) as the decision variables. This is possible because the state evolution is precisely defined by the linear system dynamics, which act as equality constraints.\n\n1. Prediction of Future States:\nWe start by recursively expanding the system dynamics equation: \\(x(k+i+1|k) = Ax(k+i|k) + Bu(k+i|k)\\).\n\nInitial State: \\(x(k|k) = x(k)\\) (the current measured state).\n1-step ahead prediction: \\(x(k+1|k) = Ax(k|k) + Bu(k|k)\\)\n2-step ahead prediction: \\(x(k+2|k) = Ax(k+1|k) + Bu(k+1|k)\\) Substitute \\(x(k+1|k)\\): \\(x(k+2|k) = A(Ax(k|k) + Bu(k|k)) + Bu(k+1|k)\\) \\(x(k+2|k) = A^2 x(k|k) + ABu(k|k) + Bu(k+1|k)\\)\n\nAnd so on, up to \\(H_p\\) steps:\nGeneralizing, the predicted state at any future time \\(k+i\\) can be expressed as a sum of terms related to the initial state \\(x(k)\\) and the future control inputs \\(u(k|k), \\dots, u(k+i-1|k)\\):\n\\[x(k+i|k) = A^i x(k) + \\sum_{j=0}^{i-1} A^{i-1-j} Bu(k+j|k)\\]\nNow, let‚Äôs stack all the predicted states and controls into large vectors.\nLet \\(X_k = \\begin{bmatrix} x(k+1|k) \\\\ x(k+2|k) \\\\ \\vdots \\\\ x(k+H_p|k) \\end{bmatrix}\\) and \\(U_k = \\begin{bmatrix} u(k|k) \\\\ u(k+1|k) \\\\ \\vdots \\\\ u(k+H_c-1|k) \\end{bmatrix}\\) (remembering the assumption that \\(u\\) is constant after \\(H_c-1\\)).\nWe can write the entire sequence of future states as:\n\\[X_k = \\mathbf{\\Phi} x(k) + \\mathbf{\\Gamma} U_k\\]\nwhere:\n\n\\(\\mathbf{\\Phi}\\) is a large block matrix derived from powers of \\(A\\).\n\\(\\mathbf{\\Gamma}\\) is a large block lower triangular matrix (often called the Toeplitz matrix or controllability matrix) containing terms like \\(B, AB, A^2 B, \\dots\\). Its structure reflects how current and past controls affect future states.\n\n\n\n2. Substituting into the Objective Function:\nRecall the objective function:\n\\[J(U_k, x(k)) = \\sum_{i=0}^{H_p-1} \\left( \\|x(k+i|k) - x_{ref}(k+i)\\|_Q^2 + \\|u(k+i|k) - u_{ref}(k+i)\\|_R^2 \\right) + \\|x(k+H_p|k) - x_{ref}(k+H_p)\\|_P^2\\]\nWe can rewrite this in a compact quadratic form. Let‚Äôs simplify by assuming \\(x_{ref}=0\\) and \\(u_{ref}=0\\) for now to highlight the structure. The general case simply introduces linear terms.\n\\[J(U_k, x(k)) = \\sum_{i=0}^{H_p-1} x(k+i|k)^T Q x(k+i|k) + \\sum_{i=0}^{H_c-1} u(k+i|k)^T R u(k+i|k) + x(k+H_p|k)^T P x(k+H_p|k)\\]\nBy substituting \\(x(k+i|k) = A^i x(k) + \\sum_{j=0}^{i-1} A^{i-1-j} Bu(k+j|k)\\) into the expression for \\(J\\), the objective function becomes a quadratic function of \\(U_k\\) and \\(x(k)\\):\n\\[J(U_k, x(k)) = \\frac{1}{2} U_k^T H U_k + G^T U_k + J_{const}\\]\nwhere:\n\n\\(H\\) is a symmetric positive definite matrix (or positive semi-definite, depending on \\(R\\)). It encapsulates the weights \\(Q, R, P\\) and system matrices \\(A, B\\).\n\\(G\\) is a vector that depends on the current state \\(x(k)\\) and the reference trajectories.\n\\(J_{const}\\) is a term that depends only on \\(x(k)\\) and the reference trajectories, which doesn‚Äôt affect the minimization with respect to \\(U_k\\).\n\n\n\n3. Substituting into Constraints:\nSimilarly, all constraints (input, state, output) are originally expressed in terms of \\(x(k+i|k)\\) and \\(u(k+i|k)\\). By substituting the state prediction equation \\(x(k+i|k) = \\mathbf{\\Phi}_i x(k) + \\mathbf{\\Gamma}_i U_k\\) (where \\(\\mathbf{\\Phi}_i\\) and \\(\\mathbf{\\Gamma}_i\\) are parts of \\(\\mathbf{\\Phi}\\) and \\(\\mathbf{\\Gamma}\\) corresponding to time \\(k+i\\)), all constraints can be rewritten purely in terms of \\(x(k)\\) (which is known) and \\(U_k\\) (the decision variables).\nFor example, a state constraint \\(x_{min} \\le x(k+i|k) \\le x_{max}\\) becomes:\n\\[x_{min} \\le \\mathbf{\\Phi}_{i} x(k) + \\mathbf{\\Gamma}_{i} \\mathbf{U}_k \\le x_{max}\\]\nThis can be rearranged into the standard inequality form:\n\\[-\\mathbf{\\Gamma}_{i} \\mathbf{U}_k \\le \\mathbf{\\Phi}_{i} x(k) - x_{min}\\] \\[\\mathbf{\\Gamma}_{i} \\mathbf{U}_k \\le x_{max} - \\mathbf{\\Phi}_{i} x(k)\\]\nThese are stacked for all \\(i\\) and all types of constraints (input, state, output) into the compact form:\n\\[\\mathbf{L} \\mathbf{U}_k \\le \\mathbf{W}\\]\nwhere \\(\\mathbf{L}\\) and \\(\\mathbf{W}\\) are matrices and vectors that encapsulate all the constraint bounds, system matrices, and the current state \\(x(k)\\). Equality constraints (like the dynamics, if kept explicit rather than condensed) would form \\(\\mathbf{M}U_k = \\mathbf{V}\\)."
  },
  {
    "objectID": "posts/ml4r-control/index.html#the-result-a-standard-qp-form",
    "href": "posts/ml4r-control/index.html#the-result-a-standard-qp-form",
    "title": "From Control to Model-based Learning",
    "section": "The Result: A Standard QP Form",
    "text": "The Result: A Standard QP Form\nAfter this transformation, the MPC problem at each time step \\(k\\) is reduced to:\nMinimize: \\[\\frac{1}{2} U_k^T H U_k + G^T U_k\\] Subject to: \\[\\mathbf{L}U_k \\le \\mathbf{W} \\quad \\text{(and possibly } \\mathbf{M}U_k = \\mathbf{V} \\text{ for equality constraints)}\\]\nThis is precisely the standard form of a Quadratic Program (QP), where \\(U_k\\) is the optimization variable (what‚Äôs called ‚Äòz‚Äô in a generic QP solver). The matrices \\(H, G, L, W, M, V\\) are updated at each time step based on the current measured state \\(x(k)\\)."
  },
  {
    "objectID": "posts/ml4r-control/index.html#solutions-for-mpc-quadratic-programming-qp-solvers",
    "href": "posts/ml4r-control/index.html#solutions-for-mpc-quadratic-programming-qp-solvers",
    "title": "From Control to Model-based Learning",
    "section": "Solutions for MPC (Quadratic Programming (QP) Solvers)",
    "text": "Solutions for MPC (Quadratic Programming (QP) Solvers)\n\nActive Set Methods: These methods work by iteratively selecting a subset of the inequality constraints to be ‚Äúactive‚Äù (i.e., treated as equality constraints). They solve an equality-constrained QP at each iteration, update the active set, and move towards the optimal solution.\n\nPros: Highly reliable, provide exact solutions (within machine precision), often used for small to medium-sized problems.\nCons: Can be slow for large problems, especially if many active set changes are required. Number of iterations can be high.\n\nInterior-Point Methods: These methods transform the constrained QP into a sequence of unconstrained problems by adding ‚Äúbarrier functions‚Äù to the objective that penalize approaching the constraint boundaries. They then use Newton‚Äôs method to solve these unconstrained problems.\n\nPros: Generally scale much better with problem size (fewer iterations, though each iteration is more complex) and are often faster for large-scale QPs. They work well with warm-starting (using the previous solution as an initial guess).\nCons: Can be more complex to implement than active set methods.\n\nPrimal-Dual Methods: A broader class that includes many interior-point methods. They simultaneously solve the primal (original) QP problem and its dual problem."
  },
  {
    "objectID": "posts/ml4r-control/index.html#derivation",
    "href": "posts/ml4r-control/index.html#derivation",
    "title": "From Control to Model-based Learning",
    "section": "Derivation",
    "text": "Derivation\n\nState Prediction in Compact Matrix Form\nTHE POINT: ONLY REPRESENT IN U, YOU CAN SEE IN THE MATRIX BELOW\nRecap: We want to express all future predicted states \\(x(k+i|k)\\) as a function of the current state \\(x(k)\\) and the sequence of future control inputs \\(U_k\\).\nSystem: \\(x(k+1)=Ax(k)+Bu(k)\\) Prediction Horizon: \\(H_p\\) Control Horizon: \\(H_c\\) (For simplicity, let‚Äôs assume \\(H_c=H_p\\) for now. The case \\(H_c&lt;H_p\\) just means some \\(u\\)s are repeated, which is a minor modification.)\nStep-by-step prediction expansion:\n\\(x(k+1|k)=Ax(k)+Bu(k|k)\\) \\(x(k+2|k)=Ax(k+1|k)+Bu(k+1|k)=A(Ax(k)+Bu(k|k))+Bu(k+1|k)=A^2x(k)+ABu(k|k)+Bu(k+1|k)\\) \\(x(k+3|k)=Ax(k+2|k)+Bu(k+2|k)=A(A^2x(k)+ABu(k|k)+Bu(k+1|k))+Bu(k+2|k)=A^3x(k)+A^2Bu(k|k)+ABu(k+1|k)+Bu(k+2|k)\\) ‚Ä¶ and so on, up to \\(x(k+H_p|k)\\).\nStacking the Predictions:\nNow, let‚Äôs define the stacked vectors:\n\nFuture States Vector: \\(X_k = \\begin{bmatrix} x(k+1|k) \\\\ x(k+2|k) \\\\ \\vdots \\\\ x(k+H_p|k) \\end{bmatrix}\\) (size \\(n \\cdot H_p \\times 1\\))\nControl Input Sequence (Decision Variable): \\(U_k = \\begin{bmatrix} u(k|k) \\\\ u(k+1|k) \\\\ \\vdots \\\\ u(k+H_c-1|k) \\end{bmatrix}\\) (size \\(m \\cdot H_c \\times 1\\))\n\nSubstitute the expanded predictions into \\(X_k\\):\n\\[X_k = \\begin{bmatrix} Ax(k)+Bu(k|k) \\\\ A^2x(k)+ABu(k|k)+Bu(k+1|k) \\\\ A^3x(k)+A^2Bu(k|k)+ABu(k+1|k)+Bu(k+2|k) \\\\ \\vdots \\\\ A^{H_p}x(k)+A^{H_p-1}Bu(k|k)+\\dots+Bu(k+H_p-1|k) \\end{bmatrix}\\]\nNow, we separate the terms involving \\(x(k)\\) from the terms involving \\(U_k\\):\n\\[X_k = \\begin{bmatrix} A \\\\ A^2 \\\\ A^3 \\\\ \\vdots \\\\ A^{H_p} \\end{bmatrix} x(k) + \\begin{bmatrix} B & 0 & 0 & \\dots & 0 \\\\ AB & B & 0 & \\dots & 0 \\\\ A^2B & AB & B & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ A^{H_p-1}B & A^{H_p-2}B & A^{H_p-3}B & \\dots & B \\end{bmatrix} \\begin{bmatrix} u(k|k) \\\\ u(k+1|k) \\\\ u(k+2|k) \\\\ \\vdots \\\\ u(k+H_p-1|k) \\end{bmatrix}\\]\nThis is exactly the form: \\(X_k = \\mathbf{\\Phi} x(k) + \\mathbf{\\Gamma} U_k\\)\nWhere:\n\n\\(\\mathbf{\\Phi}\\) (State Contribution Matrix): \\[\\mathbf{\\Phi} = \\begin{bmatrix} A \\\\ A^2 \\\\ \\vdots \\\\ A^{H_p} \\end{bmatrix}\\] Size: (\\(n \\cdot H_p\\)) \\(\\times n\\). Each block is \\(n \\times n\\). This matrix shows how the initial state \\(x(k)\\) propagates to all future states if no control were applied.\n\\(\\mathbf{\\Gamma}\\) (Control Contribution Matrix / Block Controllability Matrix): \\[\\mathbf{\\Gamma} = \\begin{bmatrix} B & 0 & 0 & \\dots & 0 \\\\ AB & B & 0 & \\dots & 0 \\\\ A^2B & AB & B & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ A^{H_p-1}B & A^{H_p-2}B & A^{H_p-3}B & \\dots & B \\end{bmatrix}\\] Size: (\\(n \\cdot H_p\\)) \\(\\times\\) (\\(m \\cdot H_p\\)). Each block is \\(n \\times m\\). This lower triangular block matrix (sometimes called a block Toeplitz matrix) shows how the sequence of control inputs \\(U_k\\) influences the future states. The ‚Äúcontrollability‚Äù aspect comes from the powers of \\(A\\) multiplying \\(B\\), indicating how inputs at different times propagate through the system.\n\n\nNumerical Example\nLet‚Äôs use a very simple system: \\(x(k+1)=Ax(k)+Bu(k)\\) \\(A=\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\\), \\(B=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) (This is a discrete-time integrator/position system) Let \\(H_p=2\\) and \\(H_c=2\\).\nPredictions:\n\\(x(k+1|k)=Ax(k)+Bu(k|k)\\) \\(x(k+2|k)=Ax(k+1|k)+Bu(k+1|k)=A^2x(k)+ABu(k|k)+Bu(k+1|k)\\)\nCalculate powers of A and products with B: \\(A^2 = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\) \\(AB = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\)\nStacked Vectors: \\(X_k = \\begin{bmatrix} x(k+1|k) \\\\ x(k+2|k) \\end{bmatrix}\\) \\(U_k = \\begin{bmatrix} u(k|k) \\\\ u(k+1|k) \\end{bmatrix}\\)\nForming \\(\\mathbf{\\Phi}\\) and \\(\\mathbf{\\Gamma}\\):\n\\[X_k = \\begin{bmatrix} A \\\\ A^2 \\end{bmatrix} x(k) + \\begin{bmatrix} B & 0 \\\\ AB & B \\end{bmatrix} U_k\\]\nSubstitute the numerical matrices:\n\\[\\mathbf{\\Phi} = \\begin{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\\\ \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\quad \\text{(size } (2 \\cdot 2) \\times 2 = 4 \\times 2 \\text{)}\\]\n\\[\\mathbf{\\Gamma} = \\begin{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} & \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\\\ \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} & \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 0 \\\\ 1 & 1 \\end{bmatrix} \\quad \\text{(size } (2 \\cdot 2) \\times (1 \\cdot 2) = 4 \\times 2 \\text{)}\\]\nSo, \\(X_k = \\mathbf{\\Phi} x(k) + \\mathbf{\\Gamma} U_k\\) where all components are explicitly defined. This is how the state prediction is condensed.\n\n\n\n\nSubstituting into the Objective Function (Deriving H, G, J_const)\nRecap: We want to transform \\(J(U_k,x(k))\\) into \\(\\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}\\).\nObjective Function:\n\\[J(U_k,x(k)) = \\sum_{i=0}^{H_p-1} (\\|x(k+i|k)-x_{ref}(k+i)\\|_Q^2 + \\|u(k+i|k)-u_{ref}(k+i)\\|_R^2) + \\|x(k+H_p|k)-x_{ref}(k+H_p)\\|_P^2\\]\nLet \\(e_x(k+i|k)=x(k+i|k)-x_{ref}(k+i)\\) and \\(e_u(k+i|k)=u(k+i|k)-u_{ref}(k+i)\\). The term \\(\\|v\\|_W^2 = v^T W v\\).\nLet‚Äôs adjust the sum for x states to start from \\(i=1\\) to \\(H_p\\) to align with the stacked \\(X_k\\) derived in Part 1 (which starts from \\(x(k+1|k)\\)). The terminal cost (with P) is explicitly included at \\(H_p\\).\n\\[J(U_k,x(k)) = \\sum_{i=1}^{H_p} \\|x(k+i|k)-x_{ref}(k+i)\\|_Q^2 + \\sum_{i=0}^{H_c-1} \\|u(k+i|k)-u_{ref}(k+i)\\|_R^2\\] (Note: For clarity, the terminal cost is typically a separate term at \\(x(k+H_p|k)\\) with weight \\(P\\). If \\(P\\) is used as a Q for the last state in the sum, then \\(Q_{H_p}\\) is \\(P\\). Let‚Äôs explicitly keep it in the sum by making \\(Q_{H_p} = P\\) for \\(i=H_p\\).)\nWe can write the sums in stacked form:\n\\[J = E_X^T \\bar{Q} E_X + E_U^T \\bar{R} E_U\\]\nWhere: * \\(E_X = \\begin{bmatrix} e_x(k+1|k) \\\\ e_x(k+2|k) \\\\ \\vdots \\\\ e_x(k+H_p|k) \\end{bmatrix}\\) (size \\(n \\cdot H_p \\times 1\\)) * \\(E_U = \\begin{bmatrix} e_u(k|k) \\\\ e_u(k+1|k) \\\\ \\vdots \\\\ e_u(k+H_c-1|k) \\end{bmatrix}\\) (size \\(m \\cdot H_c \\times 1\\))\n\n\\(\\bar{Q} = \\text{diag}(Q, Q, \\dots, Q, P)\\) (a block diagonal matrix of size \\((n \\cdot H_p) \\times (n \\cdot H_p)\\), where the last block is \\(P\\) for \\(x(k+H_p|k)\\) and others are \\(Q\\)).\n\\(\\bar{R} = \\text{diag}(R, R, \\dots, R)\\) (\\(H_c\\) times, block diagonal matrix, size \\((m \\cdot H_c) \\times (m \\cdot H_c)\\)).\n\nNow, substitute \\(X_k=\\mathbf{\\Phi}x(k)+\\mathbf{\\Gamma}U_k\\) into the objective. Let‚Äôs define the full reference vector \\(X_{ref}=[x_{ref}(k+1)^T \\dots x_{ref}(k+H_p)^T]^T\\) and \\(U_{ref}=[u_{ref}(k)^T \\dots u_{ref}(k+H_c-1)^T]^T\\).\nThe stacked state error is \\(E_X = X_k - X_{ref} = (\\mathbf{\\Phi}x(k)+\\mathbf{\\Gamma}U_k) - X_{ref}\\). The stacked input error is \\(E_U = U_k - U_{ref}\\).\nThen:\n\\[J = (\\mathbf{\\Phi}x(k)+\\mathbf{\\Gamma}U_k - X_{ref})^T \\bar{Q} (\\mathbf{\\Phi}x(k)+\\mathbf{\\Gamma}U_k - X_{ref}) + (U_k - U_{ref})^T \\bar{R} (U_k - U_{ref})\\]\nNow, expand and collect terms by powers of \\(U_k\\):\n\nQuadratic term in \\(U_k\\) (for \\(H\\)): From the first term: \\((\\mathbf{\\Gamma}U_k)^T \\bar{Q} (\\mathbf{\\Gamma}U_k) = U_k^T \\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Gamma} U_k\\) From the second term: \\(U_k^T \\bar{R} U_k\\) So, the full quadratic term is \\(U_k^T (\\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Gamma} + \\bar{R}) U_k\\). Therefore, \\(H = 2(\\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Gamma} + \\bar{R})\\). (The factor of 2 comes from the standard \\(\\frac{1}{2}z^T H z\\) form of QP objective).\nLinear term in \\(U_k\\) (for \\(G\\)): Let \\(c_x = \\mathbf{\\Phi}x(k) - X_{ref}\\). From the first term (cross-term \\(2 c_x^T \\bar{Q} \\mathbf{\\Gamma} U_k\\)): \\(2(\\mathbf{\\Phi}x(k) - X_{ref})^T \\bar{Q} \\mathbf{\\Gamma} U_k\\) From the second term (cross-term \\(-2 U_{ref}^T \\bar{R} U_k\\)): \\(-2U_{ref}^T \\bar{R} U_k\\) So, the full linear term is \\(2(\\mathbf{\\Phi}x(k) - X_{ref})^T \\bar{Q} \\mathbf{\\Gamma} U_k - 2U_{ref}^T \\bar{R} U_k\\). Therefore, \\(G^T = 2(\\mathbf{\\Phi}x(k) - X_{ref})^T \\bar{Q} \\mathbf{\\Gamma} - 2U_{ref}^T \\bar{R}\\). Or, \\(G = 2\\mathbf{\\Gamma}^T \\bar{Q} (\\mathbf{\\Phi}x(k) - X_{ref}) - 2\\bar{R}^T U_{ref}\\). Note that \\(\\bar{R}\\) is symmetric, so \\(\\bar{R}^T=\\bar{R}\\).\nConstant term (for \\(J_{const}\\)): This term does not depend on \\(U_k\\) and is effectively ignored by the optimizer. It includes: \\((\\mathbf{\\Phi}x(k) - X_{ref})^T \\bar{Q} (\\mathbf{\\Phi}x(k) - X_{ref}) + U_{ref}^T \\bar{R} U_{ref}\\). This term is important if you want to know the actual minimum cost, but not for finding the optimal \\(U_k\\).\n\n\nNumerical Example\nContinue with our previous example: \\(A=\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\\), \\(B=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) \\(H_p=2\\), \\(H_c=2\\). Let \\(Q=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) (identity matrix for state penalty), \\(R=[0.1]\\) (scalar for input penalty). Let \\(P=Q\\) (for simplicity of \\(\\bar{Q}\\) structure), \\(x_{ref}=0\\), \\(u_{ref}=0\\).\nFrom Part 1: \\(\\mathbf{\\Phi}=\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\) \\(\\mathbf{\\Gamma}=\\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 0 \\\\ 1 & 1 \\end{bmatrix}\\)\nAlso: \\(\\bar{Q}=\\begin{bmatrix} Q & 0 \\\\ 0 & P \\end{bmatrix} = \\begin{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} & \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} \\\\ \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} & \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\) (size \\(4 \\times 4\\)) \\(\\bar{R}=\\begin{bmatrix} R & 0 \\\\ 0 & R \\end{bmatrix}=\\begin{bmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{bmatrix}\\) (size \\(2 \\times 2\\))\nNow, let‚Äôs compute \\(H\\) (assuming \\(H = 2(\\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Gamma} + \\bar{R})\\) as per the QP formulation): First, \\(\\mathbf{\\Gamma}^T \\bar{Q}\\): \\[\\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\] Then, \\((\\mathbf{\\Gamma}^T \\bar{Q})\\mathbf{\\Gamma}\\): \\[\\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 0 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} (0)(0)+(1)(1)+(0)(0)+(1)(1) & (0)(0)+(1)(0)+(0)(0)+(1)(1) \\\\ (0)(0)+(0)(1)+(0)(0)+(1)(1) & (0)(0)+(0)(0)+(0)(0)+(1)(1) \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 1 \\end{bmatrix}\\] Finally, \\(H = 2 \\left( \\begin{bmatrix} 2 & 1 \\\\ 1 & 1 \\end{bmatrix} + \\begin{bmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{bmatrix} \\right) = 2 \\begin{bmatrix} 2.1 & 1 \\\\ 1 & 1.1 \\end{bmatrix} = \\begin{bmatrix} 4.2 & 2 \\\\ 2 & 2.2 \\end{bmatrix}\\)\nNow, let‚Äôs compute \\(G\\) (assuming \\(x_{ref}=0, u_{ref}=0\\), so \\(X_{ref}=0, U_{ref}=0\\)). In this case, \\(G = 2\\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Phi}x(k)\\). Let \\(x(k)=\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\).\nFirst, \\(\\mathbf{\\Phi}x(k)\\): \\[\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_1+x_2 \\\\ x_1 \\\\ 2x_1+x_2 \\end{bmatrix}\\] Then, \\(\\bar{Q} \\mathbf{\\Phi}x(k)\\): \\[\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_1+x_2 \\\\ x_1 \\\\ 2x_1+x_2 \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_1+x_2 \\\\ x_1 \\\\ 2x_1+x_2 \\end{bmatrix}\\] Finally, \\(G = 2\\mathbf{\\Gamma}^T (\\bar{Q} \\mathbf{\\Phi}x(k))\\): \\[G = 2 \\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_1+x_2 \\\\ x_1 \\\\ 2x_1+x_2 \\end{bmatrix} = 2 \\begin{bmatrix} (0)x_1 + (1)(x_1+x_2) + (0)x_1 + (1)(2x_1+x_2) \\\\ (0)x_1 + (0)(x_1+x_2) + (0)x_1 + (1)(2x_1+x_2) \\end{bmatrix}\\] \\[G = 2 \\begin{bmatrix} x_1+x_2 + 2x_1+x_2 \\\\ 2x_1+x_2 \\end{bmatrix} = 2 \\begin{bmatrix} 3x_1+2x_2 \\\\ 2x_1+x_2 \\end{bmatrix} = \\begin{bmatrix} 6x_1+4x_2 \\\\ 4x_1+2x_2 \\end{bmatrix}\\]\nThe \\(J_{const}\\) term would be \\(x(k)^T \\mathbf{\\Phi}^T \\bar{Q} \\mathbf{\\Phi} x(k)\\), which would be a scalar depending only on \\(x(k)\\).\nThis detailed breakdown shows how the matrices \\(H\\) and \\(G\\) are explicitly constructed from the system matrices, weights, and the current state. This allows a standard QP solver to take these numerical matrices and solve for the optimal \\(U_k\\) sequence at each time step.\n\n\n\nBut why do we need to transform the cost function into separated linear, quadratic and Jconst term?\nQP solvers are designed to solve problems in a very specific mathematical form. The standard general form of a Quadratic Program is:\nMinimize: \\[f(z) = \\frac{1}{2} z^T H z + g^T z\\]\nSubject to: \\[A_{eq} z = b_{eq}\\] \\[A_{ineq} z \\le b_{ineq}\\]\nWhere: * \\(z\\) is the vector of optimization variables. * \\(H\\) is the Hessian matrix (symmetric). It determines the curvature of the objective function. * \\(g\\) is the linear term vector. * \\(A_{eq}, b_{eq}, A_{ineq}, b_{ineq}\\) define the linear equality and inequality constraints.\nBy transforming the MPC objective into the form \\(\\frac{1}{2} U_k^T H U_k + G^T U_k + J_{const}\\):\n\nOur decision variable \\(U_k\\) maps directly to the generic \\(z\\).\nOur calculated \\(H\\) matrix maps directly to the generic \\(H\\).\nOur calculated \\(G\\) vector maps directly to the generic \\(g\\)."
  },
  {
    "objectID": "posts/ml4r-control/index.html#simple-example-1d-car-path-following-with-mpc",
    "href": "posts/ml4r-control/index.html#simple-example-1d-car-path-following-with-mpc",
    "title": "From Control to Model-based Learning",
    "section": "Simple Example: 1D Car Path Following with MPC",
    "text": "Simple Example: 1D Car Path Following with MPC\nScenario: Imagine a car that can only move along a straight line (1D). Its goal is to follow a predefined ‚Äúpath‚Äù (a sequence of desired positions) along this line. We want to control its acceleration to achieve this.\n\n1. Dynamics Model:\nLet‚Äôs use a very simple discrete-time model for longitudinal motion:\n\nStates (\\(x\\)):\n\n\\(p(k)\\): Position of the car at time \\(k\\)\n\\(v(k)\\): Velocity of the car at time \\(k\\) So, \\(x(k)=\\begin{bmatrix} p(k) \\\\ v(k) \\end{bmatrix}\\)\n\nInput (\\(u\\)):\n\n\\(a(k)\\): Acceleration applied by the car at time \\(k\\) So, \\(u(k)=[a(k)]\\)\n\n\nDiscrete-Time Equations: Assume a sampling time \\(\\Delta t=1\\) second for simplicity. \\(p(k+1)=p(k)+v(k)\\Delta t+\\frac{1}{2}a(k)(\\Delta t)^2\\) \\(v(k+1)=v(k)+a(k)\\Delta t\\)\nPlugging in \\(\\Delta t=1\\): \\(p(k+1)=p(k)+v(k)+\\frac{1}{2}a(k)\\) \\(v(k+1)=v(k)+a(k)\\)\nIn state-space form \\(x(k+1)=Ax(k)+Bu(k)\\): \\(A=\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\), \\(B=\\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}\\) (Self-correction: The position update \\(p(k+1)=p(k)+v(k)\\Delta t\\) means the v(k) term is in the first row of A. The velocity update \\(v(k+1)=v(k)+a(k)\\Delta t\\) means v(k) is in the second row of A, and a(k) is in the second row of B. The original matrix A was incorrect for this model. The corrected A matrix is \\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\) and B is \\(\\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}\\).)\n\n\n2. Path Representation & Reference Trajectory:\nLet our desired path be a set of target positions. For this 1D example, let the car‚Äôs goal be to reach position 10 and stop. * Target positions (path): \\(p_{ref}=[1,2,3,4,5,6,7,8,9,10,10,10,\\dots]\\) * Target velocities: \\(v_{ref}=[0,0,0,0,0,0,0,0,0,0,0,0,\\dots]\\) (we want it to stop at 10)\nSo, \\(x_{ref}(k+i)=\\begin{bmatrix} p_{ref}(k+i) \\\\ v_{ref}(k+i) \\end{bmatrix}\\)\n\n\n3. MPC Parameters:\n\nPrediction Horizon (\\(H_p\\)): Let‚Äôs choose \\(H_p=3\\) steps.\nControl Horizon (\\(H_c\\)): Let‚Äôs choose \\(H_c=2\\) steps. This means we‚Äôll optimize \\(a(k)\\) and \\(a(k+1)\\), and assume \\(a(k+2)\\) (and beyond) is zero.\nWeights:\n\n\\(Q=\\begin{bmatrix} 1 & 0 \\\\ 0 & 0.1 \\end{bmatrix}\\) (Penalize position error strongly, velocity error lightly)\n\\(R=[0.01]\\) (Penalize acceleration lightly to allow motion)\n\\(P=\\begin{bmatrix} 5 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) (Strong terminal penalty on position at the end of horizon)\n\nConstraints:\n\nAcceleration limits: \\(-1 \\le a(k) \\le 1\\) (\\(m/s^2\\))\nVelocity limits: \\(0 \\le v(k) \\le 5\\) (\\(m/s\\)) (Car cannot go backwards, max speed 5 m/s)\nPosition limits: \\(0 \\le p(k) \\le 11\\) (Stay within a narrow road segment)\n\n\n\n\n4. MPC at Time \\(k=0\\) (First Iteration):\nAssume current measured state: \\(x(0)=\\begin{bmatrix} p(0) \\\\ v(0) \\end{bmatrix}=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) (car starts at origin, at rest).\na) Define Decision Variables: The control sequence we need to find is \\(U_0=\\begin{bmatrix} u(0|0) \\\\ u(1|0) \\end{bmatrix}=\\begin{bmatrix} a(0) \\\\ a(1) \\end{bmatrix}\\). (Length \\(m \\cdot H_c = 1 \\cdot 2 = 2\\))\nb) Generate Reference Trajectory for the Horizon: From our path, for \\(H_p=3\\) steps:\n\\(x_{ref}(1)=\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) \\(x_{ref}(2)=\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) \\(x_{ref}(3)=\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\)\nc) Formulate Objective Function (QP style): The goal is to find \\(U_0\\) that minimizes:\n\\[J = \\sum_{i=1}^{3} \\|x(i|0)-x_{ref}(i)\\|_Q^2 + \\sum_{i=0}^{1} \\|u(i|0)-u_{ref}(i)\\|_R^2\\]\n(Here, \\(u_{ref}(i)=0\\) as we just want to follow the path, not target specific acceleration) Note: The terminal cost is explicit if \\(P\\) is the \\(Q\\) for the final state in the sum. Let‚Äôs make it explicit for clarity:\n\\[J = \\|x(1|0)-x_{ref}(1)\\|_Q^2 + \\|x(2|0)-x_{ref}(2)\\|_Q^2 + \\|x(3|0)-x_{ref}(3)\\|_P^2 + \\|u(0|0)\\|_R^2 + \\|u(1|0)\\|_R^2\\]\nCondensing into \\(\\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}\\):\nFirst, state predictions: \\(x(1|0)=Ax(0)+Bu(0|0)\\) \\(x(2|0)=A^2x(0)+ABu(0|0)+Bu(1|0)\\) \\(x(3|0)=A^3x(0)+A^2Bu(0|0)+ABu(1|0)+Bu(2|0)\\) (Since \\(H_c=2\\), \\(u(2|0)\\) would be zero here or \\(u(1|0)\\)) Let‚Äôs assume \\(u(k+i|k)=0\\) for \\(i \\ge H_c\\). So \\(u(2|0)=0\\).\nCalculate powers of A and products with B: \\(A^2 = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}\\) \\(A^3 = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 3 \\\\ 0 & 1 \\end{bmatrix}\\) \\(B = \\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}\\) \\(AB = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 1 \\end{bmatrix}\\) \\(A^2B = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2.5 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{\\Phi} = \\begin{bmatrix} A \\\\ A^2 \\\\ A^3 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 1 & 2 \\\\ 0 & 1 \\\\ 1 & 3 \\\\ 0 & 1 \\end{bmatrix}\\) (size \\(6 \\times 2\\))\n\\(\\mathbf{\\Gamma} = \\begin{bmatrix} B & 0 \\\\ AB & B \\\\ A^2B & AB \\end{bmatrix} = \\begin{bmatrix} 0.5 & 0 \\\\ 1 & 0 \\\\ 1.5 & 0.5 \\\\ 1 & 1 \\\\ 2.5 & 1.5 \\\\ 1 & 1 \\end{bmatrix}\\) (size \\(6 \\times 2\\))\n\\(\\bar{Q}_{stacked} = \\text{diag}(Q, Q, P) = \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0.1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0.1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 5 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{bmatrix}\\) (size \\(6 \\times 6\\)) \\(\\bar{R} = \\text{diag}(R,R) = \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix}\\) (size \\(2 \\times 2\\))\nThen, \\(H = 2(\\mathbf{\\Gamma}^T \\bar{Q}_{stacked} \\mathbf{\\Gamma} + \\bar{R})\\). And \\(G = 2\\mathbf{\\Gamma}^T \\bar{Q}_{stacked}(\\mathbf{\\Phi}x(0) - X_{ref}) - 2\\bar{R}U_{ref}\\). Since \\(x(0) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) and \\(u_{ref}(i)=0\\), \\(U_{ref}=0\\). \\(X_{ref} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 3 \\\\ 0 \\end{bmatrix}\\). So, \\(G = 2\\mathbf{\\Gamma}^T \\bar{Q}_{stacked}(\\mathbf{\\Phi}x(0) - X_{ref})\\).\nd) Formulate Constraints (QP style):\n\nInput Constraints: \\(-1 \\le a(k) \\le 1 \\implies \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} a(k) \\le \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) Stacked for \\(U_0 = \\begin{bmatrix} a(0) \\\\ a(1) \\end{bmatrix}\\): \\[\\begin{bmatrix} 1 & 0 \\\\ -1 & 0 \\\\ 0 & 1 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} a(0) \\\\ a(1) \\end{bmatrix} \\le \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\]\nState Constraints (Velocity): \\(0 \\le v(k+i|0) \\le 5\\) for \\(i=1,2,3\\). \\(v(k+i|0)\\) is the second component of \\(x(k+i|0)\\). \\(x(k+i|0) = \\mathbf{\\Phi}_i x(0) + \\mathbf{\\Gamma}_i U_0\\). The constraint can be written as: \\(\\begin{bmatrix} 0 & 1 \\end{bmatrix} x(k+i|0) \\le 5\\) \\(-\\begin{bmatrix} 0 & 1 \\end{bmatrix} x(k+i|0) \\le 0\\) Substitute \\(x(k+i|0)\\): \\(\\begin{bmatrix} 0 & 1 \\end{bmatrix} (\\mathbf{\\Phi}_i x(0) + \\mathbf{\\Gamma}_i U_0) \\le 5\\) \\(-\\begin{bmatrix} 0 & 1 \\end{bmatrix} (\\mathbf{\\Phi}_i x(0) + \\mathbf{\\Gamma}_i U_0) \\le 0\\)\nFor \\(x(0)=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\): \\(v(1|0) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (A x(0) + B u(0|0)) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} B u(0|0) = 1 \\cdot a(0) = a(0)\\). So, \\(0 \\le a(0) \\le 5\\). (This is covered by \\(-1 \\le a(0) \\le 1\\) limits).\n\\(v(2|0) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (A^2 x(0) + AB u(0|0) + B u(1|0)) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (AB u(0|0) + B u(1|0)) = 1 \\cdot a(0) + 1 \\cdot a(1) = a(0)+a(1)\\). So, \\(0 \\le a(0)+a(1) \\le 5\\).\n\\(v(3|0) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (A^3 x(0) + A^2B u(0|0) + AB u(1|0)) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (A^2B u(0|0) + AB u(1|0)) = 1 \\cdot a(0) + 1 \\cdot a(1) = a(0)+a(1)\\). So, \\(0 \\le a(0)+a(1) \\le 5\\).\nState Constraints (Position): \\(0 \\le p(k+i|0) \\le 11\\) for \\(i=1,2,3\\). \\(p(k+i|0)\\) is the first component of \\(x(k+i|0)\\). \\(\\begin{bmatrix} 1 & 0 \\end{bmatrix} x(k+i|0) \\le 11\\) \\(-\\begin{bmatrix} 1 & 0 \\end{bmatrix} x(k+i|0) \\le 0\\)\nFor \\(x(0)=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\): \\(p(1|0) = \\begin{bmatrix} 1 & 0 \\end{bmatrix} B u(0|0) = 0.5 \\cdot a(0)\\). So, \\(0 \\le 0.5 a(0) \\le 11\\).\n\\(p(2|0) = \\begin{bmatrix} 1 & 0 \\end{bmatrix} (AB u(0|0) + B u(1|0)) = 1.5 \\cdot a(0) + 0.5 \\cdot a(1)\\). So, \\(0 \\le 1.5 a(0) + 0.5 a(1) \\le 11\\).\n\\(p(3|0) = \\begin{bmatrix} 1 & 0 \\end{bmatrix} (A^2B u(0|0) + AB u(1|0)) = 2.5 \\cdot a(0) + 1.5 \\cdot a(1)\\). So, \\(0 \\le 2.5 a(0) + 1.5 a(1) \\le 11\\).\n\nYou would formulate all of these into the final \\(\\mathbf{L}U_k \\le \\mathbf{W}\\) matrix.\n\n\n5. Solve the QP:\nAt time \\(k=0\\), a QP solver (like quadprog in MATLAB, or cvxopt in Python) is given \\(H, G, L, W\\) (and \\(x(0)\\) is embedded in \\(G\\) and \\(W\\)). The solver would return the optimal \\(U_0^*=\\begin{bmatrix} a^*(0) \\\\ a^*(1) \\end{bmatrix}\\).\nExample Output (Illustrative - not numerically solved): Let‚Äôs assume the solver returns: \\(a^*(0)=1.0\\) (accelerate to get moving) \\(a^*(1)=0.8\\) (continue accelerating)\n\n\n6. Apply First Control Action:\nThe car‚Äôs controller applies only the first element: \\(a_{applied}(0)=a^*(0)=1.0\\).\n\n\n7. Move to Next Timestep (\\(k=1\\)):\nThe car‚Äôs actual state is measured: \\(x(1)\\). Using \\(a_{applied}(0)=1.0\\): \\(p(1)=p(0)+v(0)+\\frac{1}{2}a(0)=0+0+\\frac{1}{2}(1.0)=0.5\\) \\(v(1)=v(0)+a(0)=0+1.0=1.0\\) So, \\(x(1)=\\begin{bmatrix} 0.5 \\\\ 1.0 \\end{bmatrix}\\)\nThe entire process repeats: * MPC takes \\(x(1)\\) as its new current state. * It generates a new reference trajectory for \\(k=1,2,3\\) (e.g., \\(x_{ref}(2), x_{ref}(3), x_{ref}(4)\\) from the path). * It re-formulates a new QP with updated \\(G\\) and \\(W\\) matrices (as they depend on \\(x(1)\\)). * It solves the QP for a new optimal control sequence \\(U_1^*=\\begin{bmatrix} a^*(1) \\\\ a^*(2) \\end{bmatrix}\\). * Only \\(a^*(1)\\) from this new sequence is applied."
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html",
    "href": "posts/cg1-meshprocessing/index.html",
    "title": "Surface Analysis",
    "section": "",
    "text": "There are three main ways to represent an analytic surface:\n\n\nat any given \\((x,y)\\) coordinate, the function gives a surface‚Äôs height (\\(z\\)). Downside that only for surface that doesnt overlap the z value more than once.\n\nEquation: \\(z=f(x,y)\\)\nExample: A plane \\(z=ax+by+c\\).\n\n\n\n\nThis is a vector-valued function that maps a 2D parameter space (defined by parameters like \\(u\\) and \\(v\\)) to a 3D surface. This is a very flexible representation that can describe a wide range of shapes, including closed surfaces, so we focus on this.\n\nEquation: \\(\\mathbf{S}(u,v)=\\begin{pmatrix} x(u,v) \\\\ y(u,v) \\\\ z(u,v) \\end{pmatrix}\\)\nExample: A sphere defined by the equations:\n\n\\(x=r\\cos\\theta\\sin\\phi\\) (Basically each element in the target vector has their own function x, y, z)\n\\(y=r\\sin\\theta\\sin\\phi\\)\n\\(z=r\\cos\\phi\\)\n\n\n\n\n\nThis is a function that defines a surface as the set of all points that satisfy a specific equation, where the function equals a constant (often zero). Implicit surfaces can represent complex, closed shapes and are useful for collision detection and boolean operations.\n\nEquation: \\(f(x,y,z)=0\\)\nExample: A sphere with radius \\(r\\) centered at the origin \\(x^2+y^2+z^2-r^2=0\\).\n\n\n\n\n\nHow much order we take is the function of that order (first-order is a linear function, second order is a quadratic function (parabol go across point p))\nUnivariate scalar:\n\\[f(x+\\delta) = f(x) + f'(x)\\cdot\\delta + \\frac{1}{2}f''(x)\\cdot\\delta^2 + \\mathcal{O}(\\delta^3)\\]\nMultivariate scalar:\n\\[f(\\mathbf{x}+\\boldsymbol{\\delta}) = f(\\mathbf{x}) + f'_{\\mathbf{x}}(\\mathbf{x})\\cdot\\boldsymbol{\\delta} + \\frac{1}{2}\\boldsymbol{\\delta}^T H_f^\\dagger(\\mathbf{x})\\boldsymbol{\\delta} + \\mathcal{O}(\\left\\|\\boldsymbol{\\delta}\\right\\|^3)\\]\nMultivariate vector-valued: \\[\\mathbf{f}(\\mathbf{x}+\\boldsymbol{\\delta}) = \\mathbf{f}(\\mathbf{x}) + J_{\\mathbf{f}_{\\boldsymbol{\\delta}}}(\\mathbf{x})\\boldsymbol{\\delta} + \\mathcal{O}(\\left\\|\\boldsymbol{\\delta}\\right\\|^2)\\]\n\n\n\\[f: \\mathbb{R}^n \\to \\mathbb{R}\\] e.g.¬†height field \\(h = f(\\mathbf{x}) = f(x,y)\\)\nDerivation w.r.t. any of the variables gives partial derivatives:\n\\[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}: \\mathbb{R}^n \\to \\mathbb{R}\\]\nShort forms: \\(f_x = \\frac{\\partial f}{\\partial x}, f_y = \\frac{\\partial f}{\\partial y}\\)\n2nd deriv.: \\(f_{xx} = \\frac{\\partial^2 f}{\\partial x^2}, f_{yy} = \\frac{\\partial^2 f}{\\partial y^2}, f_{xy} = \\frac{\\partial^2 f}{\\partial x \\partial y}\\)\nThe gradient of a function at a given point is the (row)-vector (in direction of steepest ascent) of partial derivatives (\\(\\nabla_{\\mathbf{x}} f: \\mathbb{R}^n \\to \\mathbb{R}^{1\\times n}\\)):\n\\(\\nabla_{\\mathbf{x}} f = f_{\\mathbf{x}} = (f_x, f_y, \\ldots)\\) (it points in direction of steepest ascent)\n\n\n\n\\[f: \\mathbb{R} \\to \\mathbb{R}^m\\]\nE.g. a curve:\n\\[\\mathbf{f}(t) = \\begin{pmatrix} x(t) \\\\ y(t) \\end{pmatrix} = \\begin{pmatrix} \\cos t \\\\ \\sin t \\end{pmatrix}\\]\nDerivatives are computed per component independently\n\\[\\frac{d\\mathbf{f}}{dt}: \\mathbb{R} \\to \\mathbb{R}^2\\] \\[\\mathbf{f}'(t) = \\begin{pmatrix} x'(t) \\\\ y'(t) \\end{pmatrix}\\]\nWhere:\n\nThe derivative is a tangent vector to the surface\nIts length represents the speed of change.\n\n\n\n\n\\[f: \\mathbb{R}^n \\to \\mathbb{R}^m\\]\nE.g. a surface:\n\\[\\mathbf{s}(\\boldsymbol{\\tau}) = \\mathbf{s}\\begin{pmatrix} u \\\\ v \\end{pmatrix} = \\begin{pmatrix} x(\\boldsymbol{\\tau}) \\\\ y(\\boldsymbol{\\tau}) \\\\ z(\\boldsymbol{\\tau}) \\end{pmatrix} = \\begin{pmatrix} x(u,v) \\\\ y(u,v) \\\\ z(u,v) \\end{pmatrix} = \\begin{pmatrix} \\cos u \\cos v \\\\ \\sin u \\cos v \\\\ \\sin v \\end{pmatrix}\\]\nThe partial derivatives are tangent vectors:\n\\[\\frac{\\partial \\mathbf{s}}{\\partial u} = \\mathbf{s}_u = \\begin{pmatrix} x_u \\\\ y_u \\\\ z_u \\end{pmatrix}\\]\nThe row-vector of partial derivatives is now a matrix says how changes in u and v affect the coordinate x,y,z:\n\\[J_{\\mathbf{s}_{\\boldsymbol{\\tau}}} = \\mathbf{s}_{\\boldsymbol{\\tau}} = \\begin{pmatrix} \\mathbf{s}_u & \\mathbf{s}_v \\end{pmatrix} = \\begin{pmatrix} x_u & x_v \\\\ y_u & y_v \\\\ z_u & z_v \\end{pmatrix}\\]\nThis matrix is called the Jacobian. Basically a transformation matrix that map direction from parameter space to value space. The Jacobian‚Äôs columns are the tangent vectors s_u and s_v in value space.\n\n\n\nWe can calculate the derivative in a specific direction \\(\\boldsymbol{\\delta}\\) (\\(\\left\\|\\boldsymbol{\\delta}\\right\\|=1\\)).\nIn the scalar case, just do the dot product with the gradien, it projects on it:\n\\[\\frac{\\partial f}{\\partial \\boldsymbol{\\delta}}(\\mathbf{x}) = f_{\\mathbf{x}}(\\mathbf{x})\\cdot\\boldsymbol{\\delta} = f_x \\cdot \\delta[x] + f_y \\cdot \\delta[y] + \\ldots\\]\nThis generalizes to vector-valued functions:\n\\[\\frac{\\partial \\mathbf{f}}{\\partial \\boldsymbol{\\delta}}(\\mathbf{x}) = \\mathbf{f}_{\\mathbf{x}}\\cdot\\boldsymbol{\\delta} = \\mathbf{f}_x \\cdot \\delta[x] + \\mathbf{f}_y \\cdot \\delta[y] + \\ldots\\]\nAs said above: The Jacobian‚Äôs columns are the tangent vectors s_u and s_v in value space.\n\n\n\nThe transformation chain can be represented as:\n\\[\n\\begin{align*}\n\\vec{\\tau} = \\begin{pmatrix} u \\\\ v \\end{pmatrix}\n\\xrightarrow{s} \\vec{p} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}\n\\xrightarrow{\\tilde{c}} \\vec{c} = \\begin{pmatrix} r \\\\ g \\\\ b \\\\ a \\end{pmatrix}\n\\end{align*}\n\\]\nChain rule:\n\\[\n[ \\dot{c}(s(\\vec{\\tau})) ]_{\\vec{\\tau}} =\n[ \\dot{c}_{\\vec{p}}(s(\\vec{\\tau})) ] s_{\\vec{\\tau}}(\\vec{\\tau})\n\\]\n\\[\\frac{\\partial \\mathbf{c}}{\\partial \\boldsymbol{\\tau}} = \\frac{\\partial \\mathbf{c}}{\\partial \\mathbf{p}} \\frac{\\partial \\mathbf{p}}{\\partial \\boldsymbol{\\tau}}\\]\nExpressed in component form:\n\\[\n\\begin{pmatrix}\nr_u & r_v \\\\\ng_u & g_v \\\\\nb_u & b_v \\\\\na_u & a_v\n\\end{pmatrix} =\n\\begin{pmatrix}\nr_x & r_y & r_z \\\\\ng_x & g_y & g_z \\\\\nb_x & b_y & b_z \\\\\na_x & a_y & a_z\n\\end{pmatrix}\n\\begin{pmatrix}\nx_u & x_v \\\\\ny_u & y_v \\\\\nz_u & z_v\n\\end{pmatrix}\n\\]\n\\[\\begin{pmatrix} \\frac{\\partial c_r}{\\partial u} & \\frac{\\partial c_r}{\\partial v} \\\\ \\frac{\\partial c_g}{\\partial u} & \\frac{\\partial c_g}{\\partial v} \\\\ \\frac{\\partial c_b}{\\partial u} & \\frac{\\partial c_b}{\\partial v} \\\\ \\frac{\\partial c_a}{\\partial u} & \\frac{\\partial c_a}{\\partial v} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial c_r}{\\partial x} & \\frac{\\partial c_r}{\\partial y} & \\frac{\\partial c_r}{\\partial z} \\\\ \\frac{\\partial c_g}{\\partial x} & \\frac{\\partial c_g}{\\partial y} & \\frac{\\partial c_g}{\\partial z} \\\\ \\frac{\\partial c_b}{\\partial x} & \\frac{\\partial c_b}{\\partial y} & \\frac{\\partial c_b}{\\partial z} \\\\ \\frac{\\partial c_a}{\\partial x} & \\frac{\\partial c_a}{\\partial y} & \\frac{\\partial c_a}{\\partial z} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\\\ \\frac{\\partial z}{\\partial u} & \\frac{\\partial z}{\\partial v} \\end{pmatrix}\\]\n\n\n\n\n\n\nCore idea: Discrete Approximation by densely sampling points along it: \\[\ns \\approx \\sum_i \\Delta s_i = \\sum_i \\| c(h \\cdot (i + 1)) - c(h \\cdot i) \\|\n\\]\nWhere:\n\n\\(c(t)\\) is the curve parameterized by \\(t \\in [0,1]\\)\n\\(h\\) is the step size between samples\n\\(\\Delta s_i\\) represents the straight-line distance between consecutive points\n\nUsing first-order Taylor expansion around each sampled point: \\[\nc(h \\cdot i + h) \\approx c(h \\cdot i) + c'(h \\cdot i) \\cdot h\n\\]\n\\[\ns \\approx \\sum_i \\| c'(h \\cdot i) \\| \\cdot h\n\\]\n\n\nTaking the limit as \\(h \\to 0\\) gives us the exact arc length formula:\n\\[\ns = \\int_0^1 \\| c'(t) \\| dt\n\\]\nKey concepts:\n\nThe discrete sum becomes a Riemann integral\n\\(\\| c'(t) \\|\\) represents the instantaneous ‚Äúspeed‚Äù along the curve\nThis works for curves in any dimension (2D, 3D, etc.)\n\n\n\n\nWe consider a parametric curve in 2D space:\n\\[\n\\underline{c}(t) = \\frac{1}{2} \\begin{pmatrix}\n\\cos(2\\pi t) + 1 \\\\\n\\sin(2\\pi t) + 1\n\\end{pmatrix}, \\quad t \\in [0,1]\n\\]\nProperties:\n\nThis describes a circle with radius ¬Ω centered at (¬Ω, ¬Ω)\nThe parameter \\(t\\) completes one full revolution as it goes from 0 to 1\n\nFirst, we compute the derivative (velocity vector) by time:\n\\[\n\\underline{c}^{\\prime}(t) = \\begin{pmatrix}\n-\\pi \\sin(2\\pi t) \\\\\n\\pi \\cos(2\\pi t)\n\\end{pmatrix}\n\\]\nThe speed (magnitude of velocity) is interestingly constant for every t, so we just need to calculate once:\n\\[\n\\begin{aligned}\n\\| \\underline{c}^{\\prime}(t) \\| &= \\sqrt{(-\\pi \\sin(2\\pi t))^2 + (\\pi \\cos(2\\pi t))^2} \\\\\n&= \\sqrt{\\pi^2 (\\sin^2 (2\\pi t) + \\cos^2 (2\\pi t))} \\\\\n&= \\sqrt{\\pi^2} = \\pi\n\\end{aligned}\n\\]\nTotal arc-length of the curve:\n\\[\ns = \\int_{0}^{1} \\| \\underline{c}^{\\prime}(t) \\| \\, dt =  \\int_{0}^{1} \\pi \\, dt = \\pi\n\\]\nGoal: now make sure the velocity by length should also be constant (also claled arclength parameterization)\n\nCumulative length function: \\[ s(t) = \\int_{0}^{t} \\pi \\, dt = \\pi t \\]\nInverse relationship: This tells us the time t needed to travel a distance s along the curve. \\[ t(s) = \\frac{s}{\\pi} \\]\nSubstitute \\(t(s)\\) to original \\(c(t)\\) tells the curve‚Äôs position after travel a distance s: \\[\n\\underline{c}(t) = \\underline{c}(t(s)) = \\underline{c}(s) = \\frac{1}{2} \\begin{pmatrix}\n\\cos(2s) + 1 \\\\\n\\sin(2s) + 1\n\\end{pmatrix}, \\quad s \\in [0,\\pi]\n\\]\n\nWhy this matters: This ensures \\(\\| \\underline{c}^{\\prime}(t) \\| = 1\\) everywhere, means the curve travel at constant speed.\n\n\n\n\nBasically just a stretched velocity, implemented by mapping A curve \\(\\underline{c}(t)\\) in parameter space (e.g., texture coordinates) is mapped to a 3D surface (liek a deformable fabric) \\(\\underline{x}(\\underline{\\tau})\\): \\[\n\\underline{x}(\\underline{\\tau}) = \\underline{x}(\\underline{c}(t))\n\\] The physical length on deformable fabric becomes, now we also account this deformable stretching surface: \\[\ns = \\int_{0}^{1} \\left\\| \\frac{d}{dt} \\underline{x}(\\underline{c}(t)) \\right\\| dt\n\\] The derivative decomposes into surface and curve components (Chain Rule Application (remember above?)): \\[\n\\frac{d}{dt} \\underline{x}(\\underline{c}(t)) = \\underbrace{\\nabla_{\\underline{\\tau}} \\underline{x}}_{\\text{Surface Jacobian}} \\cdot \\underbrace{\\frac{d\\underline{c}}{dt}}_{\\text{Curve velocity}}\n\\]\nComponent dimensions:\n\n\\(\\nabla_{\\underline{\\tau}} \\underline{x}\\): \\(3 \\times 2\\) matrix (tangent vectors)\n\\(\\frac{d\\underline{c}}{dt}\\): \\(2 \\times 1\\) vector\n\nFinally the general form:\n\\[\ns = \\int_{0}^{1} \\left\\| \\nabla_{\\underline{\\tau}} \\underline{x}(\\underline{c}(t)) \\cdot \\underline{c}'(t) \\right\\| dt\n\\]\n\nThe integrand \\(\\left\\| J \\cdot \\underline{c}'(t) \\right\\|\\) is the stretched speed:\n\n\\(J\\) encodes how the surface stretches/compresses space\n\\(\\underline{c}'(t)\\) is the original curve‚Äôs velocity\n\n\n\n\n\n\n\nCenter at \\((0.5, 0.5)\\) with radius 0.5 in parameter space: \\[\n\\underline{c}(t) = \\frac{1}{2} \\begin{pmatrix}\n\\cos(2\\pi t) + 1 \\\\\n\\sin(2\\pi t) + 1\n\\end{pmatrix}, \\quad t \\in [0,1]\n\\]\nDerivative (velocity): \\[\n\\underline{c}'(t) = \\begin{pmatrix}\n-\\pi \\sin(2\\pi t) \\\\\n\\pi \\cos(2\\pi t)\n\\end{pmatrix}\n\\]\n\n\n\nA surface that is stretched as a parabol: \\[\n\\underline{x}(\\underline{\\tau}) = \\underline{x}(u,v) = \\begin{pmatrix}\nu \\\\\nv \\\\\nu^2 + v^2\n\\end{pmatrix}\n\\]\nSurface Jacobian (tangential vectors): \\[\n\\nabla_{\\underline{\\tau}} \\underline{x} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n2u & 2v\n\\end{pmatrix}\n\\]\nInterpretation:\n\nThe last row shows how the surface ‚Äúbends‚Äù in the z-direction\nFirst two rows maintain the identity mapping in x and y\n\n\n\n\nEvaluate the Jacobian at \\(\\underline{c}(t)\\): \\[\n\\nabla_{\\underline{\\tau}} \\underline{x}(\\underline{c}(t)) = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\cos(2\\pi t) + 1 & \\sin(2\\pi t) + 1\n\\end{pmatrix}\n\\] Key observation: The z-component now varies with the curve‚Äôs position\n\n\n\nMultiply Jacobian by curve velocity: \\[\n\\nabla_{\\underline{\\tau}} \\underline{x} \\cdot \\underline{c}'(t) = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\cos(2\\pi t) + 1 & \\sin(2\\pi t) + 1\n\\end{pmatrix} \\begin{pmatrix}\n-\\pi \\sin(2\\pi t) \\\\\n\\pi \\cos(2\\pi t)\n\\end{pmatrix}\n\\] \\[\n\\nabla_{\\underline{\\tau}} \\underline{x} \\cdot \\underline{c}'(t) = \\begin{pmatrix}\n-\\pi \\sin(2\\pi t) \\\\\n\\pi \\cos(2\\pi t) \\\\\n\\pi (\\cos(2\\pi t) - \\sin(2\\pi t))\n\\end{pmatrix}\n\\]\nCompute the magnitude: \\[\n\\begin{aligned}\n\\left\\| \\nabla_{\\underline{\\tau}} \\underline{x} \\cdot \\underline{c}'(t) \\right\\| &= \\pi \\sqrt{\\sin^2(2\\pi t) + \\cos^2(2\\pi t) + (\\cos(2\\pi t) - \\sin(2\\pi t))^2} \\\\\n&= \\pi \\sqrt{1 + 1 - 2\\sin(2\\pi t)\\cos(2\\pi t)} \\\\\n&= \\pi \\sqrt{2 - \\sin(4\\pi t)}\n\\end{aligned}\n\\]\nFinal total length becomes: \\[\ns = \\pi \\int_{0}^{1} \\sqrt{2 - \\sin(4\\pi t)} \\, dt \\approx 1.39\\pi\n\\]\nNumerical insight:\n\nThe integral evaluates to ‚âà1.39 (requires numerical methods)\nCompare to flat case (where \\(s=\\pi\\))\nThe surface curvature adds ~39% more length\n\n\n\n\n\n\n\n\n\n\nConsider a surface \\(S\\) parameterized by: \\[\n\\underline{x}(u,v) : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\n\\] where \\(\\underline{\\tau} = (u,v)\\) are the 2D parameter coordinates.\n\n\n\nThe partial derivatives form a basis for the tangent plane: \\[\n\\underline{x}_u = \\frac{\\partial \\underline{x}}{\\partial u}, \\quad\n\\underline{x}_v = \\frac{\\partial \\underline{x}}{\\partial v}\n\\]\nPhysical meaning: These vectors span the plane ‚Äútangent‚Äù to the surface at point \\(\\underline{\\tau}\\).\n\n\n\nFor an infinitesimal displacement \\(d\\underline{\\tau} = (du, dv)^T\\) in parameter space, the corresponding 3D displacement on the surface is: \\[\nd\\underline{x} = \\underline{x}_u du + \\underline{x}_v dv = \\begin{pmatrix} \\underline{x}_u & \\underline{x}_v \\end{pmatrix} \\begin{pmatrix} du \\\\ dv \\end{pmatrix}\n\\]\nThe squared length in 3D is: \\[\n\\|d\\underline{x}\\|^2 = \\langle d\\underline{x}, d\\underline{x} \\rangle = \\langle \\underline{x}_u du + \\underline{x}_v dv, \\underline{x}_u du + \\underline{x}_v dv \\rangle\n\\] \\[\n\\|d\\underline{x}\\|^2 = \\underbrace{\\langle \\underline{x}_u, \\underline{x}_u \\rangle}_{E} du^2 + 2 \\underbrace{\\langle \\underline{x}_u, \\underline{x}_v \\rangle}_{F} dudv + \\underbrace{\\langle \\underline{x}_v, \\underline{x}_v \\rangle}_{G} dv^2\n\\]\nWe can eventually write this quadratic form as a matrix: \\[\n\\|d\\underline{x}\\|^2 = \\begin{pmatrix} du & dv \\end{pmatrix}\n\\underbrace{\\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix}}_{I_{\\underline{\\tau}}^S}\n\\begin{pmatrix} du \\\\ dv \\end{pmatrix}\n\\]\nThus, the First Fundamental Form emerges naturally as the matrix: \\[\nI_{\\underline{\\tau}}^S = \\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix} = \\begin{pmatrix} \\underline{x}_u^T \\underline{x}_u & \\underline{x}_u^T \\underline{x}_v \\\\ \\underline{x}_u^T \\underline{x}_v & \\underline{x}_v^T \\underline{x}_v \\end{pmatrix}\n\\]\nComponents:\n\n\\(E = \\|\\underline{s}_u\\|^2\\): Squared length of u-tangent vector\n\\(G = \\|\\underline{s}_v\\|^2\\): Squared length of v-tangent vector\n\n\\(F = \\langle \\underline{s}_u, \\underline{s}_v \\rangle\\): Cosine similarity between tangent vectors\n\nThe FFF is the basically just quadratic the Jacobian: \\[\nI_{\\underline{\\tau}}^S = J^T J \\quad \\text{where} \\quad J = [\\underline{x}_u \\ \\underline{x}_v] \\ \\text{(Jacobian)}\n\\]\n\n\n\n\n\n\nFor any vectors in parameter space \\(\\mathbb{R}^3\\) where we draw the curve originally on, the length of this vector on parameter surface is: \\[ \\|\\vec{v}\\|^2 = \\langle \\vec{v}, \\vec{v} \\rangle \\]\n\n\n\n\\[\n\\|\\vec{v}\\|_{I_{\\underline{\\tau}}^S} = \\sqrt{\\langle \\vec{v}, \\vec{v} \\rangle_{I_{\\underline{\\tau}}^S}} = \\sqrt{\\vec{v}^T I_{\\underline{\\tau}}^S \\vec{v}}\n\\]\n\n\n\nWell, simply add all small displacement that together, remember that each point \\(\\underline{\\tau}\\) or \\((u,v)\\) from parameter space will have a totally different stretch \\(I_{\\underline{\\tau}}^S\\) on the surface, so the stretch at each point is different. \\[s = \\int_0^1 \\|\\underline{c}'(t)\\|_{I_{\\underline{\\tau}}^S} dt = \\int_0^1 \\sqrt{ \\langle \\underline{c}'(t), \\underline{c}'(t) \\rangle_{I_{\\underline{\\tau}}^S}} dt = \\int_0^1 \\sqrt{ \\underline{c}'(t) {I_{\\underline{\\tau}}^S} \\underline{c}'(t)} dt\\]\n\n\n\nWe can also decompose in eigenvalues:\n\\[\nI_{\\underline{\\tau}}^S = \\begin{pmatrix}\nE & F \\\\\nF & G\n\\end{pmatrix} = \\lambda_1 \\hat{v}_1 \\hat{v}_1^T + \\lambda_2 \\hat{v}_2 \\hat{v}_2^T\n\\]\n\n\n\n\n\\(\\lambda_i\\): Principal stretching factors\n\\(\\hat{v}_i\\): Principal directions in parameter space\nOrthonormal basis: The eigenvectors form a coordinate system for the surface at point \\(\\underline{\\tau}\\) or \\((u,v)\\)\n\nFor each vector \\(W = (w_1, w_2)^T\\) in the eigenbasis at point \\(\\underline{\\tau}\\) aka \\((u,v)\\), it will be stretched like this:\n\\[\n\\|W\\|_{I_{\\underline{\\tau}}^S} = \\sqrt{\\lambda_1 w_1^2 + \\lambda_2 w_2^2}\n\\]\n\n\n\n\nFor any vectors in parameter space \\(\\mathbb{R}^3\\) where we draw the curve originally on: \\[ \\cos \\theta(\\vec{v}, \\vec{w}) = \\frac{\\langle \\vec{v}, \\vec{w} \\rangle}{\\|\\vec{v}\\|\\|\\vec{w}\\|} \\]\nNow we MAP ON THE SURFACE with surface parameterization \\(\\underline{s}(\\vec{t})\\), the upper part is: \\[ \\langle \\underline{s}_{\\vec{t}} \\vec{v}, \\underline{s}_{\\vec{t}} \\vec{w} \\rangle = \\vec{v}^T \\underbrace{(\\underline{s}_{\\vec{t}}^T \\underline{s}_{\\vec{t}})}_{\\text{FFF Matrix } \\Gamma} \\vec{w} \\]\nWe can also write it like this: \\[\n\\langle \\vec{v}, \\vec{w} \\rangle_{I_{\\underline{\\tau}}^S} = \\vec{v}^T I_{\\underline{\\tau}}^S \\vec{w}\n\\]\nAnd the bottom part is yeah actually we just learned from above: \\[\n\\|\\vec{v}\\|_{I_{\\underline{\\tau}}^S} = \\sqrt{\\langle \\vec{v}, \\vec{v} \\rangle_{I_{\\underline{\\tau}}^S}} = \\sqrt{\\vec{v}^T I_{\\underline{\\tau}}^S \\vec{v}}\n\\]\nTo assemble we will have: \\[\n\\cos \\theta = \\frac{\\langle \\vec{v}, \\vec{w} \\rangle_{I_{\\underline{\\tau}}^S}}{\\|\\vec{v}\\|_{I_{\\underline{\\tau}}^S} \\|\\vec{w}\\|_{I_{\\underline{\\tau}}^S}} = \\frac{\\vec{v}^T I_{\\underline{\\tau}}^S \\vec{w}}{\\sqrt{\\vec{v}^T I_{\\underline{\\tau}}^S \\vec{v}} \\sqrt{\\vec{w}^T I_{\\underline{\\tau}}^S \\vec{w}}}\n\\] Angle \\(\\theta\\) between two directions \\(d\\vec{t}_1\\), \\(d\\vec{t}_2\\): \\[ \\cos \\theta = \\frac{Edu_1du_2 + F(du_1dv_2 + du_2dv_1) + Gdv_1dv_2}{\\sqrt{Edu_1^2 + 2Fdu_1dv_1 + Gdv_1^2}\\sqrt{Edu_2^2 + 2Fdu_2dv_2 + Gdv_2^2}} \\]\n\n\n\nCross Product area of an infinitesimal parallelogram spanned by \\(\\underline{s}_u du\\) and \\(\\underline{s}_v dv\\) is: \\[\ndA = \\|(\\underline{s}_u du) \\times (\\underline{s}_v dv)\\| = \\|\\underline{s}_u \\times \\underline{s}_v\\| du dv\n\\]\nWe can express this purely in terms of the FFF coefficients:\n\nStart with the cross product magnitude identity: \\[\n\\|\\underline{s}_u \\times \\underline{s}_v\\|^2 = \\|\\underline{s}_u\\|^2 \\|\\underline{s}_v\\|^2 - \\langle \\underline{s}_u, \\underline{s}_v \\rangle^2\n\\]\nRecognize this matches the determinant of \\(I_{\\underline{\\tau}}^S\\): \\[\n\\det(I_{\\underline{\\tau}}^S) = EG - F^2\n\\]\nThus: \\[\ndA = \\sqrt{EG - F^2} du dv\n\\]\n\n\n\n\\[\n\\|\\vec{a} \\times \\vec{b}\\|^2 + |\\vec{a} \\cdot \\vec{b}|^2 = \\|\\vec{a}\\|^2 \\|\\vec{b}\\|^2\n\\]\n\n\n\nCross Product Definition\nMeasures ‚Äúperpendicularity‚Äù (the area of the parallelogram formed) \\[\n\\|\\vec{a} \\times \\vec{b}\\| = \\|\\vec{a}\\|\\|\\vec{b}\\|\\sin\\theta\n\\] \\[\n\\|\\vec{a} \\times \\vec{b}\\|^2 = \\|\\vec{a}\\|^2\\|\\vec{b}\\|^2\\sin^2\\theta\n\\]\nDot Product Definition Measures ‚Äúperpendicularity‚Äù (the area of the parallelogram formed) Measures ‚Äúparallelism‚Äù (how aligned the vectors are) \\[\n\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos\\theta\n\\] \\[\n|\\vec{a} \\cdot \\vec{b}|^2 = \\|\\vec{a}\\|^2\\|\\vec{b}\\|^2\\cos^2\\theta\n\\]\nCombining Both Terms\nRepresents the total magnitude regardless of angle \\[\n\\|\\vec{a} \\times \\vec{b}\\|^2 + |\\vec{a} \\cdot \\vec{b}|^2 = \\|\\vec{a}\\|^2\\|\\vec{b}\\|^2(\\sin^2\\theta + \\cos^2\\theta) = \\|\\vec{a}\\|^2\\|\\vec{b}\\|^2\n\\]\n\n\n\n\n\n\n\n\nWe start with the spherical coordinates parameterization: \\[\n\\underline{S}(u,v) = r \\begin{pmatrix}\n\\cos u \\cos v \\\\\n\\sin u \\cos v \\\\\n\\sin v\n\\end{pmatrix}, \\quad\n\\begin{cases}\nu \\in [-\\pi, \\pi] \\ (\\text{longitude}) \\\\\nv \\in [-\\frac{\\pi}{2}, \\frac{\\pi}{2}] \\ (\\text{latitude})\n\\end{cases}\n\\]\n\n\n\nPartial derivatives form the tangent basis: \\[\n\\underline{S}_u = r \\begin{pmatrix}\n-\\sin u \\cos v \\\\\n\\cos u \\cos v \\\\\n0\n\\end{pmatrix}, \\quad\n\\underline{S}_v = r \\begin{pmatrix}\n-\\cos u \\sin v \\\\\n-\\sin u \\sin v \\\\\n\\cos v\n\\end{pmatrix}\n\\]\n\n\n\nCalculate the metric tensor coefficients: \\[\n\\begin{aligned}\nE &= \\|\\underline{S}_u\\|^2 = r^2 \\cos^2 v (\\sin^2 u + \\cos^2 u) = r^2 \\cos^2 v \\\\\nF &= \\langle \\underline{S}_u, \\underline{S}_v \\rangle = 0 \\quad \\text{(orthogonal coordinates)} \\\\\nG &= \\|\\underline{S}_v\\|^2 = r^2 (\\cos^2 u \\sin^2 v + \\sin^2 u \\sin^2 v + \\cos^2 v) = r^2\n\\end{aligned}\n\\]\nThus, the metric tensor is: \\[\nI = \\begin{pmatrix}\nr^2 \\cos^2 v & 0 \\\\\n0 & r^2\n\\end{pmatrix}\n\\]\n\n\n\nThe area element derives from the metric determinant: \\[\n\\begin{aligned}\ndA &= \\sqrt{\\det I} \\, du dv = \\sqrt{r^4 \\cos^2 v} \\, du dv \\\\\n&= r^2 |\\cos v| \\, du dv\n\\end{aligned}\n\\]\n\n\n\nIntegrate over the parameter domain: \\[\n\\begin{aligned}\nA &= \\int_{-\\pi}^{\\pi} \\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}} r^2 |\\cos v| \\, dv du \\\\\n&= r^2 \\int_{-\\pi}^{\\pi} du \\cdot 2 \\int_{0}^{\\frac{\\pi}{2}} \\cos v \\, dv \\quad \\text{(by symmetry)} \\\\\n&= r^2 (2\\pi) \\cdot 2 \\left[ \\sin v \\right]_0^{\\frac{\\pi}{2}} \\\\\n&= 4\\pi r^2\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nThe method ([Gum ‚Äô04]) designs smooth curves through control points by optimizing a combined length-curvature functional:\n\\[\nF_{simp}(c, \\lambda) = \\int_{s=0}^{L(c)} \\underbrace{(1 - \\lambda)q}_{\\text{length term}} + \\underbrace{\\lambda |\\kappa L_0|^2}_{\\text{curvature term}} ds\n\\]\nwhere:\n\n\\(c\\): Curve parameterization\n\\(\\lambda \\in [0,1]\\): Blending parameter\n\\(L_0\\): Length of control polygon\n\\(q = 5000\\): Normalization constant\n\\(\\kappa\\): Curvature at each point\n\n\n\n\nOptimization Objective: \\[\n\\text{C}_{simp}(\\lambda) = \\min_{\\mathbf{c}: \\mathbf{c}(x_i) = \\mathbf{p}_i} F_{simp}(\\mathbf{c}, \\lambda)\n\\]\nBehavioral Spectrum:\n\n\\(\\lambda = 0\\): Pure length minimization ‚Üí yields control polygon (blue curve)\n\\(\\lambda = 1\\): Pure curvature minimization ‚Üí yields smoothest curve (red curve)\nIntermediate \\(\\lambda\\): Balanced trade-off\n\n\n\n\n\n\n\nFind a curve \\(\\mathbf{c}(s)\\) that:\n\nExactly interpolates given control points \\(\\{\\mathbf{p}_i\\}_{i=0}^n\\)\nMinimizes the hybrid energy functional: \\[\nF_{simp}(\\mathbf{c}, \\lambda) = \\int \\underbrace{(1-\\lambda)q}_{\\text{length term}} + \\underbrace{\\lambda|\\kappa(s)L_0|^2}_{\\text{curvature term}} ds\n\\]\n\n\n\n\n\n\nUse piecewise cubic Hermite splines between each \\(\\mathbf{p}_i\\) and \\(\\mathbf{p}_{i+1}\\): \\[\n\\mathbf{c}_i(t) = \\mathbf{a}_i t^3 + \\mathbf{b}_i t^2 + \\mathbf{c}_i t + \\mathbf{d}_i, \\quad t \\in [0,1]\n\\]\n\n\n\n\nPosition: \\[\\mathbf{c}_i(0) = \\mathbf{p}_i, \\quad \\mathbf{c}_i(1) = \\mathbf{p}_{i+1}\\]\nTangent continuity (with \\(\\mathbf{m}_i\\) as unknown tangent vectors): \\[\\mathbf{c}_i'(0) = \\mathbf{m}_i, \\quad \\mathbf{c}_i'(1) = \\mathbf{m}_{i+1}\\]\n\n\n\n\nExpressed in terms of curve derivatives: \\[\nF_{simp} = \\sum_{i=0}^{n-1} \\int_0^1 \\left[\n(1-\\lambda)q \\|\\mathbf{c}_i'(t)\\| +\n\\lambda L_0^2 \\left(\\frac{\\|\\mathbf{c}_i'(t) \\times \\mathbf{c}_i''(t)\\|}{\\|\\mathbf{c}_i'(t)\\|^3}\\right)^2\n\\right] dt\n\\]\n\n\n\n\n\n\nExpress cubic coefficients using control points and tangents: \\[\n\\begin{aligned}\n\\mathbf{a}_i &= 2(\\mathbf{p}_i - \\mathbf{p}_{i+1}) + \\mathbf{m}_i + \\mathbf{m}_{i+1} \\\\\n\\mathbf{b}_i &= -3(\\mathbf{p}_i - \\mathbf{p}_{i+1}) - 2\\mathbf{m}_i - \\mathbf{m}_{i+1} \\\\\n\\mathbf{c}_i &= \\mathbf{m}_i \\\\\n\\mathbf{d}_i &= \\mathbf{p}_i\n\\end{aligned}\n\\]\n\n\n\nFor energy calculation: \\[\n\\begin{aligned}\n\\mathbf{c}_i'(t) &= 3\\mathbf{a}_i t^2 + 2\\mathbf{b}_i t + \\mathbf{c}_i \\\\\n\\mathbf{c}_i''(t) &= 6\\mathbf{a}_i t + 2\\mathbf{b}_i\n\\end{aligned}\n\\]\n\n\n\n\nInitial guess: Set \\(\\mathbf{m}_i = \\frac{\\mathbf{p}_{i+1} - \\mathbf{p}_{i-1}}{2}\\) (central differences)\nGradient descent: \\[\n\\mathbf{m}_i^{(k+1)} = \\mathbf{m}_i^{(k)} - \\gamma \\frac{\\partial F_{simp}}{\\partial \\mathbf{m}_i}\n\\] where the gradient involves: \\[\n\\frac{\\partial F_{simp}}{\\partial \\mathbf{m}_i} = \\int_0^1 \\left[\n(1-\\lambda)q \\frac{\\partial \\|\\mathbf{c}_i'\\|}{\\partial \\mathbf{m}_i} +\n2\\lambda L_0^2 \\kappa \\frac{\\partial \\kappa}{\\partial \\mathbf{m}_i}\n\\right] dt\n\\]\n\n\n\n\n\nGiven \\(\\mathbf{p}_0, \\mathbf{p}_1, \\mathbf{p}_2\\) with \\(\\mathbf{p}_1\\) at \\(t^*=0.5\\):\n\nSingle cubic segment with constraint \\(\\mathbf{c}(0.5) = \\mathbf{p}_1\\)\nSolve for \\(\\mathbf{m}_0, \\mathbf{m}_1, \\mathbf{m}_2\\) minimizing \\(F_{simp}\\)\nFinal curve guarantees:\n\n\\(\\mathbf{c}(0) = \\mathbf{p}_0\\), \\(\\mathbf{c}(0.5) = \\mathbf{p}_1\\), \\(\\mathbf{c}(1) = \\mathbf{p}_2\\)\nMinimal energy for chosen \\(\\lambda\\)\n\n\n\n\n\n\n\n\nCurvature measures how fast the tangent vector turns per unit distance traveled. We need to:\n\nTrack how much the tangent direction changes (numerator)\nNormalize by how fast we‚Äôre moving (denominator)\n\n\n\n\n\n\nDefine the unit tangent vector: \\[\n\\vec{T}(t) = \\frac{\\vec{c}_t(t)}{\\|\\vec{c}_t(t)\\|}\n\\] (Note: \\(\\vec{c}_t = \\frac{d\\vec{c}}{dt}\\) is the velocity vector)\n\n\n\nThe curvature comes from \\(\\frac{d\\vec{T}}{ds}\\) (how \\(\\vec{T}\\) changes with arc length \\(s\\)).\nFirst, apply chain rule: \\[\n\\frac{d\\vec{T}}{dt} = \\frac{d}{dt}\\left(\\frac{\\vec{c}_t}{\\|\\vec{c}_t\\|}\\right) = \\frac{\\vec{c}_{tt}\\|\\vec{c}_t\\| - \\vec{c}_t(\\vec{c}_t \\cdot \\vec{c}_{tt})/\\|\\vec{c}_t\\|}{\\|\\vec{c}_t\\|^2}\n\\]\n\n\n\nCurvature only cares about perpendicular changes to \\(\\vec{T}\\) (lateral bending). The tangential component affects speed, not shape.\nThe perpendicular part is: \\[\n\\left(\\frac{d\\vec{T}}{dt}\\right)_{\\perp} = \\frac{\\vec{c}_{tt}}{\\|\\vec{c}_t\\|} - \\frac{\\vec{c}_t (\\vec{c}_t \\cdot \\vec{c}_{tt})}{\\|\\vec{c}_t\\|^3}\n\\]\n\n\n\nCompute \\(\\|\\frac{d\\vec{T}}{dt}\\|\\) using vector identity \\(\\|\\vec{a}\\| = \\sqrt{\\vec{a} \\cdot \\vec{a}}\\): \\[\n\\left\\|\\left(\\frac{d\\vec{T}}{dt}\\right)_{\\perp}\\right\\| = \\frac{\\|\\vec{c}_t \\times \\vec{c}_{tt}\\|}{\\|\\vec{c}_t\\|^2}\n\\]\n\n\n\nSince curvature is \\(\\kappa = \\|\\frac{d\\vec{T}}{ds}\\|\\) and \\(ds = \\|\\vec{c}_t\\|dt\\): \\[\n\\kappa = \\left\\|\\frac{d\\vec{T}}{ds}\\right\\| = \\frac{\\|\\frac{d\\vec{T}}{dt}\\|}{ds/dt} = \\frac{\\|\\vec{c}_t \\times \\vec{c}_{tt}\\|/\\|\\vec{c}_t\\|^2}{\\|\\vec{c}_t\\|} = \\frac{\\|\\vec{c}_t \\times \\vec{c}_{tt}\\|}{\\|\\vec{c}_t\\|^3}\n\\]\n\n\n\n\nThe denominator accounts for: 1. One \\(\\|\\vec{c}_t\\|\\): Converts time derivative to arc-length derivative (\\(ds = \\|\\vec{c}_t\\|dt\\)) 2. Two more \\(\\|\\vec{c}_t\\|\\) terms:\n\nOne from normalizing the tangent vector (\\(\\vec{T} = \\vec{c}_t/\\|\\vec{c}_t\\|\\))\nOne from isolating the perpendicular component\n\n\n\n\n\nNumerator \\(\\|\\vec{c}_t \\times \\vec{c}_{tt}\\|\\): Measures the area of the parallelogram formed by velocity and acceleration ‚Üí pure bending effect\nDenominator \\(\\|\\vec{c}_t\\|^3\\): Normalizes for:\n\nParameterization speed (\\(\\|\\vec{c}_t\\|\\))\nUnit tangent construction (two \\(\\|\\vec{c}_t\\|\\) terms)\n\n\n\n\n\nFor \\(\\vec{c}(t) = (r\\cos t, r\\sin t)\\):\n\n\\(\\vec{c}_t = (-r\\sin t, r\\cos t)\\) ‚Üí \\(\\|\\vec{c}_t\\| = r\\)\n\\(\\vec{c}_{tt} = (-r\\cos t, -r\\sin t)\\)\n\\(\\vec{c}_t \\times \\vec{c}_{tt} = r^2\\)\n\\(\\kappa = \\frac{r^2}{r^3} = \\frac{1}{r}\\) (correct for circle radius \\(r\\))\n\n\n\n\n\n\n\n\nSurface Parameterization:\n\n\\(\\underline{S}(u,v)\\): A 2D ‚Üí 3D mapping defining the surface\n\\(\\underline{S}_u = \\frac{\\partial \\underline{S}}{\\partial u}\\): Partial derivative in u-direction (tangent vector)\n\\(\\underline{S}_v = \\frac{\\partial \\underline{S}}{\\partial v}\\): Partial derivative in v-direction (tangent vector)\n\nFirst Fundamental Form (Metric Tensor): \\[I = \\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix} = \\begin{pmatrix} \\langle \\underline{S}_u, \\underline{S}_u \\rangle & \\langle \\underline{S}_u, \\underline{S}_v \\rangle \\\\ \\langle \\underline{S}_u, \\underline{S}_v \\rangle & \\langle \\underline{S}_v, \\underline{S}_v \\rangle \\end{pmatrix}\\]\nUnit Normal Vector: \\[\\hat{n} = \\frac{\\underline{S}_u \\times \\underline{S}_v}{\\|\\underline{S}_u \\times \\underline{S}_v\\|}\\]\n\n\n\n\nFor a given point \\(\\underline{p}\\) on the surface:\n\nDefinition: Curvature of the intersection curve created by slicing the surface with a plane containing:\n\nThe surface normal \\(\\hat{n}\\)\nA tangent vector \\(\\underline{v} = \\cosŒ±\\,\\underline{S}_u + \\sinŒ±\\,\\underline{S}_v\\)\n\nAngle Parameter:\n\n\\(Œ±\\): Angle between \\(\\underline{S}_u\\) and the chosen tangent direction \\(\\underline{v}\\)\n\\(Œ≤ = Œ± - Œ±_0\\): Angle relative to principal direction\n\n\n\n\n\nThere exist two orthogonal principal directions where normal curvature reaches extremal values: \\[Œ∫_n(Œ≤) = Œ∫_{\\min}\\cos^2Œ≤ + Œ∫_{\\max}\\sin^2Œ≤\\]\n\n\\(Œ∫_{\\min}\\): Minimum normal curvature\n\\(Œ∫_{\\max}\\): Maximum normal curvature\n\\(Œ±_0\\): Angle of first principal direction from \\(\\underline{S}_u\\)\n\n\n\n\nMeasures how the surface bends in 3D space: \\[II = \\begin{pmatrix} L & M \\\\ M & N \\end{pmatrix}\\] where:\n\n\\(L = \\langle \\underline{S}_{uu}, \\hat{n} \\rangle\\)\n\\(M = \\langle \\underline{S}_{uv}, \\hat{n} \\rangle\\)\n\\(N = \\langle \\underline{S}_{vv}, \\hat{n} \\rangle\\)\n\n\n\n\n\nGaussian Curvature (intrinsic): \\[K = Œ∫_{\\min}Œ∫_{\\max} = \\frac{LN - M^2}{EG - F^2}\\]\nMean Curvature (extrinsic): \\[H = \\frac{Œ∫_{\\min} + Œ∫_{\\max}}{2} = \\frac{EN - 2FM + GL}{2(EG - F^2)}\\]\nPrincipal Curvatures: \\[Œ∫_{\\max,\\min} = H \\pm \\sqrt{H^2 - K}\\]\n\n\n\n\n\nPositive Gaussian Curvature (\\(K &gt; 0\\)): Bowl-like (both curvatures same sign)\nNegative Gaussian Curvature (\\(K &lt; 0\\)): Saddle-like (opposite signs)\nZero Gaussian Curvature (\\(K = 0\\)): Developable surface (at least one zero curvature)\n\n\n\n\nTo find principal directions: 1. Solve eigenvalue problem for the shape operator: \\[S = I^{-1}II\\] 2. Eigenvalues ‚Üí principal curvatures 3. Eigenvectors ‚Üí principal directions\n\n\n\n\n\\(Œ∫_{\\min} = 0\\) (along axis)\n\\(Œ∫_{\\max} = 1/r\\) (around circumference)\n\\(K = 0\\) (developable)\n\\(H = 1/(2r)\\)\n\n\n\n\n\n\n\nWhile the First Fundamental Form (I) measures distances on the surface, the Second Fundamental Form (II) quantifies how the surface bends in 3D space by tracking normal vector variations.\n\n\n\n\n\nConsider a surface patch \\(\\underline{S}(u,v)\\) with unit normal \\(\\hat{n}(u,v)\\). The rate of normal change reveals curvature:\n\\[\nd\\hat{n} = \\hat{n}_u du + \\hat{n}_v dv\n\\]\n\n\n\nSince \\(\\hat{n} \\perp\\) tangent plane, we have: \\[\n\\langle \\hat{n}, \\underline{S}_u \\rangle = \\langle \\hat{n}, \\underline{S}_v \\rangle = 0\n\\]\nDifferentiating these gives the Weingarten equations: \\[\n\\begin{aligned}\n\\langle \\hat{n}_u, \\underline{S}_u \\rangle &= -\\langle \\hat{n}, \\underline{S}_{uu} \\rangle = -L \\\\\n\\langle \\hat{n}_u, \\underline{S}_v \\rangle &= -\\langle \\hat{n}, \\underline{S}_{uv} \\rangle = -M \\\\\n\\langle \\hat{n}_v, \\underline{S}_v \\rangle &= -\\langle \\hat{n}, \\underline{S}_{vv} \\rangle = -N\n\\end{aligned}\n\\]\n\n\n\nThe second fundamental form emerges when expressing the normal component of surface acceleration:\nFor a curve \\(\\underline{c}(t) = \\underline{S}(u(t),v(t))\\): \\[\n\\underline{c}''(t) = \\underline{S}_{uu}(u')^2 + 2\\underline{S}_{uv}u'v' + \\underline{S}_{vv}(v')^2 + \\text{tangent terms}\n\\]\nThe normal curvature is: \\[\n\\kappa_n = \\frac{\\langle \\underline{c}'', \\hat{n} \\rangle}{\\langle \\underline{c}', \\underline{c}' \\rangle} = \\frac{L(u')^2 + 2Mu'v' + N(v')^2}{E(u')^2 + 2Fu'v' + G(v')^2}\n\\]\nThus we define: \\[\nII = \\begin{pmatrix} L & M \\\\ M & N \\end{pmatrix} = \\begin{pmatrix}\n\\langle \\underline{S}_{uu}, \\hat{n} \\rangle & \\langle \\underline{S}_{uv}, \\hat{n} \\rangle \\\\\n\\langle \\underline{S}_{uv}, \\hat{n} \\rangle & \\langle \\underline{S}_{vv}, \\hat{n} \\rangle\n\\end{pmatrix}\n\\]\n\n\n\n\n\nHistorical Context:\n\nFirst Fundamental Form (I) measures intrinsic properties (lengths/angles)\nSecond Fundamental Form (II) captures extrinsic bending (how the surface embeds in 3D space)\n\nMathematical Hierarchy:\n\nI comes from the metric tensor (1st derivatives)\nII requires 2nd derivatives and normal information\n\nDuality:\n\nI: \\(\\langle d\\underline{S}, d\\underline{S} \\rangle\\) (dot product of tangent vectors)\nII: \\(\\langle d^2\\underline{S}, \\hat{n} \\rangle\\) (projection of curvature onto normal)\n\n\n\n\n\nThe quadratic form \\(II(v,v)\\) gives the normal curvature in direction \\(v\\):\n\nPositive value: Surface bends toward \\(\\hat{n}\\)\nNegative value: Surface bends away from \\(\\hat{n}\\)\nZero value: Asymptotic direction (no normal bending)\n\n\n\n\nFor \\(\\underline{S}(\\theta,\\phi) = R(\\sin\\theta\\cos\\phi, \\sin\\theta\\sin\\phi, \\cos\\theta)\\):\n\n\\(\\hat{n} = (\\sin\\theta\\cos\\phi, \\sin\\theta\\sin\\phi, \\cos\\theta)\\)\n\\(L = \\langle \\underline{S}_{\\theta\\theta}, \\hat{n} \\rangle = -R\\)\n\\(M = \\langle \\underline{S}_{\\theta\\phi}, \\hat{n} \\rangle = 0\\)\n\\(N = \\langle \\underline{S}_{\\phi\\phi}, \\hat{n} \\rangle = -R\\sin^2\\theta\\)\n\nThus: \\[\nII = \\begin{pmatrix} -R & 0 \\\\ 0 & -R\\sin^2\\theta \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#representation",
    "href": "posts/cg1-meshprocessing/index.html#representation",
    "title": "Surface Analysis",
    "section": "",
    "text": "There are three main ways to represent an analytic surface:\n\n\nat any given \\((x,y)\\) coordinate, the function gives a surface‚Äôs height (\\(z\\)). Downside that only for surface that doesnt overlap the z value more than once.\n\nEquation: \\(z=f(x,y)\\)\nExample: A plane \\(z=ax+by+c\\).\n\n\n\n\nThis is a vector-valued function that maps a 2D parameter space (defined by parameters like \\(u\\) and \\(v\\)) to a 3D surface. This is a very flexible representation that can describe a wide range of shapes, including closed surfaces, so we focus on this.\n\nEquation: \\(\\mathbf{S}(u,v)=\\begin{pmatrix} x(u,v) \\\\ y(u,v) \\\\ z(u,v) \\end{pmatrix}\\)\nExample: A sphere defined by the equations:\n\n\\(x=r\\cos\\theta\\sin\\phi\\) (Basically each element in the target vector has their own function x, y, z)\n\\(y=r\\sin\\theta\\sin\\phi\\)\n\\(z=r\\cos\\phi\\)\n\n\n\n\n\nThis is a function that defines a surface as the set of all points that satisfy a specific equation, where the function equals a constant (often zero). Implicit surfaces can represent complex, closed shapes and are useful for collision detection and boolean operations.\n\nEquation: \\(f(x,y,z)=0\\)\nExample: A sphere with radius \\(r\\) centered at the origin \\(x^2+y^2+z^2-r^2=0\\)."
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#taylor-expansion",
    "href": "posts/cg1-meshprocessing/index.html#taylor-expansion",
    "title": "Surface Analysis",
    "section": "",
    "text": "How much order we take is the function of that order (first-order is a linear function, second order is a quadratic function (parabol go across point p))\nUnivariate scalar:\n\\[f(x+\\delta) = f(x) + f'(x)\\cdot\\delta + \\frac{1}{2}f''(x)\\cdot\\delta^2 + \\mathcal{O}(\\delta^3)\\]\nMultivariate scalar:\n\\[f(\\mathbf{x}+\\boldsymbol{\\delta}) = f(\\mathbf{x}) + f'_{\\mathbf{x}}(\\mathbf{x})\\cdot\\boldsymbol{\\delta} + \\frac{1}{2}\\boldsymbol{\\delta}^T H_f^\\dagger(\\mathbf{x})\\boldsymbol{\\delta} + \\mathcal{O}(\\left\\|\\boldsymbol{\\delta}\\right\\|^3)\\]\nMultivariate vector-valued: \\[\\mathbf{f}(\\mathbf{x}+\\boldsymbol{\\delta}) = \\mathbf{f}(\\mathbf{x}) + J_{\\mathbf{f}_{\\boldsymbol{\\delta}}}(\\mathbf{x})\\boldsymbol{\\delta} + \\mathcal{O}(\\left\\|\\boldsymbol{\\delta}\\right\\|^2)\\]\n\n\n\\[f: \\mathbb{R}^n \\to \\mathbb{R}\\] e.g.¬†height field \\(h = f(\\mathbf{x}) = f(x,y)\\)\nDerivation w.r.t. any of the variables gives partial derivatives:\n\\[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}: \\mathbb{R}^n \\to \\mathbb{R}\\]\nShort forms: \\(f_x = \\frac{\\partial f}{\\partial x}, f_y = \\frac{\\partial f}{\\partial y}\\)\n2nd deriv.: \\(f_{xx} = \\frac{\\partial^2 f}{\\partial x^2}, f_{yy} = \\frac{\\partial^2 f}{\\partial y^2}, f_{xy} = \\frac{\\partial^2 f}{\\partial x \\partial y}\\)\nThe gradient of a function at a given point is the (row)-vector (in direction of steepest ascent) of partial derivatives (\\(\\nabla_{\\mathbf{x}} f: \\mathbb{R}^n \\to \\mathbb{R}^{1\\times n}\\)):\n\\(\\nabla_{\\mathbf{x}} f = f_{\\mathbf{x}} = (f_x, f_y, \\ldots)\\) (it points in direction of steepest ascent)\n\n\n\n\\[f: \\mathbb{R} \\to \\mathbb{R}^m\\]\nE.g. a curve:\n\\[\\mathbf{f}(t) = \\begin{pmatrix} x(t) \\\\ y(t) \\end{pmatrix} = \\begin{pmatrix} \\cos t \\\\ \\sin t \\end{pmatrix}\\]\nDerivatives are computed per component independently\n\\[\\frac{d\\mathbf{f}}{dt}: \\mathbb{R} \\to \\mathbb{R}^2\\] \\[\\mathbf{f}'(t) = \\begin{pmatrix} x'(t) \\\\ y'(t) \\end{pmatrix}\\]\nWhere:\n\nThe derivative is a tangent vector to the surface\nIts length represents the speed of change.\n\n\n\n\n\\[f: \\mathbb{R}^n \\to \\mathbb{R}^m\\]\nE.g. a surface:\n\\[\\mathbf{s}(\\boldsymbol{\\tau}) = \\mathbf{s}\\begin{pmatrix} u \\\\ v \\end{pmatrix} = \\begin{pmatrix} x(\\boldsymbol{\\tau}) \\\\ y(\\boldsymbol{\\tau}) \\\\ z(\\boldsymbol{\\tau}) \\end{pmatrix} = \\begin{pmatrix} x(u,v) \\\\ y(u,v) \\\\ z(u,v) \\end{pmatrix} = \\begin{pmatrix} \\cos u \\cos v \\\\ \\sin u \\cos v \\\\ \\sin v \\end{pmatrix}\\]\nThe partial derivatives are tangent vectors:\n\\[\\frac{\\partial \\mathbf{s}}{\\partial u} = \\mathbf{s}_u = \\begin{pmatrix} x_u \\\\ y_u \\\\ z_u \\end{pmatrix}\\]\nThe row-vector of partial derivatives is now a matrix says how changes in u and v affect the coordinate x,y,z:\n\\[J_{\\mathbf{s}_{\\boldsymbol{\\tau}}} = \\mathbf{s}_{\\boldsymbol{\\tau}} = \\begin{pmatrix} \\mathbf{s}_u & \\mathbf{s}_v \\end{pmatrix} = \\begin{pmatrix} x_u & x_v \\\\ y_u & y_v \\\\ z_u & z_v \\end{pmatrix}\\]\nThis matrix is called the Jacobian. Basically a transformation matrix that map direction from parameter space to value space. The Jacobian‚Äôs columns are the tangent vectors s_u and s_v in value space.\n\n\n\nWe can calculate the derivative in a specific direction \\(\\boldsymbol{\\delta}\\) (\\(\\left\\|\\boldsymbol{\\delta}\\right\\|=1\\)).\nIn the scalar case, just do the dot product with the gradien, it projects on it:\n\\[\\frac{\\partial f}{\\partial \\boldsymbol{\\delta}}(\\mathbf{x}) = f_{\\mathbf{x}}(\\mathbf{x})\\cdot\\boldsymbol{\\delta} = f_x \\cdot \\delta[x] + f_y \\cdot \\delta[y] + \\ldots\\]\nThis generalizes to vector-valued functions:\n\\[\\frac{\\partial \\mathbf{f}}{\\partial \\boldsymbol{\\delta}}(\\mathbf{x}) = \\mathbf{f}_{\\mathbf{x}}\\cdot\\boldsymbol{\\delta} = \\mathbf{f}_x \\cdot \\delta[x] + \\mathbf{f}_y \\cdot \\delta[y] + \\ldots\\]\nAs said above: The Jacobian‚Äôs columns are the tangent vectors s_u and s_v in value space.\n\n\n\nThe transformation chain can be represented as:\n\\[\n\\begin{align*}\n\\vec{\\tau} = \\begin{pmatrix} u \\\\ v \\end{pmatrix}\n\\xrightarrow{s} \\vec{p} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}\n\\xrightarrow{\\tilde{c}} \\vec{c} = \\begin{pmatrix} r \\\\ g \\\\ b \\\\ a \\end{pmatrix}\n\\end{align*}\n\\]\nChain rule:\n\\[\n[ \\dot{c}(s(\\vec{\\tau})) ]_{\\vec{\\tau}} =\n[ \\dot{c}_{\\vec{p}}(s(\\vec{\\tau})) ] s_{\\vec{\\tau}}(\\vec{\\tau})\n\\]\n\\[\\frac{\\partial \\mathbf{c}}{\\partial \\boldsymbol{\\tau}} = \\frac{\\partial \\mathbf{c}}{\\partial \\mathbf{p}} \\frac{\\partial \\mathbf{p}}{\\partial \\boldsymbol{\\tau}}\\]\nExpressed in component form:\n\\[\n\\begin{pmatrix}\nr_u & r_v \\\\\ng_u & g_v \\\\\nb_u & b_v \\\\\na_u & a_v\n\\end{pmatrix} =\n\\begin{pmatrix}\nr_x & r_y & r_z \\\\\ng_x & g_y & g_z \\\\\nb_x & b_y & b_z \\\\\na_x & a_y & a_z\n\\end{pmatrix}\n\\begin{pmatrix}\nx_u & x_v \\\\\ny_u & y_v \\\\\nz_u & z_v\n\\end{pmatrix}\n\\]\n\\[\\begin{pmatrix} \\frac{\\partial c_r}{\\partial u} & \\frac{\\partial c_r}{\\partial v} \\\\ \\frac{\\partial c_g}{\\partial u} & \\frac{\\partial c_g}{\\partial v} \\\\ \\frac{\\partial c_b}{\\partial u} & \\frac{\\partial c_b}{\\partial v} \\\\ \\frac{\\partial c_a}{\\partial u} & \\frac{\\partial c_a}{\\partial v} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial c_r}{\\partial x} & \\frac{\\partial c_r}{\\partial y} & \\frac{\\partial c_r}{\\partial z} \\\\ \\frac{\\partial c_g}{\\partial x} & \\frac{\\partial c_g}{\\partial y} & \\frac{\\partial c_g}{\\partial z} \\\\ \\frac{\\partial c_b}{\\partial x} & \\frac{\\partial c_b}{\\partial y} & \\frac{\\partial c_b}{\\partial z} \\\\ \\frac{\\partial c_a}{\\partial x} & \\frac{\\partial c_a}{\\partial y} & \\frac{\\partial c_a}{\\partial z} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\\\ \\frac{\\partial z}{\\partial u} & \\frac{\\partial z}{\\partial v} \\end{pmatrix}\\]"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#differential-geometry-in-1st-fundamental-form",
    "href": "posts/cg1-meshprocessing/index.html#differential-geometry-in-1st-fundamental-form",
    "title": "Surface Analysis",
    "section": "",
    "text": "Core idea: Discrete Approximation by densely sampling points along it: \\[\ns \\approx \\sum_i \\Delta s_i = \\sum_i \\| c(h \\cdot (i + 1)) - c(h \\cdot i) \\|\n\\]\nWhere:\n\n\\(c(t)\\) is the curve parameterized by \\(t \\in [0,1]\\)\n\\(h\\) is the step size between samples\n\\(\\Delta s_i\\) represents the straight-line distance between consecutive points\n\nUsing first-order Taylor expansion around each sampled point: \\[\nc(h \\cdot i + h) \\approx c(h \\cdot i) + c'(h \\cdot i) \\cdot h\n\\]\n\\[\ns \\approx \\sum_i \\| c'(h \\cdot i) \\| \\cdot h\n\\]\n\n\nTaking the limit as \\(h \\to 0\\) gives us the exact arc length formula:\n\\[\ns = \\int_0^1 \\| c'(t) \\| dt\n\\]\nKey concepts:\n\nThe discrete sum becomes a Riemann integral\n\\(\\| c'(t) \\|\\) represents the instantaneous ‚Äúspeed‚Äù along the curve\nThis works for curves in any dimension (2D, 3D, etc.)\n\n\n\n\nWe consider a parametric curve in 2D space:\n\\[\n\\underline{c}(t) = \\frac{1}{2} \\begin{pmatrix}\n\\cos(2\\pi t) + 1 \\\\\n\\sin(2\\pi t) + 1\n\\end{pmatrix}, \\quad t \\in [0,1]\n\\]\nProperties:\n\nThis describes a circle with radius ¬Ω centered at (¬Ω, ¬Ω)\nThe parameter \\(t\\) completes one full revolution as it goes from 0 to 1\n\nFirst, we compute the derivative (velocity vector) by time:\n\\[\n\\underline{c}^{\\prime}(t) = \\begin{pmatrix}\n-\\pi \\sin(2\\pi t) \\\\\n\\pi \\cos(2\\pi t)\n\\end{pmatrix}\n\\]\nThe speed (magnitude of velocity) is interestingly constant for every t, so we just need to calculate once:\n\\[\n\\begin{aligned}\n\\| \\underline{c}^{\\prime}(t) \\| &= \\sqrt{(-\\pi \\sin(2\\pi t))^2 + (\\pi \\cos(2\\pi t))^2} \\\\\n&= \\sqrt{\\pi^2 (\\sin^2 (2\\pi t) + \\cos^2 (2\\pi t))} \\\\\n&= \\sqrt{\\pi^2} = \\pi\n\\end{aligned}\n\\]\nTotal arc-length of the curve:\n\\[\ns = \\int_{0}^{1} \\| \\underline{c}^{\\prime}(t) \\| \\, dt =  \\int_{0}^{1} \\pi \\, dt = \\pi\n\\]\nGoal: now make sure the velocity by length should also be constant (also claled arclength parameterization)\n\nCumulative length function: \\[ s(t) = \\int_{0}^{t} \\pi \\, dt = \\pi t \\]\nInverse relationship: This tells us the time t needed to travel a distance s along the curve. \\[ t(s) = \\frac{s}{\\pi} \\]\nSubstitute \\(t(s)\\) to original \\(c(t)\\) tells the curve‚Äôs position after travel a distance s: \\[\n\\underline{c}(t) = \\underline{c}(t(s)) = \\underline{c}(s) = \\frac{1}{2} \\begin{pmatrix}\n\\cos(2s) + 1 \\\\\n\\sin(2s) + 1\n\\end{pmatrix}, \\quad s \\in [0,\\pi]\n\\]\n\nWhy this matters: This ensures \\(\\| \\underline{c}^{\\prime}(t) \\| = 1\\) everywhere, means the curve travel at constant speed.\n\n\n\n\nBasically just a stretched velocity, implemented by mapping A curve \\(\\underline{c}(t)\\) in parameter space (e.g., texture coordinates) is mapped to a 3D surface (liek a deformable fabric) \\(\\underline{x}(\\underline{\\tau})\\): \\[\n\\underline{x}(\\underline{\\tau}) = \\underline{x}(\\underline{c}(t))\n\\] The physical length on deformable fabric becomes, now we also account this deformable stretching surface: \\[\ns = \\int_{0}^{1} \\left\\| \\frac{d}{dt} \\underline{x}(\\underline{c}(t)) \\right\\| dt\n\\] The derivative decomposes into surface and curve components (Chain Rule Application (remember above?)): \\[\n\\frac{d}{dt} \\underline{x}(\\underline{c}(t)) = \\underbrace{\\nabla_{\\underline{\\tau}} \\underline{x}}_{\\text{Surface Jacobian}} \\cdot \\underbrace{\\frac{d\\underline{c}}{dt}}_{\\text{Curve velocity}}\n\\]\nComponent dimensions:\n\n\\(\\nabla_{\\underline{\\tau}} \\underline{x}\\): \\(3 \\times 2\\) matrix (tangent vectors)\n\\(\\frac{d\\underline{c}}{dt}\\): \\(2 \\times 1\\) vector\n\nFinally the general form:\n\\[\ns = \\int_{0}^{1} \\left\\| \\nabla_{\\underline{\\tau}} \\underline{x}(\\underline{c}(t)) \\cdot \\underline{c}'(t) \\right\\| dt\n\\]\n\nThe integrand \\(\\left\\| J \\cdot \\underline{c}'(t) \\right\\|\\) is the stretched speed:\n\n\\(J\\) encodes how the surface stretches/compresses space\n\\(\\underline{c}'(t)\\) is the original curve‚Äôs velocity\n\n\n\n\n\n\n\nCenter at \\((0.5, 0.5)\\) with radius 0.5 in parameter space: \\[\n\\underline{c}(t) = \\frac{1}{2} \\begin{pmatrix}\n\\cos(2\\pi t) + 1 \\\\\n\\sin(2\\pi t) + 1\n\\end{pmatrix}, \\quad t \\in [0,1]\n\\]\nDerivative (velocity): \\[\n\\underline{c}'(t) = \\begin{pmatrix}\n-\\pi \\sin(2\\pi t) \\\\\n\\pi \\cos(2\\pi t)\n\\end{pmatrix}\n\\]\n\n\n\nA surface that is stretched as a parabol: \\[\n\\underline{x}(\\underline{\\tau}) = \\underline{x}(u,v) = \\begin{pmatrix}\nu \\\\\nv \\\\\nu^2 + v^2\n\\end{pmatrix}\n\\]\nSurface Jacobian (tangential vectors): \\[\n\\nabla_{\\underline{\\tau}} \\underline{x} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n2u & 2v\n\\end{pmatrix}\n\\]\nInterpretation:\n\nThe last row shows how the surface ‚Äúbends‚Äù in the z-direction\nFirst two rows maintain the identity mapping in x and y\n\n\n\n\nEvaluate the Jacobian at \\(\\underline{c}(t)\\): \\[\n\\nabla_{\\underline{\\tau}} \\underline{x}(\\underline{c}(t)) = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\cos(2\\pi t) + 1 & \\sin(2\\pi t) + 1\n\\end{pmatrix}\n\\] Key observation: The z-component now varies with the curve‚Äôs position\n\n\n\nMultiply Jacobian by curve velocity: \\[\n\\nabla_{\\underline{\\tau}} \\underline{x} \\cdot \\underline{c}'(t) = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\cos(2\\pi t) + 1 & \\sin(2\\pi t) + 1\n\\end{pmatrix} \\begin{pmatrix}\n-\\pi \\sin(2\\pi t) \\\\\n\\pi \\cos(2\\pi t)\n\\end{pmatrix}\n\\] \\[\n\\nabla_{\\underline{\\tau}} \\underline{x} \\cdot \\underline{c}'(t) = \\begin{pmatrix}\n-\\pi \\sin(2\\pi t) \\\\\n\\pi \\cos(2\\pi t) \\\\\n\\pi (\\cos(2\\pi t) - \\sin(2\\pi t))\n\\end{pmatrix}\n\\]\nCompute the magnitude: \\[\n\\begin{aligned}\n\\left\\| \\nabla_{\\underline{\\tau}} \\underline{x} \\cdot \\underline{c}'(t) \\right\\| &= \\pi \\sqrt{\\sin^2(2\\pi t) + \\cos^2(2\\pi t) + (\\cos(2\\pi t) - \\sin(2\\pi t))^2} \\\\\n&= \\pi \\sqrt{1 + 1 - 2\\sin(2\\pi t)\\cos(2\\pi t)} \\\\\n&= \\pi \\sqrt{2 - \\sin(4\\pi t)}\n\\end{aligned}\n\\]\nFinal total length becomes: \\[\ns = \\pi \\int_{0}^{1} \\sqrt{2 - \\sin(4\\pi t)} \\, dt \\approx 1.39\\pi\n\\]\nNumerical insight:\n\nThe integral evaluates to ‚âà1.39 (requires numerical methods)\nCompare to flat case (where \\(s=\\pi\\))\nThe surface curvature adds ~39% more length"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#first-fundamental-form-measuring-lengths-and-angles-on-surfaces",
    "href": "posts/cg1-meshprocessing/index.html#first-fundamental-form-measuring-lengths-and-angles-on-surfaces",
    "title": "Surface Analysis",
    "section": "",
    "text": "Consider a surface \\(S\\) parameterized by: \\[\n\\underline{x}(u,v) : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\n\\] where \\(\\underline{\\tau} = (u,v)\\) are the 2D parameter coordinates.\n\n\n\nThe partial derivatives form a basis for the tangent plane: \\[\n\\underline{x}_u = \\frac{\\partial \\underline{x}}{\\partial u}, \\quad\n\\underline{x}_v = \\frac{\\partial \\underline{x}}{\\partial v}\n\\]\nPhysical meaning: These vectors span the plane ‚Äútangent‚Äù to the surface at point \\(\\underline{\\tau}\\).\n\n\n\nFor an infinitesimal displacement \\(d\\underline{\\tau} = (du, dv)^T\\) in parameter space, the corresponding 3D displacement on the surface is: \\[\nd\\underline{x} = \\underline{x}_u du + \\underline{x}_v dv = \\begin{pmatrix} \\underline{x}_u & \\underline{x}_v \\end{pmatrix} \\begin{pmatrix} du \\\\ dv \\end{pmatrix}\n\\]\nThe squared length in 3D is: \\[\n\\|d\\underline{x}\\|^2 = \\langle d\\underline{x}, d\\underline{x} \\rangle = \\langle \\underline{x}_u du + \\underline{x}_v dv, \\underline{x}_u du + \\underline{x}_v dv \\rangle\n\\] \\[\n\\|d\\underline{x}\\|^2 = \\underbrace{\\langle \\underline{x}_u, \\underline{x}_u \\rangle}_{E} du^2 + 2 \\underbrace{\\langle \\underline{x}_u, \\underline{x}_v \\rangle}_{F} dudv + \\underbrace{\\langle \\underline{x}_v, \\underline{x}_v \\rangle}_{G} dv^2\n\\]\nWe can eventually write this quadratic form as a matrix: \\[\n\\|d\\underline{x}\\|^2 = \\begin{pmatrix} du & dv \\end{pmatrix}\n\\underbrace{\\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix}}_{I_{\\underline{\\tau}}^S}\n\\begin{pmatrix} du \\\\ dv \\end{pmatrix}\n\\]\nThus, the First Fundamental Form emerges naturally as the matrix: \\[\nI_{\\underline{\\tau}}^S = \\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix} = \\begin{pmatrix} \\underline{x}_u^T \\underline{x}_u & \\underline{x}_u^T \\underline{x}_v \\\\ \\underline{x}_u^T \\underline{x}_v & \\underline{x}_v^T \\underline{x}_v \\end{pmatrix}\n\\]\nComponents:\n\n\\(E = \\|\\underline{s}_u\\|^2\\): Squared length of u-tangent vector\n\\(G = \\|\\underline{s}_v\\|^2\\): Squared length of v-tangent vector\n\n\\(F = \\langle \\underline{s}_u, \\underline{s}_v \\rangle\\): Cosine similarity between tangent vectors\n\nThe FFF is the basically just quadratic the Jacobian: \\[\nI_{\\underline{\\tau}}^S = J^T J \\quad \\text{where} \\quad J = [\\underline{x}_u \\ \\underline{x}_v] \\ \\text{(Jacobian)}\n\\]\n\n\n\n\n\n\nFor any vectors in parameter space \\(\\mathbb{R}^3\\) where we draw the curve originally on, the length of this vector on parameter surface is: \\[ \\|\\vec{v}\\|^2 = \\langle \\vec{v}, \\vec{v} \\rangle \\]\n\n\n\n\\[\n\\|\\vec{v}\\|_{I_{\\underline{\\tau}}^S} = \\sqrt{\\langle \\vec{v}, \\vec{v} \\rangle_{I_{\\underline{\\tau}}^S}} = \\sqrt{\\vec{v}^T I_{\\underline{\\tau}}^S \\vec{v}}\n\\]\n\n\n\nWell, simply add all small displacement that together, remember that each point \\(\\underline{\\tau}\\) or \\((u,v)\\) from parameter space will have a totally different stretch \\(I_{\\underline{\\tau}}^S\\) on the surface, so the stretch at each point is different. \\[s = \\int_0^1 \\|\\underline{c}'(t)\\|_{I_{\\underline{\\tau}}^S} dt = \\int_0^1 \\sqrt{ \\langle \\underline{c}'(t), \\underline{c}'(t) \\rangle_{I_{\\underline{\\tau}}^S}} dt = \\int_0^1 \\sqrt{ \\underline{c}'(t) {I_{\\underline{\\tau}}^S} \\underline{c}'(t)} dt\\]\n\n\n\nWe can also decompose in eigenvalues:\n\\[\nI_{\\underline{\\tau}}^S = \\begin{pmatrix}\nE & F \\\\\nF & G\n\\end{pmatrix} = \\lambda_1 \\hat{v}_1 \\hat{v}_1^T + \\lambda_2 \\hat{v}_2 \\hat{v}_2^T\n\\]\n\n\n\n\n\\(\\lambda_i\\): Principal stretching factors\n\\(\\hat{v}_i\\): Principal directions in parameter space\nOrthonormal basis: The eigenvectors form a coordinate system for the surface at point \\(\\underline{\\tau}\\) or \\((u,v)\\)\n\nFor each vector \\(W = (w_1, w_2)^T\\) in the eigenbasis at point \\(\\underline{\\tau}\\) aka \\((u,v)\\), it will be stretched like this:\n\\[\n\\|W\\|_{I_{\\underline{\\tau}}^S} = \\sqrt{\\lambda_1 w_1^2 + \\lambda_2 w_2^2}\n\\]\n\n\n\n\nFor any vectors in parameter space \\(\\mathbb{R}^3\\) where we draw the curve originally on: \\[ \\cos \\theta(\\vec{v}, \\vec{w}) = \\frac{\\langle \\vec{v}, \\vec{w} \\rangle}{\\|\\vec{v}\\|\\|\\vec{w}\\|} \\]\nNow we MAP ON THE SURFACE with surface parameterization \\(\\underline{s}(\\vec{t})\\), the upper part is: \\[ \\langle \\underline{s}_{\\vec{t}} \\vec{v}, \\underline{s}_{\\vec{t}} \\vec{w} \\rangle = \\vec{v}^T \\underbrace{(\\underline{s}_{\\vec{t}}^T \\underline{s}_{\\vec{t}})}_{\\text{FFF Matrix } \\Gamma} \\vec{w} \\]\nWe can also write it like this: \\[\n\\langle \\vec{v}, \\vec{w} \\rangle_{I_{\\underline{\\tau}}^S} = \\vec{v}^T I_{\\underline{\\tau}}^S \\vec{w}\n\\]\nAnd the bottom part is yeah actually we just learned from above: \\[\n\\|\\vec{v}\\|_{I_{\\underline{\\tau}}^S} = \\sqrt{\\langle \\vec{v}, \\vec{v} \\rangle_{I_{\\underline{\\tau}}^S}} = \\sqrt{\\vec{v}^T I_{\\underline{\\tau}}^S \\vec{v}}\n\\]\nTo assemble we will have: \\[\n\\cos \\theta = \\frac{\\langle \\vec{v}, \\vec{w} \\rangle_{I_{\\underline{\\tau}}^S}}{\\|\\vec{v}\\|_{I_{\\underline{\\tau}}^S} \\|\\vec{w}\\|_{I_{\\underline{\\tau}}^S}} = \\frac{\\vec{v}^T I_{\\underline{\\tau}}^S \\vec{w}}{\\sqrt{\\vec{v}^T I_{\\underline{\\tau}}^S \\vec{v}} \\sqrt{\\vec{w}^T I_{\\underline{\\tau}}^S \\vec{w}}}\n\\] Angle \\(\\theta\\) between two directions \\(d\\vec{t}_1\\), \\(d\\vec{t}_2\\): \\[ \\cos \\theta = \\frac{Edu_1du_2 + F(du_1dv_2 + du_2dv_1) + Gdv_1dv_2}{\\sqrt{Edu_1^2 + 2Fdu_1dv_1 + Gdv_1^2}\\sqrt{Edu_2^2 + 2Fdu_2dv_2 + Gdv_2^2}} \\]\n\n\n\nCross Product area of an infinitesimal parallelogram spanned by \\(\\underline{s}_u du\\) and \\(\\underline{s}_v dv\\) is: \\[\ndA = \\|(\\underline{s}_u du) \\times (\\underline{s}_v dv)\\| = \\|\\underline{s}_u \\times \\underline{s}_v\\| du dv\n\\]\nWe can express this purely in terms of the FFF coefficients:\n\nStart with the cross product magnitude identity: \\[\n\\|\\underline{s}_u \\times \\underline{s}_v\\|^2 = \\|\\underline{s}_u\\|^2 \\|\\underline{s}_v\\|^2 - \\langle \\underline{s}_u, \\underline{s}_v \\rangle^2\n\\]\nRecognize this matches the determinant of \\(I_{\\underline{\\tau}}^S\\): \\[\n\\det(I_{\\underline{\\tau}}^S) = EG - F^2\n\\]\nThus: \\[\ndA = \\sqrt{EG - F^2} du dv\n\\]\n\n\n\n\\[\n\\|\\vec{a} \\times \\vec{b}\\|^2 + |\\vec{a} \\cdot \\vec{b}|^2 = \\|\\vec{a}\\|^2 \\|\\vec{b}\\|^2\n\\]\n\n\n\nCross Product Definition\nMeasures ‚Äúperpendicularity‚Äù (the area of the parallelogram formed) \\[\n\\|\\vec{a} \\times \\vec{b}\\| = \\|\\vec{a}\\|\\|\\vec{b}\\|\\sin\\theta\n\\] \\[\n\\|\\vec{a} \\times \\vec{b}\\|^2 = \\|\\vec{a}\\|^2\\|\\vec{b}\\|^2\\sin^2\\theta\n\\]\nDot Product Definition Measures ‚Äúperpendicularity‚Äù (the area of the parallelogram formed) Measures ‚Äúparallelism‚Äù (how aligned the vectors are) \\[\n\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\|\\|\\vec{b}\\|\\cos\\theta\n\\] \\[\n|\\vec{a} \\cdot \\vec{b}|^2 = \\|\\vec{a}\\|^2\\|\\vec{b}\\|^2\\cos^2\\theta\n\\]\nCombining Both Terms\nRepresents the total magnitude regardless of angle \\[\n\\|\\vec{a} \\times \\vec{b}\\|^2 + |\\vec{a} \\cdot \\vec{b}|^2 = \\|\\vec{a}\\|^2\\|\\vec{b}\\|^2(\\sin^2\\theta + \\cos^2\\theta) = \\|\\vec{a}\\|^2\\|\\vec{b}\\|^2\n\\]\n\n\n\n\n\n\n\n\nWe start with the spherical coordinates parameterization: \\[\n\\underline{S}(u,v) = r \\begin{pmatrix}\n\\cos u \\cos v \\\\\n\\sin u \\cos v \\\\\n\\sin v\n\\end{pmatrix}, \\quad\n\\begin{cases}\nu \\in [-\\pi, \\pi] \\ (\\text{longitude}) \\\\\nv \\in [-\\frac{\\pi}{2}, \\frac{\\pi}{2}] \\ (\\text{latitude})\n\\end{cases}\n\\]\n\n\n\nPartial derivatives form the tangent basis: \\[\n\\underline{S}_u = r \\begin{pmatrix}\n-\\sin u \\cos v \\\\\n\\cos u \\cos v \\\\\n0\n\\end{pmatrix}, \\quad\n\\underline{S}_v = r \\begin{pmatrix}\n-\\cos u \\sin v \\\\\n-\\sin u \\sin v \\\\\n\\cos v\n\\end{pmatrix}\n\\]\n\n\n\nCalculate the metric tensor coefficients: \\[\n\\begin{aligned}\nE &= \\|\\underline{S}_u\\|^2 = r^2 \\cos^2 v (\\sin^2 u + \\cos^2 u) = r^2 \\cos^2 v \\\\\nF &= \\langle \\underline{S}_u, \\underline{S}_v \\rangle = 0 \\quad \\text{(orthogonal coordinates)} \\\\\nG &= \\|\\underline{S}_v\\|^2 = r^2 (\\cos^2 u \\sin^2 v + \\sin^2 u \\sin^2 v + \\cos^2 v) = r^2\n\\end{aligned}\n\\]\nThus, the metric tensor is: \\[\nI = \\begin{pmatrix}\nr^2 \\cos^2 v & 0 \\\\\n0 & r^2\n\\end{pmatrix}\n\\]\n\n\n\nThe area element derives from the metric determinant: \\[\n\\begin{aligned}\ndA &= \\sqrt{\\det I} \\, du dv = \\sqrt{r^4 \\cos^2 v} \\, du dv \\\\\n&= r^2 |\\cos v| \\, du dv\n\\end{aligned}\n\\]\n\n\n\nIntegrate over the parameter domain: \\[\n\\begin{aligned}\nA &= \\int_{-\\pi}^{\\pi} \\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}} r^2 |\\cos v| \\, dv du \\\\\n&= r^2 \\int_{-\\pi}^{\\pi} du \\cdot 2 \\int_{0}^{\\frac{\\pi}{2}} \\cos v \\, dv \\quad \\text{(by symmetry)} \\\\\n&= r^2 (2\\pi) \\cdot 2 \\left[ \\sin v \\right]_0^{\\frac{\\pi}{2}} \\\\\n&= 4\\pi r^2\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#optimal-curve-design-in-2d",
    "href": "posts/cg1-meshprocessing/index.html#optimal-curve-design-in-2d",
    "title": "Surface Analysis",
    "section": "",
    "text": "The method ([Gum ‚Äô04]) designs smooth curves through control points by optimizing a combined length-curvature functional:\n\\[\nF_{simp}(c, \\lambda) = \\int_{s=0}^{L(c)} \\underbrace{(1 - \\lambda)q}_{\\text{length term}} + \\underbrace{\\lambda |\\kappa L_0|^2}_{\\text{curvature term}} ds\n\\]\nwhere:\n\n\\(c\\): Curve parameterization\n\\(\\lambda \\in [0,1]\\): Blending parameter\n\\(L_0\\): Length of control polygon\n\\(q = 5000\\): Normalization constant\n\\(\\kappa\\): Curvature at each point\n\n\n\n\nOptimization Objective: \\[\n\\text{C}_{simp}(\\lambda) = \\min_{\\mathbf{c}: \\mathbf{c}(x_i) = \\mathbf{p}_i} F_{simp}(\\mathbf{c}, \\lambda)\n\\]\nBehavioral Spectrum:\n\n\\(\\lambda = 0\\): Pure length minimization ‚Üí yields control polygon (blue curve)\n\\(\\lambda = 1\\): Pure curvature minimization ‚Üí yields smoothest curve (red curve)\nIntermediate \\(\\lambda\\): Balanced trade-off"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#minimal-energy-interpolating-curves",
    "href": "posts/cg1-meshprocessing/index.html#minimal-energy-interpolating-curves",
    "title": "Surface Analysis",
    "section": "",
    "text": "Find a curve \\(\\mathbf{c}(s)\\) that:\n\nExactly interpolates given control points \\(\\{\\mathbf{p}_i\\}_{i=0}^n\\)\nMinimizes the hybrid energy functional: \\[\nF_{simp}(\\mathbf{c}, \\lambda) = \\int \\underbrace{(1-\\lambda)q}_{\\text{length term}} + \\underbrace{\\lambda|\\kappa(s)L_0|^2}_{\\text{curvature term}} ds\n\\]\n\n\n\n\n\n\nUse piecewise cubic Hermite splines between each \\(\\mathbf{p}_i\\) and \\(\\mathbf{p}_{i+1}\\): \\[\n\\mathbf{c}_i(t) = \\mathbf{a}_i t^3 + \\mathbf{b}_i t^2 + \\mathbf{c}_i t + \\mathbf{d}_i, \\quad t \\in [0,1]\n\\]\n\n\n\n\nPosition: \\[\\mathbf{c}_i(0) = \\mathbf{p}_i, \\quad \\mathbf{c}_i(1) = \\mathbf{p}_{i+1}\\]\nTangent continuity (with \\(\\mathbf{m}_i\\) as unknown tangent vectors): \\[\\mathbf{c}_i'(0) = \\mathbf{m}_i, \\quad \\mathbf{c}_i'(1) = \\mathbf{m}_{i+1}\\]\n\n\n\n\nExpressed in terms of curve derivatives: \\[\nF_{simp} = \\sum_{i=0}^{n-1} \\int_0^1 \\left[\n(1-\\lambda)q \\|\\mathbf{c}_i'(t)\\| +\n\\lambda L_0^2 \\left(\\frac{\\|\\mathbf{c}_i'(t) \\times \\mathbf{c}_i''(t)\\|}{\\|\\mathbf{c}_i'(t)\\|^3}\\right)^2\n\\right] dt\n\\]\n\n\n\n\n\n\nExpress cubic coefficients using control points and tangents: \\[\n\\begin{aligned}\n\\mathbf{a}_i &= 2(\\mathbf{p}_i - \\mathbf{p}_{i+1}) + \\mathbf{m}_i + \\mathbf{m}_{i+1} \\\\\n\\mathbf{b}_i &= -3(\\mathbf{p}_i - \\mathbf{p}_{i+1}) - 2\\mathbf{m}_i - \\mathbf{m}_{i+1} \\\\\n\\mathbf{c}_i &= \\mathbf{m}_i \\\\\n\\mathbf{d}_i &= \\mathbf{p}_i\n\\end{aligned}\n\\]\n\n\n\nFor energy calculation: \\[\n\\begin{aligned}\n\\mathbf{c}_i'(t) &= 3\\mathbf{a}_i t^2 + 2\\mathbf{b}_i t + \\mathbf{c}_i \\\\\n\\mathbf{c}_i''(t) &= 6\\mathbf{a}_i t + 2\\mathbf{b}_i\n\\end{aligned}\n\\]\n\n\n\n\nInitial guess: Set \\(\\mathbf{m}_i = \\frac{\\mathbf{p}_{i+1} - \\mathbf{p}_{i-1}}{2}\\) (central differences)\nGradient descent: \\[\n\\mathbf{m}_i^{(k+1)} = \\mathbf{m}_i^{(k)} - \\gamma \\frac{\\partial F_{simp}}{\\partial \\mathbf{m}_i}\n\\] where the gradient involves: \\[\n\\frac{\\partial F_{simp}}{\\partial \\mathbf{m}_i} = \\int_0^1 \\left[\n(1-\\lambda)q \\frac{\\partial \\|\\mathbf{c}_i'\\|}{\\partial \\mathbf{m}_i} +\n2\\lambda L_0^2 \\kappa \\frac{\\partial \\kappa}{\\partial \\mathbf{m}_i}\n\\right] dt\n\\]\n\n\n\n\n\nGiven \\(\\mathbf{p}_0, \\mathbf{p}_1, \\mathbf{p}_2\\) with \\(\\mathbf{p}_1\\) at \\(t^*=0.5\\):\n\nSingle cubic segment with constraint \\(\\mathbf{c}(0.5) = \\mathbf{p}_1\\)\nSolve for \\(\\mathbf{m}_0, \\mathbf{m}_1, \\mathbf{m}_2\\) minimizing \\(F_{simp}\\)\nFinal curve guarantees:\n\n\\(\\mathbf{c}(0) = \\mathbf{p}_0\\), \\(\\mathbf{c}(0.5) = \\mathbf{p}_1\\), \\(\\mathbf{c}(1) = \\mathbf{p}_2\\)\nMinimal energy for chosen \\(\\lambda\\)"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#curvature-formula-deep-dive-why-kappa-fracvecc_t-times-vecc_ttvecc_t3",
    "href": "posts/cg1-meshprocessing/index.html#curvature-formula-deep-dive-why-kappa-fracvecc_t-times-vecc_ttvecc_t3",
    "title": "Surface Analysis",
    "section": "",
    "text": "Curvature measures how fast the tangent vector turns per unit distance traveled. We need to:\n\nTrack how much the tangent direction changes (numerator)\nNormalize by how fast we‚Äôre moving (denominator)\n\n\n\n\n\n\nDefine the unit tangent vector: \\[\n\\vec{T}(t) = \\frac{\\vec{c}_t(t)}{\\|\\vec{c}_t(t)\\|}\n\\] (Note: \\(\\vec{c}_t = \\frac{d\\vec{c}}{dt}\\) is the velocity vector)\n\n\n\nThe curvature comes from \\(\\frac{d\\vec{T}}{ds}\\) (how \\(\\vec{T}\\) changes with arc length \\(s\\)).\nFirst, apply chain rule: \\[\n\\frac{d\\vec{T}}{dt} = \\frac{d}{dt}\\left(\\frac{\\vec{c}_t}{\\|\\vec{c}_t\\|}\\right) = \\frac{\\vec{c}_{tt}\\|\\vec{c}_t\\| - \\vec{c}_t(\\vec{c}_t \\cdot \\vec{c}_{tt})/\\|\\vec{c}_t\\|}{\\|\\vec{c}_t\\|^2}\n\\]\n\n\n\nCurvature only cares about perpendicular changes to \\(\\vec{T}\\) (lateral bending). The tangential component affects speed, not shape.\nThe perpendicular part is: \\[\n\\left(\\frac{d\\vec{T}}{dt}\\right)_{\\perp} = \\frac{\\vec{c}_{tt}}{\\|\\vec{c}_t\\|} - \\frac{\\vec{c}_t (\\vec{c}_t \\cdot \\vec{c}_{tt})}{\\|\\vec{c}_t\\|^3}\n\\]\n\n\n\nCompute \\(\\|\\frac{d\\vec{T}}{dt}\\|\\) using vector identity \\(\\|\\vec{a}\\| = \\sqrt{\\vec{a} \\cdot \\vec{a}}\\): \\[\n\\left\\|\\left(\\frac{d\\vec{T}}{dt}\\right)_{\\perp}\\right\\| = \\frac{\\|\\vec{c}_t \\times \\vec{c}_{tt}\\|}{\\|\\vec{c}_t\\|^2}\n\\]\n\n\n\nSince curvature is \\(\\kappa = \\|\\frac{d\\vec{T}}{ds}\\|\\) and \\(ds = \\|\\vec{c}_t\\|dt\\): \\[\n\\kappa = \\left\\|\\frac{d\\vec{T}}{ds}\\right\\| = \\frac{\\|\\frac{d\\vec{T}}{dt}\\|}{ds/dt} = \\frac{\\|\\vec{c}_t \\times \\vec{c}_{tt}\\|/\\|\\vec{c}_t\\|^2}{\\|\\vec{c}_t\\|} = \\frac{\\|\\vec{c}_t \\times \\vec{c}_{tt}\\|}{\\|\\vec{c}_t\\|^3}\n\\]\n\n\n\n\nThe denominator accounts for: 1. One \\(\\|\\vec{c}_t\\|\\): Converts time derivative to arc-length derivative (\\(ds = \\|\\vec{c}_t\\|dt\\)) 2. Two more \\(\\|\\vec{c}_t\\|\\) terms:\n\nOne from normalizing the tangent vector (\\(\\vec{T} = \\vec{c}_t/\\|\\vec{c}_t\\|\\))\nOne from isolating the perpendicular component\n\n\n\n\n\nNumerator \\(\\|\\vec{c}_t \\times \\vec{c}_{tt}\\|\\): Measures the area of the parallelogram formed by velocity and acceleration ‚Üí pure bending effect\nDenominator \\(\\|\\vec{c}_t\\|^3\\): Normalizes for:\n\nParameterization speed (\\(\\|\\vec{c}_t\\|\\))\nUnit tangent construction (two \\(\\|\\vec{c}_t\\|\\) terms)\n\n\n\n\n\nFor \\(\\vec{c}(t) = (r\\cos t, r\\sin t)\\):\n\n\\(\\vec{c}_t = (-r\\sin t, r\\cos t)\\) ‚Üí \\(\\|\\vec{c}_t\\| = r\\)\n\\(\\vec{c}_{tt} = (-r\\cos t, -r\\sin t)\\)\n\\(\\vec{c}_t \\times \\vec{c}_{tt} = r^2\\)\n\\(\\kappa = \\frac{r^2}{r^3} = \\frac{1}{r}\\) (correct for circle radius \\(r\\))"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#surface-curvatures-explained",
    "href": "posts/cg1-meshprocessing/index.html#surface-curvatures-explained",
    "title": "Surface Analysis",
    "section": "",
    "text": "Surface Parameterization:\n\n\\(\\underline{S}(u,v)\\): A 2D ‚Üí 3D mapping defining the surface\n\\(\\underline{S}_u = \\frac{\\partial \\underline{S}}{\\partial u}\\): Partial derivative in u-direction (tangent vector)\n\\(\\underline{S}_v = \\frac{\\partial \\underline{S}}{\\partial v}\\): Partial derivative in v-direction (tangent vector)\n\nFirst Fundamental Form (Metric Tensor): \\[I = \\begin{pmatrix} E & F \\\\ F & G \\end{pmatrix} = \\begin{pmatrix} \\langle \\underline{S}_u, \\underline{S}_u \\rangle & \\langle \\underline{S}_u, \\underline{S}_v \\rangle \\\\ \\langle \\underline{S}_u, \\underline{S}_v \\rangle & \\langle \\underline{S}_v, \\underline{S}_v \\rangle \\end{pmatrix}\\]\nUnit Normal Vector: \\[\\hat{n} = \\frac{\\underline{S}_u \\times \\underline{S}_v}{\\|\\underline{S}_u \\times \\underline{S}_v\\|}\\]\n\n\n\n\nFor a given point \\(\\underline{p}\\) on the surface:\n\nDefinition: Curvature of the intersection curve created by slicing the surface with a plane containing:\n\nThe surface normal \\(\\hat{n}\\)\nA tangent vector \\(\\underline{v} = \\cosŒ±\\,\\underline{S}_u + \\sinŒ±\\,\\underline{S}_v\\)\n\nAngle Parameter:\n\n\\(Œ±\\): Angle between \\(\\underline{S}_u\\) and the chosen tangent direction \\(\\underline{v}\\)\n\\(Œ≤ = Œ± - Œ±_0\\): Angle relative to principal direction\n\n\n\n\n\nThere exist two orthogonal principal directions where normal curvature reaches extremal values: \\[Œ∫_n(Œ≤) = Œ∫_{\\min}\\cos^2Œ≤ + Œ∫_{\\max}\\sin^2Œ≤\\]\n\n\\(Œ∫_{\\min}\\): Minimum normal curvature\n\\(Œ∫_{\\max}\\): Maximum normal curvature\n\\(Œ±_0\\): Angle of first principal direction from \\(\\underline{S}_u\\)\n\n\n\n\nMeasures how the surface bends in 3D space: \\[II = \\begin{pmatrix} L & M \\\\ M & N \\end{pmatrix}\\] where:\n\n\\(L = \\langle \\underline{S}_{uu}, \\hat{n} \\rangle\\)\n\\(M = \\langle \\underline{S}_{uv}, \\hat{n} \\rangle\\)\n\\(N = \\langle \\underline{S}_{vv}, \\hat{n} \\rangle\\)\n\n\n\n\n\nGaussian Curvature (intrinsic): \\[K = Œ∫_{\\min}Œ∫_{\\max} = \\frac{LN - M^2}{EG - F^2}\\]\nMean Curvature (extrinsic): \\[H = \\frac{Œ∫_{\\min} + Œ∫_{\\max}}{2} = \\frac{EN - 2FM + GL}{2(EG - F^2)}\\]\nPrincipal Curvatures: \\[Œ∫_{\\max,\\min} = H \\pm \\sqrt{H^2 - K}\\]\n\n\n\n\n\nPositive Gaussian Curvature (\\(K &gt; 0\\)): Bowl-like (both curvatures same sign)\nNegative Gaussian Curvature (\\(K &lt; 0\\)): Saddle-like (opposite signs)\nZero Gaussian Curvature (\\(K = 0\\)): Developable surface (at least one zero curvature)\n\n\n\n\nTo find principal directions: 1. Solve eigenvalue problem for the shape operator: \\[S = I^{-1}II\\] 2. Eigenvalues ‚Üí principal curvatures 3. Eigenvectors ‚Üí principal directions\n\n\n\n\n\\(Œ∫_{\\min} = 0\\) (along axis)\n\\(Œ∫_{\\max} = 1/r\\) (around circumference)\n\\(K = 0\\) (developable)\n\\(H = 1/(2r)\\)"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#derivation-of-the-second-fundamental-form",
    "href": "posts/cg1-meshprocessing/index.html#derivation-of-the-second-fundamental-form",
    "title": "Surface Analysis",
    "section": "",
    "text": "While the First Fundamental Form (I) measures distances on the surface, the Second Fundamental Form (II) quantifies how the surface bends in 3D space by tracking normal vector variations.\n\n\n\n\n\nConsider a surface patch \\(\\underline{S}(u,v)\\) with unit normal \\(\\hat{n}(u,v)\\). The rate of normal change reveals curvature:\n\\[\nd\\hat{n} = \\hat{n}_u du + \\hat{n}_v dv\n\\]\n\n\n\nSince \\(\\hat{n} \\perp\\) tangent plane, we have: \\[\n\\langle \\hat{n}, \\underline{S}_u \\rangle = \\langle \\hat{n}, \\underline{S}_v \\rangle = 0\n\\]\nDifferentiating these gives the Weingarten equations: \\[\n\\begin{aligned}\n\\langle \\hat{n}_u, \\underline{S}_u \\rangle &= -\\langle \\hat{n}, \\underline{S}_{uu} \\rangle = -L \\\\\n\\langle \\hat{n}_u, \\underline{S}_v \\rangle &= -\\langle \\hat{n}, \\underline{S}_{uv} \\rangle = -M \\\\\n\\langle \\hat{n}_v, \\underline{S}_v \\rangle &= -\\langle \\hat{n}, \\underline{S}_{vv} \\rangle = -N\n\\end{aligned}\n\\]\n\n\n\nThe second fundamental form emerges when expressing the normal component of surface acceleration:\nFor a curve \\(\\underline{c}(t) = \\underline{S}(u(t),v(t))\\): \\[\n\\underline{c}''(t) = \\underline{S}_{uu}(u')^2 + 2\\underline{S}_{uv}u'v' + \\underline{S}_{vv}(v')^2 + \\text{tangent terms}\n\\]\nThe normal curvature is: \\[\n\\kappa_n = \\frac{\\langle \\underline{c}'', \\hat{n} \\rangle}{\\langle \\underline{c}', \\underline{c}' \\rangle} = \\frac{L(u')^2 + 2Mu'v' + N(v')^2}{E(u')^2 + 2Fu'v' + G(v')^2}\n\\]\nThus we define: \\[\nII = \\begin{pmatrix} L & M \\\\ M & N \\end{pmatrix} = \\begin{pmatrix}\n\\langle \\underline{S}_{uu}, \\hat{n} \\rangle & \\langle \\underline{S}_{uv}, \\hat{n} \\rangle \\\\\n\\langle \\underline{S}_{uv}, \\hat{n} \\rangle & \\langle \\underline{S}_{vv}, \\hat{n} \\rangle\n\\end{pmatrix}\n\\]\n\n\n\n\n\nHistorical Context:\n\nFirst Fundamental Form (I) measures intrinsic properties (lengths/angles)\nSecond Fundamental Form (II) captures extrinsic bending (how the surface embeds in 3D space)\n\nMathematical Hierarchy:\n\nI comes from the metric tensor (1st derivatives)\nII requires 2nd derivatives and normal information\n\nDuality:\n\nI: \\(\\langle d\\underline{S}, d\\underline{S} \\rangle\\) (dot product of tangent vectors)\nII: \\(\\langle d^2\\underline{S}, \\hat{n} \\rangle\\) (projection of curvature onto normal)\n\n\n\n\n\nThe quadratic form \\(II(v,v)\\) gives the normal curvature in direction \\(v\\):\n\nPositive value: Surface bends toward \\(\\hat{n}\\)\nNegative value: Surface bends away from \\(\\hat{n}\\)\nZero value: Asymptotic direction (no normal bending)\n\n\n\n\nFor \\(\\underline{S}(\\theta,\\phi) = R(\\sin\\theta\\cos\\phi, \\sin\\theta\\sin\\phi, \\cos\\theta)\\):\n\n\\(\\hat{n} = (\\sin\\theta\\cos\\phi, \\sin\\theta\\sin\\phi, \\cos\\theta)\\)\n\\(L = \\langle \\underline{S}_{\\theta\\theta}, \\hat{n} \\rangle = -R\\)\n\\(M = \\langle \\underline{S}_{\\theta\\phi}, \\hat{n} \\rangle = 0\\)\n\\(N = \\langle \\underline{S}_{\\phi\\phi}, \\hat{n} \\rangle = -R\\sin^2\\theta\\)\n\nThus: \\[\nII = \\begin{pmatrix} -R & 0 \\\\ 0 & -R\\sin^2\\theta \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#heat-equation-based-denoising",
    "href": "posts/cg1-meshprocessing/index.html#heat-equation-based-denoising",
    "title": "Surface Analysis",
    "section": "Heat Equation-Based Denoising",
    "text": "Heat Equation-Based Denoising\n\nThe Heat Equation - 1D Laplace\nThe smoothing process is modeled by the 1D heat equation: \\[\n\\frac{\\partial f(x,t)}{\\partial t} = \\alpha \\frac{\\partial^2 f(x,t)}{\\partial x^2}\n\\] where:\n\n\\(f(x,t)\\): Signal value at position \\(x\\) and time \\(t\\)\n\\(\\alpha\\): Diffusion coefficient (controls smoothing rate)\n\\(\\frac{\\partial^2 f}{\\partial x^2}\\): Second spatial derivative (curvature)\n\n\n\nNumerical Implementation\n\nSpatial Discretization\n\nRepresent signal as vector \\(F = [f_1, f_2, ..., f_n]^T\\)\nApproximate second derivative using finite differences: \\[f''_i \\approx \\frac{f_{i+1} - 2f_i + f_{i-1}}{h^2}\\]\nConstruct Laplacian matrix \\(L\\): \\[\nL = \\frac{1}{h^2}\\begin{pmatrix}\n-2 & 1 & & \\\\\n1 & -2 & 1 & \\\\\n& \\ddots & \\ddots & \\ddots \\\\\n& & 1 & -2\n\\end{pmatrix}\n\\]\n\n\n\nTime Integration Methods\n\nExplicit Euler (Forward Euler)\n\\[\nF^{t+\\Delta t} = F^t + \\Delta t \\alpha L F^t = (I + \\Delta t \\alpha L)F^t\n\\]\n\nIdea: Use slope at current time to forward compute next time\nPros: Simple to implement\nCons: Requires small \\(\\Delta t\\) for stability (CFL condition: \\(\\Delta t \\leq \\frac{h^2}{2\\alpha}\\))\n\n\n\nImplicit Euler (Backward Euler)\n\\[\nF^{t+\\Delta t} = F^t + \\Delta t \\alpha L F^{t+\\Delta t}\n\\] \\[(I - \\Delta t \\alpha L)F^{t+\\Delta t} = F^t\\]\n\nIdea: Use slope of future to compute =&gt; have to compute backward from the future back to now\nPros: Unconditionally stable (any \\(\\Delta t\\) allowed)\nCons: Requires solving linear system each step (like LQR-based MPC)\n\n\n\n\n\n3. Boundary Conditions\nCommon choices for signal boundaries:\n\nDirichlet: Fixed values (\\(f=0\\) at boundaries)\nNeumann: Zero derivative (\\(f'=0\\) at boundaries)\nPeriodic: Wrap-around (for cyclic signals)\n\n\n\n2D Heat Equation Denoising Example\nConsider a 2D grayscale image \\(f(x,y,t)\\) where: - \\((x,y)\\) are pixel coordinates - \\(t\\) is diffusion time - \\(\\alpha\\) controls smoothing strength\nThe heat equation becomes: \\[\n\\frac{\\partial f}{\\partial t} = \\alpha \\Delta f = \\alpha \\left(\\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}\\right)\n\\]\n\nDiscrete Laplacian Operator\nFor a pixel grid with spacing \\(h=1\\):\n\\[\n\\Delta f_{i,j} \\approx f_{i-1,j} + f_{i+1,j} + f_{i,j-1} + f_{i,j+1} - 4f_{i,j}\n\\]\nMatrix Representation:\nmath\nL = \\begin{pmatrix} \n0 & 1 & 0 \\\\\n1 & -4 & 1 \\\\\n0 & 1 & 0 \n\\end{pmatrix}"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#what-is-divergence",
    "href": "posts/cg1-meshprocessing/index.html#what-is-divergence",
    "title": "Surface Analysis",
    "section": "What is Divergence?",
    "text": "What is Divergence?\n\n1. Core Definition\nIf Gradient applying \\(\\nabla\\) on a scalar field as input and output a vector field.\nDivergence continues on that Dot product of the \\(\\nabla\\) with a vector field for e.g.¬†F = (F‚ÇÅ, F‚ÇÇ, F‚ÇÉ): \\[\n\\text{div } \\mathbf{F} = \\nabla \\cdot \\mathbf{F} = \\frac{\\partial F_1}{\\partial x} + \\frac{\\partial F_2}{\\partial y} + \\frac{\\partial F_3}{\\partial z}\n\\]\n\n\n2. Physical Interpretation\nImagine fluid flow where F represents velocity vectors:\n\n\n\n\n\n\n\n\nDivergence Value\nWhat‚Äôs Happening\nMeaning\n\n\n\n\nPositive\nMore fluid exits than enters\nMostly gradient positively out\n\n\nNegative\nMore fluid enters than exits\nMostly gradient negatively in\n\n\nZero\nEqual inflow/outflow\npositive gradient = negative gradient\n\n\n\n\n\n3. Gradient vs Divergence Relationship\n\nGradient (‚àáf): Creates a vector field pointing uphill\nExample: Temperature gradient shows heat flow direction\nDivergence of Gradient:\n\\[\n\\nabla \\cdot (\\nabla f) = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2} + \\frac{\\partial^2 f}{\\partial z^2} = \\Delta f\n\\]\nImage Example\n\n\n\n\n\n\n\n\n\nOperation\nEffect\nKernel Example\n\n\n\n\nGradient ‚àáI\nDetect change\n\\([-1, 0, 1]\\)\n\n\nDivergence ‚àá¬∑(‚àáI)\nCompare positive change vs negative changes\n\\(\\begin{bmatrix}0&1&0\\\\1&-4&1\\\\0&1&0\\end{bmatrix}\\)"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#laplace-beltrami-operator-on-surfaces",
    "href": "posts/cg1-meshprocessing/index.html#laplace-beltrami-operator-on-surfaces",
    "title": "Surface Analysis",
    "section": "Laplace-Beltrami Operator on Surfaces",
    "text": "Laplace-Beltrami Operator on Surfaces\n\n1. Fundamental Definition\nFor a function \\(f : S \\rightarrow \\mathbb{R}\\) defined on a surface \\(S\\): \\[\n\\Delta_S f = \\text{div}_S (\\nabla_S f)\n\\] where:\n\n\\(\\nabla_S f\\) is the surface gradient (intrinsic derivative along the surface)\n\\(\\text{div}_S\\) is the surface divergence (measures flux through infinitesimal surface patches)\n\n\n\n2. Discrete Formulation\nFor triangle meshes there are 2 options, the first is without weighting (Uniform Laplace), but this one is Cotant Laplace-Beltrami: \\[\n(\\Delta_S f)_i \\approx \\frac{1}{A_i} \\sum_{j \\in N(i)} (\\cot \\alpha_{ij} + \\cot \\beta_{ij})(f_j - f_i)\n\\]\nwhere:\n\n\\(A_i\\) is the Voronoi area around vertex \\(i\\)\n\\(N(i)\\) are neighboring vertices\n\\(\\alpha_{ij}, \\beta_{ij}\\) are angles opposite to edge \\((i,j)\\)\n\n\n\n3. Key Properties\n\nINTERESTING Curvature Relationship: \\[\n\\Delta_S p = -2H\\hat{n}\n\\] where:\n\n\\(H = \\frac{\\kappa_1 + \\kappa_2}{2}\\) (mean curvature)\n\\(\\hat{n}\\) is the unit normal vector\n\n\n\n\n4. Numerical Implementation\nExplicit smoothing: \\[\nf_i^{t+1} = f_i^t + \\Delta t \\alpha (\\Delta_S f^t)_i\n\\]\n\n\n6. Mean Curvature Flow\n\\[\n\\frac{\\partial p}{\\partial t} = \\Delta_S p = -2H\\hat{n}\n\\]\n\nMOVING TO LAPLACE = MOVING TO MEAN CURVATURE DIRECTION"
  },
  {
    "objectID": "posts/cg1-meshprocessing/index.html#dynamic-connectivity",
    "href": "posts/cg1-meshprocessing/index.html#dynamic-connectivity",
    "title": "Surface Analysis",
    "section": "Dynamic Connectivity",
    "text": "Dynamic Connectivity\n\nRemeshing Pipeline\ngiven a target edge length \\(l\\): 1. split all edges that are longer than \\(\\frac{4}{3} l\\) at their midpoint. 1. collapse all edges shorter than \\(\\frac{4}{5} l\\) into their midpoint 1. flip edges in order get standard valence 6 (or 4 on boundaries). 1. relocate vertices on the surface by tangential smoothing\n\n\nAdd/Remove Face\nMostly taking care of: - boundary list for inverse HE - and the prev/next relationship at each HE"
  },
  {
    "objectID": "posts/pet-randomvariables/index.html",
    "href": "posts/pet-randomvariables/index.html",
    "title": "Random Variables",
    "section": "",
    "text": "World: \\(\\Omega\\) (All possible outcomes \\(\\xi\\)) of an experiment (e.g.¬†\\(\\Omega = \\{HH, HT, TH, TT\\}\\))\nValidation Set: \\(\\mathcal{F}\\) (The list of all valid events, usually the Power Set)\nProbability Measure: \\(P\\) is the rule that assigns a likelihood [0,1] to those events. (e.g.¬†\\(P(\\{HH\\}) = 1/4\\))\nScoreboard: \\(X(\\xi)\\) (The function mapping outcomes to numbers - Random Variable)\nValidation Rule: \\(X^{-1}(B) \\in \\mathcal{F}\\) (Condition for \\(X\\) to be a Random Variable)\n\n\n\n\n\nDespite the name, a Random Variable is not actually a variable in the algebraic sense‚Äîit is a FUNCTIONNNN, or a Scoreboard.\nA random variable \\(X\\) takes an outcome \\(\\xi\\) (‚Äúxi‚Äù) from the sample space \\(\\Omega\\) and maps it to a unique point \\(x\\) in \\(\\mathbb{R}\\).\n\\[X: \\Omega \\to \\mathbb{R}, X(\\xi) = x\\]\nExample: Let \\(X\\) be the ‚ÄúNumber of Heads‚Äù in two coin flips.\n\n\n\nOutcome (\\(\\xi\\))\nCalculation\nValue (\\(x\\))\n\n\n\n\n\\(HH\\)\n\\(X(HH)\\)\n\\(2\\)\n\n\n\\(HT\\)\n\\(X(HT)\\)\n\\(1\\)\n\n\n\\(TH\\)\n\\(X(TH)\\)\n\\(1\\)\n\n\n\\(TT\\)\n\\(X(TT)\\)\n\\(0\\)\n\n\n\n\nImportant: The above randomness values come from the sample space \\(\\Omega\\). \\(X\\) is simply the consistent rule (the function) that maps outcomes to numbers.\n\n\n\n\nHow do we determine the probability of a numerical value like \\(X = 2\\)? We have to look ‚Äúbackward‚Äù from the real numbers to our original sample space. This is called Inverse Mapping.\nGiven a set \\(B\\) (a subset of the real numbers \\(\\mathbb{R}\\)), we must find the event \\(A\\) in our sample space that ‚Äúmaps‚Äù to \\(B\\).\n\\[A = X^{-1}(B) = \\{\\xi \\in \\Omega : X(\\xi) \\in B\\}\\]\nProbability is taken directly from the sample \\(P(A)\\). \\[P(X \\in B) = P(A)\\]\nExample: Finding \\(P(B) = P(X = 2)\\) =&gt; Find outcomes \\(\\xi\\) result in a score of 2? \\(A = X^{-1}(2) = \\{HH\\}\\). Then we calculate \\(P(X = 2) = P(\\{HH\\}) = \\frac{1}{4}\\)\n\n\n\nOnce we have the Cumulative Distribution Function \\(F_X(x)\\), no longer need to go back to the sample space \\(\\Omega\\) every time; we can just subtract two values from the CDF.\n\n\nThe Cumulative Distribution Function (CDF) of a random variable \\(X\\) is the non-decreasing function \\(F_X: \\mathbb{R} \\to [0, 1]\\) defined by:\n\\[F_X(x) = P(X \\le x) = P(\\{\\xi \\in \\Omega \\mid X(\\xi) \\le x\\})\\]\nProbability is taken directly from the CDF: \\[P(x_1 &lt; X \\le x_2) = F_X(x_2) - F_X(x_1)\\]\n\n\n\n\nIt is the derivative of the Cumulative Distribution Function (CDF).\n\\[f_X(x) = \\frac{d}{dx}F_X(x)\\]\nProbability is equivalent to the area under the curve of the PDF:\n\\[P(x_1 &lt; X \\le x_2) = F_X(x_2) - F_X(x_1) = \\int_{x_1}^{x_2} f_X(x) dx\\]\n\n\n\nPDF \\(f_X(x)\\): This is not a probability. The probability at a single point \\(P(X = x)\\) is always 0. But the area under the curve of the PDF is the probability.\nCDF \\(F_X(x)\\): This is a probability - a piled up probability."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#the-probability-framework-omega-mathcalf-p",
    "href": "posts/pet-randomvariables/index.html#the-probability-framework-omega-mathcalf-p",
    "title": "Random Variables",
    "section": "",
    "text": "World: \\(\\Omega\\) (All possible outcomes \\(\\xi\\)) of an experiment (e.g.¬†\\(\\Omega = \\{HH, HT, TH, TT\\}\\))\nValidation Set: \\(\\mathcal{F}\\) (The list of all valid events, usually the Power Set)\nProbability Measure: \\(P\\) is the rule that assigns a likelihood [0,1] to those events. (e.g.¬†\\(P(\\{HH\\}) = 1/4\\))\nScoreboard: \\(X(\\xi)\\) (The function mapping outcomes to numbers - Random Variable)\nValidation Rule: \\(X^{-1}(B) \\in \\mathcal{F}\\) (Condition for \\(X\\) to be a Random Variable)"
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#what-is-a-random-variable-x",
    "href": "posts/pet-randomvariables/index.html#what-is-a-random-variable-x",
    "title": "Random Variables",
    "section": "",
    "text": "Despite the name, a Random Variable is not actually a variable in the algebraic sense‚Äîit is a FUNCTIONNNN, or a Scoreboard.\nA random variable \\(X\\) takes an outcome \\(\\xi\\) (‚Äúxi‚Äù) from the sample space \\(\\Omega\\) and maps it to a unique point \\(x\\) in \\(\\mathbb{R}\\).\n\\[X: \\Omega \\to \\mathbb{R}, X(\\xi) = x\\]\nExample: Let \\(X\\) be the ‚ÄúNumber of Heads‚Äù in two coin flips.\n\n\n\nOutcome (\\(\\xi\\))\nCalculation\nValue (\\(x\\))\n\n\n\n\n\\(HH\\)\n\\(X(HH)\\)\n\\(2\\)\n\n\n\\(HT\\)\n\\(X(HT)\\)\n\\(1\\)\n\n\n\\(TH\\)\n\\(X(TH)\\)\n\\(1\\)\n\n\n\\(TT\\)\n\\(X(TT)\\)\n\\(0\\)\n\n\n\n\nImportant: The above randomness values come from the sample space \\(\\Omega\\). \\(X\\) is simply the consistent rule (the function) that maps outcomes to numbers."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#how-to-calculate-probabilities-inverse-mapping",
    "href": "posts/pet-randomvariables/index.html#how-to-calculate-probabilities-inverse-mapping",
    "title": "Random Variables",
    "section": "",
    "text": "How do we determine the probability of a numerical value like \\(X = 2\\)? We have to look ‚Äúbackward‚Äù from the real numbers to our original sample space. This is called Inverse Mapping.\nGiven a set \\(B\\) (a subset of the real numbers \\(\\mathbb{R}\\)), we must find the event \\(A\\) in our sample space that ‚Äúmaps‚Äù to \\(B\\).\n\\[A = X^{-1}(B) = \\{\\xi \\in \\Omega : X(\\xi) \\in B\\}\\]\nProbability is taken directly from the sample \\(P(A)\\). \\[P(X \\in B) = P(A)\\]\nExample: Finding \\(P(B) = P(X = 2)\\) =&gt; Find outcomes \\(\\xi\\) result in a score of 2? \\(A = X^{-1}(2) = \\{HH\\}\\). Then we calculate \\(P(X = 2) = P(\\{HH\\}) = \\frac{1}{4}\\)"
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#or-calculating-probability-with-the-cdf",
    "href": "posts/pet-randomvariables/index.html#or-calculating-probability-with-the-cdf",
    "title": "Random Variables",
    "section": "",
    "text": "Once we have the Cumulative Distribution Function \\(F_X(x)\\), no longer need to go back to the sample space \\(\\Omega\\) every time; we can just subtract two values from the CDF.\n\n\nThe Cumulative Distribution Function (CDF) of a random variable \\(X\\) is the non-decreasing function \\(F_X: \\mathbb{R} \\to [0, 1]\\) defined by:\n\\[F_X(x) = P(X \\le x) = P(\\{\\xi \\in \\Omega \\mid X(\\xi) \\le x\\})\\]\nProbability is taken directly from the CDF: \\[P(x_1 &lt; X \\le x_2) = F_X(x_2) - F_X(x_1)\\]"
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#or-calculating-probability-via-the-pdf",
    "href": "posts/pet-randomvariables/index.html#or-calculating-probability-via-the-pdf",
    "title": "Random Variables",
    "section": "",
    "text": "It is the derivative of the Cumulative Distribution Function (CDF).\n\\[f_X(x) = \\frac{d}{dx}F_X(x)\\]\nProbability is equivalent to the area under the curve of the PDF:\n\\[P(x_1 &lt; X \\le x_2) = F_X(x_2) - F_X(x_1) = \\int_{x_1}^{x_2} f_X(x) dx\\]\n\n\n\nPDF \\(f_X(x)\\): This is not a probability. The probability at a single point \\(P(X = x)\\) is always 0. But the area under the curve of the PDF is the probability.\nCDF \\(F_X(x)\\): This is a probability - a piled up probability."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#the-gaussian-normal-distribution-x-sim-nmu-sigma2",
    "href": "posts/pet-randomvariables/index.html#the-gaussian-normal-distribution-x-sim-nmu-sigma2",
    "title": "Random Variables",
    "section": "1. The Gaussian (Normal) Distribution \\(X \\sim N(\\mu, \\sigma^2)\\)",
    "text": "1. The Gaussian (Normal) Distribution \\(X \\sim N(\\mu, \\sigma^2)\\)\nX counts the number of events that occurred in a fixed time interval.\n\nPDF\n\\[f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\nCDF (area of PDF)\n\\[F_X(x) = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}} dy\\]\nThe Problem: There is no antiderivative for \\(e^{-x^2}\\) that uses basic functions (like \\(\\sin, \\log,\\) or polynomials). You cannot solve this by hand. =&gt; We map this to a standardized distribution that was already solved numerically and just read the result from this mapping.\n\nExample\nSuppose we have a random variable \\(X \\sim N(10, 4)\\). We want to calculate: \\[P(X \\le 13) = F_X(13)\\]\nFollowing our definition, we set up the integral: \\[F_X(13) = \\int_{-\\infty}^{13} \\frac{1}{\\sqrt{2\\pi(4)}} e^{-\\frac{(y-10)^2}{2(4)}} dy\\]\n\n\nSolution: Standardization (Z-score)\nSince we can‚Äôt solve the integral for every different \\(\\mu\\) and \\(\\sigma\\), we transform our ‚ÄúSpecific \\(X\\)‚Äù into the ‚ÄúStandard \\(Z\\)‚Äù. We use the formula: \\[Z = \\frac{X - \\mu}{\\sigma}\\]\nFor our example (\\(X=13, \\mu=10, \\sigma=2\\)): \\[z = \\frac{13 - 10}{2} = 1.5\\]\nNow, instead of solving that impossible integral, we look up the value for \\(1.5\\) in a Standard Normal Table.\n\\[P(X \\le 13) = P(Z \\le 1.5) \\approx 0.9332\\]\n\n\n\nExpected Value and Variance\n\nExpected Value (The Mean)\n\\[\\mu = E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) dx\\]\n\n\nVariance (The Spread)\n\\[\\sigma^2 = Var(X) = E[(X - \\mu)^2]\\]\nComputational Trick: In practice, might be good in computer graphics, it is often easier to calculate variance like this since we know (\\(E[X]\\)) and (\\(E[X^2]\\)): \\[Var(X) = E[X^2] - (E[X])^2\\]"
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#uniform-distribution-x-sim-ua-b",
    "href": "posts/pet-randomvariables/index.html#uniform-distribution-x-sim-ua-b",
    "title": "Random Variables",
    "section": "2. Uniform Distribution \\(X \\sim U(a, b)\\)",
    "text": "2. Uniform Distribution \\(X \\sim U(a, b)\\)\nX counts the number of events that occurred in a fixed time interval. Perfectly fairness\n\nPDF\n\\[f_X(x) = \\frac{1}{b - a} \\quad \\text{for } a \\le x \\le b\\]\n\n\nApplication example:\nImagine you are waiting for a train that arrives exactly every 10 minutes, but you have no idea what the schedule is. Your wait time \\(X\\) is distributed as \\(U(0, 10)\\). * The probability of waiting between 0 and 10 minutes is 1. * The probability of waiting exactly between 5 and 7 minutes is: \\[P(5 &lt; X &lt; 7) = \\int_{5}^{7} \\frac{1}{10-0} dx = \\frac{7-5}{10} = 0.2\\]"
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#exponential-distribution-x-sim-textexplambda",
    "href": "posts/pet-randomvariables/index.html#exponential-distribution-x-sim-textexplambda",
    "title": "Random Variables",
    "section": "3. Exponential Distribution \\(X \\sim \\text{Exp}(\\lambda)\\)",
    "text": "3. Exponential Distribution \\(X \\sim \\text{Exp}(\\lambda)\\)\nX counts the time to the next event. Remember, it is about time until the next event.\n\nPDF\n\\[f_X(x) = \\lambda e^{-\\lambda x} \\quad \\text{for } x \\ge 0\\]\n\n\nKey Parameters\n\n\\(\\lambda\\) (Rate): The average number of events per unit of time.\n\\(\\mu\\) (Mean): The average time between events, calculated as \\(\\mu = 1/\\lambda\\).\n\n\n\nApplication example:\nThe Exponential distribution is famous for being ‚Äúmemoryless.‚Äù If you are modeling the time between incoming phone calls: For example, if calls arrive at a rate (\\(\\lambda\\)) of 2 per hour, the average time between calls (\\(\\mu\\)) is \\(1/2\\) an hour (30 minutes)."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#geometric-distribution-x-sim-textgeomp",
    "href": "posts/pet-randomvariables/index.html#geometric-distribution-x-sim-textgeomp",
    "title": "Random Variables",
    "section": "4. Geometric Distribution \\(X \\sim \\text{Geom}(p)\\)",
    "text": "4. Geometric Distribution \\(X \\sim \\text{Geom}(p)\\)\n\\(X\\) counts the number of failures before the first success occurs in a series of independent trials (Bernoulli trials).\n\nPMF\n\\[P(X = k) = (1-p)^k \\cdot p\\]\nWhere:\n\n\\(p\\) is the probability of success in each trial.\n\\((1-p)\\) is the probability of failure.\n\\(k\\) is the number of failures (\\(k = 0, 1, 2, \\dots\\)).\n\n\n\nApplication example:\nWhat is the probability you miss 3 times and make it on the 4th try with success probability \\(p=0.2\\)?\nCalculation: Here, \\(k=3\\). \\[P(X = 3) = (0.8)^3 \\cdot (0.2) = 0.512 \\cdot 0.2 = 0.1024\\] There is roughly a 10.2% chance that your first success happens exactly after 3 misses."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#bernoulli-distribution",
    "href": "posts/pet-randomvariables/index.html#bernoulli-distribution",
    "title": "Random Variables",
    "section": "5. Bernoulli Distribution",
    "text": "5. Bernoulli Distribution\nX counts the number of successes in a single trial. actually only \\(0\\) or \\(1\\). Cannot be simpler than this.\n\nValues: \\(X\\) can only be \\(1\\) (success) or \\(0\\) (failure).\nProbabilities: * \\(P(X=1) = p\\)\n\n\\(P(X=0) = q\\), where \\(q = 1-p\\)."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#binomial-distribution-x-sim-bn-p",
    "href": "posts/pet-randomvariables/index.html#binomial-distribution-x-sim-bn-p",
    "title": "Random Variables",
    "section": "6. Binomial Distribution \\(X \\sim B(n, p)\\)",
    "text": "6. Binomial Distribution \\(X \\sim B(n, p)\\)\nX counts the number of successes in a fixed number of independent Bernoulli trials (\\(n\\)).\nThe probability of getting exactly \\(k\\) successes in \\(n\\) trials is: \\[P(X = k) = \\binom{n}{k} p^k q^{n-k}\\]"
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#poisson-distribution-x-sim-textpoislambda",
    "href": "posts/pet-randomvariables/index.html#poisson-distribution-x-sim-textpoislambda",
    "title": "Random Variables",
    "section": "7. Poisson Distribution \\(X \\sim \\text{Pois}(\\lambda)\\)",
    "text": "7. Poisson Distribution \\(X \\sim \\text{Pois}(\\lambda)\\)\nX counts the number of times an event occurs in a fixed interval of time or space. It assumes events happen at a known constant rate (\\(\\lambda\\)) and independently of one another.\n\\[P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}\\]\n\nThe Parameter (\\(\\lambda\\)): This represents the average number of occurrences in that interval.\nRange: Technically, \\(k\\) can go from \\(0\\) all the way to infinity (\\(\\infty\\)).\n\nExample: The number of emails received in an hour or the number of typos on a page. Unlike the Binomial, there is no fixed number of ‚Äútrials‚Äù‚Äîthe events just ‚Äúarrive.‚Äù &gt; * Number of shooting stars seen in a night. &gt; * Number of customers entering a shop between 9:00 and 10:00 AM. &gt; * Number of mutations in a specific stretch of DNA.\n\nNumerical Example: The ‚ÄúBus Stop‚Äù Scenario\nImagine you are standing at a bus stop. You know from historical data that, on average, 3 buses arrive every hour.\n\n\\(\\lambda = 3\\) (This is your average rate per hour).\n\\(X\\) = The number of buses that actually show up in the next hour.\n\n\nScenario A: What is the probability that exactly 3 buses arrive?\nYou might think this is 100% since the average is 3, but in a random world, you might get lucky (4 buses) or unlucky (1 bus).\nUsing the formula \\(P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}\\) with \\(\\lambda = 3\\) and \\(k = 3\\):\n\\[P(X = 3) = \\frac{e^{-3} \\cdot 3^3}{3!} = \\frac{0.0498 \\cdot 27}{6} \\approx 0.224\\]\nThere is only a 22.4% chance that exactly 3 buses will show up.\n\n\nScenario B: What is the probability that NO buses arrive?\nThis is the ‚Äúfrustrated commuter‚Äù scenario (\\(k=0\\)).\n\\[P(X = 0) = \\frac{e^{-3} \\cdot 3^0}{0!} = \\frac{0.0498 \\cdot 1}{1} \\approx 0.05\\]\nThere is a 5% chance you will be left waiting with zero buses for the entire hour.\n\n\nScenario C: What if we change the time interval?\nThe beauty of the Poisson rate \\(\\lambda\\) is that it scales linearly. If the rate is 3 buses per 60 minutes, then for a 20-minute interval, your new \\(\\lambda\\) would be 1."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#comparison-of-discrete-distributions",
    "href": "posts/pet-randomvariables/index.html#comparison-of-discrete-distributions",
    "title": "Random Variables",
    "section": "Comparison of Discrete Distributions",
    "text": "Comparison of Discrete Distributions\n\n\n\n\n\n\n\n\n\nDistribution\nWhat does \\(X\\) count?\nKey Parameter(s)\nRange of \\(X\\)\n\n\n\n\nGeometric\nSuccess after \\(k\\) failures\n\\(p\\) (prob. of success)\n\\(\\{0, 1, \\dots, \\infty\\}\\)\n\n\nBernoulli\nSuccess in 1 trial\n\\(p\\) (prob. of success)\n\\(\\{0, 1\\}\\)\n\n\nBinomial\nSuccesses in \\(n\\) trials\n\\(n\\) (trials) and \\(p\\)\n\\(\\{0, 1, \\dots, n\\}\\)\n\n\nPoisson\nOccurrences over time/space\n\\(\\lambda\\) (average rate)\n\\(\\{0, 1, \\dots, \\infty\\}\\)\n\n\n\n\nThe Connection: When \\(n\\) is very large and \\(p\\) is very small, the Binomial distribution actually converges to the Poisson distribution. This is often called the ‚ÄúLaw of Rare Events.‚Äù"
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#solution-poisson-approximation",
    "href": "posts/pet-randomvariables/index.html#solution-poisson-approximation",
    "title": "Random Variables",
    "section": "Solution: Poisson Approximation",
    "text": "Solution: Poisson Approximation\nWhen \\(n\\) is very large (the number of trials) and \\(p\\) is very small (the probability of success), the Binomial distribution starts to behave exactly like a Poisson distribution.\n\nApproximation Rule\nIf \\(n \\ge 100\\) and \\(np \\le 10\\), we can ‚Äúswap‚Äù the Binomial parameters for a Poisson rate: \\[\\lambda = n \\cdot p\\]\nNow, instead of fighting with factorials like \\(100!\\), you can use the much simpler Poisson formula: \\[P(X = k) \\approx \\frac{e^{-\\lambda} \\lambda^k}{k!}\\]\n\n\nExample\nSuppose a factory produces 1,000 silicon chips, and the probability of a chip being defective is \\(p = 0.002\\).\n\nBinomial way: Calculating \\(P(X=3)\\) requires \\(\\binom{1000}{3} (0.002)^3 (0.998)^{997}\\). Basically we have to calculate \\(1000!\\).\nPoisson way: 1. Calculate \\(\\lambda = np = 1000 \\times 0.002 = 2\\). We only need to calculate \\(3!\\).\n\n\\(P(X=3) \\approx \\frac{e^{-2} \\cdot 2^3}{3!} = \\frac{0.1353 \\cdot 8}{6} \\approx 0.1804\\).\n\n\n\n\nDerivation\nI dont remember, but it is based on the fact that the convergence happen as \\(n \\to \\infty\\) and \\(p \\to 0\\):\nJust start from here: \\[P_n(k) = \\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}\\]\n\\[P_n(k) = \\frac{n!}{k!(n-k)!} p^k (1-p)^{n} (1-p)^{-k}\\]\n\\[P_n(k) = \\underbrace{\\frac{n(n-1)\\dots(n-k+1)}{n^k}}_{\\text{Part A}} \\cdot \\underbrace{\\frac{(np)^k}{k!}}_{\\text{Part B}} \\cdot \\underbrace{(1 - np/n)^n}_{\\text{Part C}} \\cdot \\underbrace{(1 - np/n)^{-k}}_{\\text{Part D}}\\]\n\\[P_n(k) \\approx (1) \\cdot \\frac{\\lambda^k}{k!} \\cdot e^{-\\lambda} \\cdot (1)\\]\n\nPart A: \\(\\frac{n}{n} \\cdot \\frac{n-1}{n} \\cdot \\dots \\cdot \\frac{n-k+1}{n}\\).Each of these \\(k\\) fractions (like \\(1 - \\frac{1}{n}\\)) tends toward 1 (unity).\nPart B: Since \\(np = \\lambda\\), replace \\(p\\) with \\(\\frac{\\lambda}{n}\\).\nPart C: \\(\\lim_{n \\to \\infty} (1 - \\frac{\\lambda}{n})^n = \\mathbf{e^{-\\lambda}}\\).\nPart D: As \\(n \\to \\infty\\), the term \\((1 - \\frac{\\lambda}{n})^{-k}\\) becomes \\((1 - 0)^{-k} = 1^{-k}\\), which is 1."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#direction-b-1-hard-conditioning-f_xxb",
    "href": "posts/pet-randomvariables/index.html#direction-b-1-hard-conditioning-f_xxb",
    "title": "Random Variables",
    "section": "Direction B-1: Hard Conditioning (\\(F_X(x|B)\\))",
    "text": "Direction B-1: Hard Conditioning (\\(F_X(x|B)\\))\nWe decide that some values are now impossible (Probability = 0) and others are possible. We cut the PDF and regrow the remaining part to keep the area at 1. The updated conditional distribution is the probability that \\(X \\le x\\) given that we know event \\(B\\) is true:\n\\[F_X(x | B) = P(X \\le x | B) = \\frac{P(\\{X \\le x\\} \\cap B)}{P(B)}\\]\n\\[f_X(x | B) = \\frac{d}{dx}F_X(x | B)\\]\n\nExample\nImagine a random variable \\(X\\) representing a wait time, uniformly distributed between 0 and 10 minutes: \\(X \\sim U(0, 10)\\).\n\nOriginal PDF: \\(f_X(x) = 1/10\\) for \\(0 \\le x \\le 10\\).\nOriginal CDF: \\(F_X(x) = x/10\\) for \\(0 \\le x \\le 10\\).\nCondition (\\(B\\)): You receive a text saying, ‚ÄúI know for a fact the wait is less than or equal to 5 minutes‚Äù (\\(B = \\{X \\le 5\\}\\)).\n\n\nStep 1: Update the CDF\nFor values where \\(x \\le 5\\), we scale the original CDF by the probability of the condition \\(P(B)\\):\n\nCalculate \\(P(B) = F_X(5) = 5/10 = 0.5\\). THIS VALUE IS FIXED\nFor \\(x \\le 5\\): \\(F_X(x | X \\le 5) = \\frac{x/10}{0.5} = \\frac{x}{5}\\). THIS IS THE NEW CDF FUNCTION\nFor \\(x &gt; 5\\): The probability is now \\(1\\) (it is certain the value is \\(\\le x\\) because we know it‚Äôs \\(\\le 5\\)).\n\n\n\nStep 2: Update the PDF\nUsing the derivative of our new CDF:\n\nFor \\(x \\le 5\\): \\(f_X(x | B) = \\frac{d}{dx}(\\frac{x}{5}) = 1/5\\).\nFor \\(x &gt; 5\\): \\(f_X(x | B) = 0\\).\n\nWhen the event B is \\(B = \\{a &lt; X \\le b\\}\\) find the Conditional CDF \\(F_X(x | B)\\), we look at the overlap between the event \\(\\{X \\le x\\}\\) and our new known interval \\(\\{a &lt; X \\le b\\}\\). The denominator is simply the probability of the interval itself: \\(F_X(b) - F_X(a)\\).\n\nFor \\(x &lt; a\\) (Below the window): The probability is 0.\nFor \\(a \\le x &lt; b\\) (Inside the window): \\[F_X(x | B) = \\frac{F_X(x) - F_X(a)}{F_X(b) - F_X(a)}\\]\nFor \\(x \\ge b\\) (Above the window): The probability is 1.\n\n\n\n\nExample\nImagine \\(X\\) is your exam score, ranging from 0 to 100. Let‚Äôs say the original distribution was \\(U(0, 100)\\).\nCondition \\(B\\): I tell you, ‚ÄúYou passed!‚Äù (Assuming a pass is \\(B = \\{50 &lt; X \\le 100\\}\\)).\n\nThe Lower Bound: Any score below 50 now has a conditional probability of 0. It is no longer possible that you got a 40.\nThe Scaling Effect: The denominator \\(P(B)\\) is \\(1.0 - 0.5 = 0.5\\).\nThe New Probability: Because we are dividing the original probabilities by \\(0.5\\), the likelihood of any specific passing score (like an 85) has effectively doubled."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#direction-b-2-soft-conditioning-f_xa",
    "href": "posts/pet-randomvariables/index.html#direction-b-2-soft-conditioning-f_xa",
    "title": "Random Variables",
    "section": "Direction B-2: Soft Conditioning (\\(f_{X|A}\\))",
    "text": "Direction B-2: Soft Conditioning (\\(f_{X|A}\\))\nIf our sensor is imperfect, \\(P(A | X=x)\\) might be \\(0.8\\) instead of \\(1.0\\). We multiply the original PDF by these weights and then re-normalize. =&gt; RESHAPE THE WHOLE DISTRIBUTION, NOT JUST SCALING\n\nGoal: ‚ÄúGiven that I know the signal is ON (\\(A_1\\)), what does the distribution (\\(f_{X|A}(x | A)\\)) of random variable (\\(X\\)) look like?‚Äù\n\n\\[f_{X|A}(x | A) = \\frac{P(A | X = x) f_X(x)}{P(A)}\\]\n\\[f_{X|A}(x | A) = \\frac{P(A | X = x) f_X(x)}{\\int_{-\\infty}^{+\\infty} P(A | X = x) f_X(x)dx}\\]\n\nExample: Soft Conditioning (Reshaping)\nIn this scenario, we don‚Äôt ‚Äúcut‚Äù the distribution; we ‚Äútilt‚Äù it based on an unreliable sensor. Instead of a hard threshold, we use a probabilistic weight.\n\n1. The Setup\n\nPrior Belief: Temperature \\(X \\sim N(20, 4)\\) with \\(\\mu=20\\) and \\(\\sigma^2=4\\).\nThe ‚ÄúSoft‚Äù Alarm (\\(A\\)): This sensor is noisy. The hotter it gets, the more likely it is to trigger, but it doesn‚Äôt have a sharp cutoff.\nThe Likelihood \\(P(A | X=x)\\): We define this as a linear weight function \\(w(x) = 0.02x\\).\n\nAt 10¬∞C, \\(P(A | X=10) = 0.2\\)\nAt 20¬∞C, \\(P(A | X=20) = 0.4\\)\nAt 30¬∞C, \\(P(A | X=30) = 0.6\\) (Note: This remains a valid probability between 0 and 1 for the relevant temperature range).\n\n\n\n\n2. Calculating the Numerator (Weighting)\nWe multiply the original Normal PDF by our weight function \\(0.02x\\) to find the unnormalized posterior: \\[\\text{Numerator} = (0.02x) \\cdot f_X(x)\\]\n\n\n3. Calculating the Denominator (Normalization)\nThe denominator \\(P(A)\\) is the ‚Äútotal weight,‚Äù which is the expected value of our weight function across the entire prior distribution: \\[P(A) = \\int_{-\\infty}^{\\infty} (0.02x) f_X(x) \\, dx = 0.02 \\cdot E[X]\\]\nSince the prior mean \\(E[X] = 20\\): \\[P(A) = 0.02 \\cdot 20 = 0.4\\]\n\n\n4. The Final Posterior PDF \\(f_{X|A}(x|A)\\)\nCombining the numerator and the denominator gives us the updated PDF: \\[f_{X|A}(x|A) = \\frac{0.02x \\cdot f_X(x)}{0.4} = \\frac{x}{20} f_X(x)\\]\n\nObservation: Notice that the new distribution is essentially the old distribution scaled by \\(x/20\\). Values higher than 20 are amplified, and values lower than 20 are suppressed, effectively ‚Äútilting‚Äù the bell curve to the right without creating a sharp ‚Äúcliff.‚Äù\n\n\n\n\nExample 2 - Hard Conditioning but can be applied same formula\nImagine we have a temperature sensor \\(X\\) in a server room. Usually, the temperature follows a Normal distribution \\(X \\sim N(20, 4)\\) (Mean 20¬∞C, Variance 4).\nEvent (\\(A\\)): A low-cost backup alarm triggers only if the temperature is above 22¬∞C. We receive a notification: ‚ÄúAlarm A has triggered.‚Äù\nGoal: Find the new PDF of the temperature, \\(f_{X|A}(x | A)\\), now that we know the alarm is active.\n\nStep 1: Define the components\n\nPrior PDF \\(f_X(x)\\): The original \\(N(20, 4)\\) curve.\nLikelihood \\(P(A | X = x)\\): This is a ‚ÄúStep Function.‚Äù\n\nIf \\(x \\le 22\\), \\(P(A | X=x) = 0\\) (The alarm cannot trigger).\nIf \\(x &gt; 22\\), \\(P(A | X=x) = 1\\) (The alarm must trigger).\n\nPrior Probability \\(P(A)\\): The probability the alarm triggers under normal conditions. Using Z-scores: \\(Z = (22-20)/2 = 1\\). From tables, \\(P(X &gt; 22) \\approx 0.1587\\).\n\n\n\nStep 2: Apply the Formula\nUsing the formula: \\[f_{X|A}(x | A) = \\frac{P(A | X = x) f_X(x)}{P(A)}\\]\n\nFor \\(x \\le 22\\): The numerator is \\(0 \\cdot f_X(x)\\), so the new density is 0.\nFor \\(x &gt; 22\\): The numerator is \\(1 \\cdot f_X(x)\\). We divide the original bell curve by the constant \\(0.1587\\).\n\n\n\nStep 3: The Result\nThe new distribution \\(f_{X|A}(x | A)\\) looks like the ‚Äútail‚Äù of the original bell curve, but it has been amplified (scaled up by \\(\\approx 6.3\\times\\)) so that the area under that specific tail now equals 1."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#direction-b-3-updating-the-probability-of-a-future-event-pba-beta-distribution-laplaces-rule-of-smoothed-succession",
    "href": "posts/pet-randomvariables/index.html#direction-b-3-updating-the-probability-of-a-future-event-pba-beta-distribution-laplaces-rule-of-smoothed-succession",
    "title": "Random Variables",
    "section": "Direction B-3: Updating the Probability of a Future Event (\\(P(B|A)\\)) (Beta Distribution + Laplace‚Äôs Rule of Smoothed Succession)",
    "text": "Direction B-3: Updating the Probability of a Future Event (\\(P(B|A)\\)) (Beta Distribution + Laplace‚Äôs Rule of Smoothed Succession)\nWe aren‚Äôt just looking at the past; we are updating our ‚Äúbrain‚Äù to predict the future. Imagine you are monitoring a wireless channel. You want to know the probability that the very next packet (the \\((n+1)^{th}\\)) will experience a collision.\nSince we have no data yet, we assume total uncertainty. Our prior p.d.f. \\(f_p(p)\\) is a Uniform Distribution over \\((0, 1)\\). We believe any rate from 0% to 100% is equally likely.\nThen we observe some data, We transmit \\(n\\) packets and observe that exactly \\(k\\) of them collided in a specific order: \\(P(A | P = p) = p^k (1-p)^{n-k}\\).\nTo update our belief, we first need the denominator (the evidence). We integrate the likelihood across our uniform prior: \\[P(A) = \\int_{0}^{1} p^k (1-p)^{n-k} dp = \\frac{(n-k)!k!}{(n+1)!}\\]\n\nExample: If you sent \\(n=2\\) packets and saw \\(k=1\\) collision, \\(P(A) = \\frac{1!1!}{3!} = 1/6\\).\n\nNow we update the distribution of \\(p\\) to get the A-posteriori p.d.f. \\(f_{P|A}(p|A)\\). This is our updated knowledge of the collision rate \\(p\\) after seeing the data.\n\\[f_{P|A}(p|A) = \\frac{P(A | p) f_p(p)}{P(A)}\\] We have \\(f_p(p) = 1\\) and \\(P(A | P = p) = p^k (1-p)^{n-k}\\) \\[f_{P|A}(p|A) = \\frac{p^k (1-p)^{n-k}}{\\left[ \\frac{k! (n-k)!}{(n+1)!} \\right]}\\] \\[f_{P|A}(p|A) = \\frac{(n+1)!}{(n-k)!k!} p^k (1-p)^{n-k}\\]\nVERY IMPORTANT, BETA DISTRIBUTION\n\nBefore (Prior): A flat horizontal line (Uniform).\nAfter (Posterior): A ‚ÄúBeta Distribution‚Äù curve that peaks near the observed ratio \\(k/n\\).\n\nFinally, we calculate the probability that the next packet (\\(B\\)) will collide. We do this by taking the expected value of \\(p\\) over our updated density:\n\\[P(B) = \\int_{0}^{1} p \\cdot f_{P|A}(p|A) dp\\]\nSubstituting our updated density: \\[P(B) = \\int_{0}^{1} p \\cdot \\left[ \\frac{(n+1)!}{(n-k)!k!} p^k (1-p)^{n-k} \\right] dp\\]\nAfter simplifying the factorials, we arrive at a VERY VERY VERY IMPORTANT AND INTUITIVE LAPLACE RULE OF SUCCESSION: \\[P(B) = \\frac{k+1}{n+2}\\]\nKi·ªÉu nh∆∞ l√† assume m√¨nh c√≥ th√™m 2 quan s√°t, 1 l√† success, 1 l√† failure, th√¨ c√°i x√°c su·∫•t c·ªßa m√¨nh n√≥ s·∫Ω ƒë∆∞·ª£c ‚Äútrung b√¨nh‚Äù l·∫°i, kh√¥ng b·ªã l·ªách qu√° v·ªÅ ph√≠a quan s√°t c·ªßa m√¨nh.\n\nSo yeah it is not simply \\(k/n\\)?\nIf you sent 1 packet and it collided (\\(k=1, n=1\\)), the raw ratio \\(k/n\\) would suggest a 100% collision rate for the future. That‚Äôs unrealistic! Laplace‚Äôs Rule gives you: \\[P(B) = \\frac{1+1}{1+2} = \\frac{2}{3} \\approx 66.7\\%\\]\nConclusion: Laplace‚Äôs Rule of Succession ‚Äúsmooths‚Äù our estimates. It accounts for the fact that with small sample sizes, our observations might be extreme, providing a much more reliable prediction for the \\((n+1)^{th}\\) event."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#direction-a-1-updating-the-probability-of-an-event-pa-xx",
    "href": "posts/pet-randomvariables/index.html#direction-a-1-updating-the-probability-of-an-event-pa-xx",
    "title": "Random Variables",
    "section": "Direction A-1: Updating the Probability of an Event (\\(P(A | X=x)\\))",
    "text": "Direction A-1: Updating the Probability of an Event (\\(P(A | X=x)\\))\nGoal: You have a measurement (\\(X=x=1.5\\)) and you want to know the probability that a specific event \\(A=ON\\) is true.\n\\[P(A | X = x) = \\frac{f_X(x | A)}{f_X(x)} P(A)\\]\n\nExample\nImagine we are monitoring a communication channel where a signal \\(A\\) is either ON (\\(A_1\\)) or OFF (\\(A_0\\)).\n\nPrior Knowledge: Based on history, \\(P(A_1) = 0.6\\) and \\(P(A_0) = 0.4\\).\nThe Noise: Our measurement device \\(X\\) adds Gaussian noise \\(N(\\mu, 1)\\).\n\nIf the signal is OFF, the mean is 0: \\(f_X(x | A_0) \\sim N(0, 1)\\).\nIf the signal is ON, the mean is 2: \\(f_X(x | A_1) \\sim N(2, 1)\\).\n\n\nThe Observation: We measure a specific value \\(X = 1.5\\). The Goal: Calculate the updated (Posterior) probability that the signal is actually ON.\n\nStep 1: Calculate the Likelihoods\nWe plug \\(x=1.5\\) into the Gaussian PDF formula for both scenarios:\n\nLikelihood if OFF: \\(f_X(1.5 | A_0) = \\frac{1}{\\sqrt{2\\pi}} e^{-(1.5-0)^2 / 2} \\approx 0.129\\)\nLikelihood if ON: \\(f_X(1.5 | A_1) = \\frac{1}{\\sqrt{2\\pi}} e^{-(1.5-2)^2 / 2} \\approx 0.352\\)\n\n\n\nStep 2: Calculate the Total Evidence\nWe find the total probability density at \\(x=1.5\\) by weighing both scenarios: \\[f_X(1.5) = [f_X(1.5 | A_1) \\cdot P(A_1)] + [f_X(1.5 | A_0) \\cdot P(A_0)]\\] \\[f_X(1.5) = (0.352 \\cdot 0.6) + (0.129 \\cdot 0.4) = 0.2628\\]\n\n\nStep 3: Apply Bayes‚Äô Theorem\nNow, we find the updated probability that the signal is ON: \\[P(A_1 | X = 1.5) = \\frac{f_X(1.5 | A_1) \\cdot P(A_1)}{f_X(1.5)}\\] \\[P(A_1 | X = 1.5) = \\frac{0.2112}{0.2628} \\approx \\mathbf{0.8037}\\]\n\nBefore the measurement (A-priori): We were 60% sure the signal was ON.\nAfter measuring 1.5 (Posterior): Because 1.5 is closer to the ‚ÄúON‚Äù mean (2) than the ‚ÄúOFF‚Äù mean (0), our confidence increased to 80.37%.\n\nThis shows how a single point observation (\\(X=x\\)) shifts our understanding of the world from a prior state to a more informed posterior state."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#direction-a-2-updating-the-probability-of-an-event-with-interval-observations",
    "href": "posts/pet-randomvariables/index.html#direction-a-2-updating-the-probability-of-an-event-with-interval-observations",
    "title": "Random Variables",
    "section": "Direction A-2: Updating the Probability of an Event with Interval Observations",
    "text": "Direction A-2: Updating the Probability of an Event with Interval Observations\nIn previous chapters, we updated our beliefs based on a specific point (\\(X=x\\)). However, in many real-world scenarios, we only know that a value fell within a certain range or interval.\n\\[P(A | x_1 &lt; X \\le x_2) = \\frac{\\int_{x_1}^{x_2} f_X(x | A)dx}{\\int_{x_1}^{x_2} f_X(x)dx} P(A)\\]\n\nExample\n\nEvent \\(A\\) (Overheating): Prior probability \\(P(A) = 0.10\\).\nEvent \\(A^c\\) (Normal): Prior probability \\(P(A^c) = 0.90\\).\nDistribution of Temperature (\\(X\\)):\n\nNormal State: \\(X \\sim N(70, 10^2)\\) (Mean 70¬∞C, SD 10¬∞C).\nOverheating State: \\(X \\sim N(100, 10^2)\\) (Mean 100¬∞C, SD 10¬∞C).\n\nThe Observation (\\(B\\)): A sensor reports the temperature is between 90¬∞C and 110¬∞C.\n\n\nStep 1: Calculate the Numerator \\(P(B|A)P(A)\\)\nFirst, we find the probability of seeing this temperature range if the machine is actually overheating (\\(X \\sim N(100, 10^2)\\)).\n\nStandardizing: \\(z_1 = \\frac{90-100}{10} = -1\\) and \\(z_2 = \\frac{110-100}{10} = 1\\).\nUsing the 68% rule: \\(P(B|A) \\approx 0.6827\\).\nNumerator: \\(0.6827 \\times 0.10 = \\mathbf{0.06827}\\).\n\n\n\nStep 2: Calculate the Denominator \\(P(B)\\)\nThe total probability of the temperature being in this range is the sum of both states:\n\nFrom Overheating: \\(0.06827\\) (calculated above).\nFrom Normal State (\\(X \\sim N(70, 10^2)\\)):\n\n\\(z_1 = \\frac{90-70}{10} = 2\\) and \\(z_2 = \\frac{110-70}{10} = 4\\).\n\\(P(B|A^c) = G(4) - G(2) \\approx 0.9999 - 0.9772 = 0.0227\\).\nContribution: \\(0.0227 \\times 0.90 = 0.02043\\).\n\n\nTotal \\(P(B)\\): \\(0.06827 + 0.02043 = \\mathbf{0.0887}\\).\n\n\nStep 3: Apply Bayes‚Äô Theorem\n\\[P(A|B) = \\frac{0.06827}{0.0887} \\approx \\mathbf{0.7697}\\]\n\nConclusion: Our belief that the machine is overheating jumped from 10% to nearly 77%."
  },
  {
    "objectID": "posts/pet-randomvariables/index.html#direction-a-3-updating-with-multiple-observations",
    "href": "posts/pet-randomvariables/index.html#direction-a-3-updating-with-multiple-observations",
    "title": "Random Variables",
    "section": "Direction A-3: Updating with Multiple Observations",
    "text": "Direction A-3: Updating with Multiple Observations\nWhat happens if we don‚Äôt just have one measurement, but a series of measurements \\(\\mathbf{x} = \\{x_1, x_2, \\dots, x_n\\}\\)? If the observations are independent and identically distributed (i.i.d.), each new piece of data allows us to update our belief further.\n\\[P(A | x_1, x_2, \\dots, x_n) = \\frac{f(x_1, x_2, \\dots, x_n | A) P(A)}{f(x_1, x_2, \\dots, x_n)}\\]\nBecause the observations are independent, we can simplify the likelihood into a product:\n\\[P(A | \\mathbf{x}) = \\frac{\\left[ \\prod_{i=1}^{n} f_X(x_i | A) \\right] P(A)}{f(\\mathbf{x})}\\]\n\nExample: Confirming the Overheat\nLet‚Äôs return to our Overheating Machine example.\n\nPrior: \\(P(A) = 0.10\\) (Overheating), \\(P(A^c) = 0.90\\) (Normal).\nNormal State (\\(A^c\\)): \\(X \\sim N(70, 10^2)\\).\nOverheating State (\\(A\\)): \\(X \\sim N(100, 10^2)\\).\n\nThe New Data: Instead of an interval, a digital logger gives us two precise readings: \\(x_1 = 92\\) and \\(x_2 = 98\\).\n\nStep 1: Calculate Likelihoods for State \\(A\\) (Overheating)\nWe use the PDF formula for \\(N(100, 10^2)\\):\n\n\\(f(92 | A) = \\frac{1}{10\\sqrt{2\\pi}} e^{-\\frac{(92-100)^2}{200}} \\approx 0.0287\\)\n\\(f(98 | A) = \\frac{1}{10\\sqrt{2\\pi}} e^{-\\frac{(98-100)^2}{200}} \\approx 0.0391\\)\nJoint Likelihood \\((A)\\): \\(0.0287 \\times 0.0391 = 0.001122\\)\n\n\n\nStep 2: Calculate Likelihoods for State \\(A^c\\) (Normal)\nWe use the PDF formula for \\(N(70, 10^2)\\):\n\n\\(f(92 | A^c) = \\frac{1}{10\\sqrt{2\\pi}} e^{-\\frac{(92-70)^2}{200}} \\approx 0.0035\\)\n\\(f(98 | A^c) = \\frac{1}{10\\sqrt{2\\pi}} e^{-\\frac{(98-70)^2}{200}} \\approx 0.0008\\)\nJoint Likelihood \\((A^c)\\): \\(0.0035 \\times 0.0008 = 0.0000028\\)\n\n\n\nStep 3: Apply Bayes‚Äô Theorem\nFirst, find the total evidence (denominator): \\[f(\\mathbf{x}) = (0.001122 \\times 0.10) + (0.0000028 \\times 0.90)\\] \\[f(\\mathbf{x}) = 0.0001122 + 0.00000252 = 0.00011472\\]\nNow, find the Posterior \\(P(A | \\mathbf{x})\\): \\[P(A | \\mathbf{x}) = \\frac{0.0001122}{0.00011472} \\approx \\mathbf{0.978}\\]"
  },
  {
    "objectID": "posts/ml4r-autoencoder-dreamer/index.html",
    "href": "posts/ml4r-autoencoder-dreamer/index.html",
    "title": "From Autoencoder to Dreamer",
    "section": "",
    "text": "Key Idea: Instead of mapping an input to a single point in the latent space, VAE maps it to parameters of a probability distribution (typically a Gaussian distribution) in the latent space:\n\nAn encoder that maps data to a distribution in latent space.\nA decoder that maps samples from this latent distribution back to data space.\n\n\n\nLet‚Äôs denote:\n\n\\(\\mathbf{x}\\): An input data point (e.g., an image)\n\\(\\mathbf{z}\\): A latent variable (vector) in the lower-dimensional latent space\n\\(p(\\mathbf{x})\\): The true, unknown data distribution we want to model\n\\(p(\\mathbf{z})\\): The prior distribution over the latent variables (typically a simple distribution like \\(\\mathcal{N}(0,I)\\))\n\\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\): The decoder (also called generative model or likelihood). This is a neural network parameterized by \\(\\theta\\) that outputs the parameters of the distribution over \\(\\mathbf{x}\\) given \\(\\mathbf{z}\\).\n\\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\): The encoder (also called inference model or approximate posterior). This is a neural network parameterized by \\(\\phi\\) that outputs the parameters of the distribution over \\(\\mathbf{z}\\) given \\(\\mathbf{x}\\).\n\nGoal: is to maximize the marginal likelihood: \\[p(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})}\\] which we do not know the other element to compute, or \\[\\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) d\\mathbf{z}\\] which is intractable\nTherefore, VAEs optimize the Evidence Lower Bound (ELBO), which is a lower bound on the log-likelihood of the data, we will derive it now\n\n\nThe log-likelihood of a data point \\(\\mathbf{x}\\) can be written as:\n\\[\\log p_\\theta(\\mathbf{x}) = \\log p_\\theta(\\mathbf{x})\\] \\[= \\log p_\\theta(\\mathbf{x}) \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) d\\mathbf{z} \\quad \\text{Multiply by 1}\\] \\[= \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) \\log p_\\theta(\\mathbf{x}) d\\mathbf{z} \\quad \\text{Bring inside the integral}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x})] \\quad \\text{Definition of expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{p_\\theta(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Apply the equation } p_\\theta(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{p_\\theta(\\mathbf{z}|\\mathbf{x})}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z}) q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta(\\mathbf{z}|\\mathbf{x}) q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Multiply by 1}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} + \\log \\frac{q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Split the expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] + D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p_\\theta(\\mathbf{z}|\\mathbf{x})) \\quad \\text{KL divergence}\\]\nThe second term is the Kullback-Leibler (KL) divergence, it is non-negative, now we only focus on maximizing the first term ELBO:\n\\[\\log p_\\theta(\\mathbf{x}) \\ge \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Chain rule of probability}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] + \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Split the expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z})) \\quad \\text{KL divergence}\\]\nThis is the objective function for the VAE (Loss Function), also the ELBO to maximize, or minimizing the negative ELBO.\n\\[\\mathcal{L}(\\theta, \\phi, \\mathbf{x}) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z}))\\]\n\n\n\nThis term encourages the decoder to reconstruct the input \\(\\mathbf{x}\\) accurately from a latent sample \\(\\mathbf{z}\\) drawn from the encoder‚Äôs output distribution.\nWe will derive it now\n\n\nWe assume that each data point \\(\\mathbf{x}\\) (e.g., an image, where each pixel is a dimension) is drawn from a Gaussian distribution, given the latent variable \\(\\mathbf{z}\\) that the decoder outputs. For simplicity, let‚Äôs assume the dimensions of \\(\\mathbf{x}\\) are independent given \\(\\mathbf{z}\\), and they share a fixed variance \\(\\sigma^2\\).\nSo, for each dimension \\(j\\) of \\(\\mathbf{x}\\), \\(x_j\\) is distributed as: \\(p_\\theta(x_j|\\mathbf{z}) = \\mathcal{N}(x_j; \\mu_j(\\mathbf{z}), \\sigma^2)\\)\nHere:\n\n\\(\\mu_j(\\mathbf{z})\\) is the mean for the \\(j\\)-th dimension, which is the output of your decoder network for that dimension when given \\(\\mathbf{z}\\).\n\\(\\sigma^2\\) is the variance. For simplicity, we often assume a fixed \\(\\sigma^2\\) (e.g., \\(\\sigma^2=1\\), or a small constant, or even absorbed into the weights). More advanced VAEs can learn this variance as well (the decoder outputs both mean and variance).\n\n\n\n\nThe PDF for a single-dimensional Gaussian variable \\(x_j\\) is:\n\\(f(x_j) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right)\\)\n\n\n\nSince we assume the dimensions of \\(\\mathbf{x}\\) are independent given \\(\\mathbf{z}\\), the joint probability \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\) is the product of the individual probabilities:\n\\(p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\prod_{j=1}^{D_x} p_\\theta(x_j|\\mathbf{z})\\)\nwhere \\(D_x\\) is the dimensionality of \\(\\mathbf{x}\\) (e.g., number of pixels in an image). Now, let‚Äôs take the logarithm of this product:\n\\[\\log p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\log \\left( \\prod_{j=1}^{D_x} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\log\\left(\\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right)\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2} \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) \\right) - \\sum_{j=1}^{D_x} \\left( \\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2} \\right)\\]The first term is a constant multiplied by \\(D_x\\): \\[= -\\frac{D_x}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{D_x} (x_j - \\mu_j(\\mathbf{z}))^2\\] \\[\\text{MSE} = \\frac{1}{D_x} \\sum_{j=1}^{D_x} (x_j - \\mu_j(\\mathbf{z}))^2\\]\n\n\n\n\nThis term acts as a regularizer. It pushes the approximate posterior \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) (the distribution output by the encoder for a given \\(\\mathbf{x}\\)) to be close to the prior distribution \\(p(\\mathbf{z})\\) (e.g., \\(\\mathcal{N}(0,I)\\)).\nIf \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is assumed to be a diagonal Gaussian \\(\\mathcal{N}(\\mu, \\Sigma)\\) (where \\(\\Sigma\\) is diagonal) and \\(p(\\mathbf{z})\\) is \\(\\mathcal{N}(0,I)\\), the KL divergence has a closed-form solution:\n\\[D_{KL}(\\mathcal{N}(\\mu,\\Sigma)||\\mathcal{N}(0,I)) = \\frac{1}{2} \\sum_{j=1}^{D_z} (\\exp(\\sigma_j) + \\mu_j^2 - 1 - \\sigma_j)\\] where \\(D_z\\) is the dimensionality of \\(\\mathbf{z}\\), and \\(\\mu_j\\) and \\(\\sigma_j\\) are the mean and log-variance (diagonal elements of \\(\\Sigma\\)) for the \\(j\\)-th latent dimension, as output by the encoder.\n\n\nLet‚Äôs say our latent space \\(\\mathbf{z}\\) is 2-dimensional (\\(D_z=2\\)). Our input is a specific image \\(\\mathbf{x}_{cat}\\) (an image of a cat).\nThe encoder network takes \\(\\mathbf{x}_{cat}\\) as input. Its output layer (after processing through several hidden layers) has two sets of \\(D_z=2\\) nodes:\n\nMean Output Nodes: For \\(\\mu_\\phi(\\mathbf{x}_{cat})\\)\nLog-Variance Output Nodes: For \\(\\log(\\sigma_\\phi(\\mathbf{x}_{cat})^2)\\) (we use log-variance for numerical stability, as variance must be positive).\n\nSuppose for this specific \\(\\mathbf{x}_{cat}\\), the encoder outputs:\n\\(\\mu_\\phi(\\mathbf{x}_{cat})=\\begin{bmatrix} 0.8 \\\\ -1.2 \\end{bmatrix}\\)\n\\(\\log(\\sigma_\\phi(\\mathbf{x}_{cat})^2)=\\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix}\\)\nFrom the log-variances, we calculate the variances:\n\\(\\sigma_1^2 = \\exp(-0.2) \\approx 0.8187\\) \\(\\sigma_2^2 = \\exp(0.1) \\approx 1.1052\\)\nSo, for this input \\(\\mathbf{x}_{cat}\\), the encoder defines the latent distribution: \\[q_\\phi(\\mathbf{z}|\\mathbf{x}_{cat})=\\mathcal{N}\\left(\\mathbf{z};\\begin{bmatrix} 0.8 \\\\ -1.2 \\end{bmatrix},\\begin{bmatrix} 0.8187 & 0 \\\\ 0 & 1.1052 \\end{bmatrix}\\right)\\]\nThis means:\n\nThe first latent dimension (\\(z_1\\)) is modeled by a Gaussian with mean 0.8 and variance 0.8187.\nThe second latent dimension (\\(z_2\\)) is modeled by a Gaussian with mean \\(-1.2\\) and variance 1.1052.\n\nThese two dimensions are assumed to be independent (because the off-diagonal elements of the covariance matrix are zero).\nWhen we ‚Äúsample \\(\\mathbf{z}\\) from \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_{cat})\\)‚Äù using the reparameterization trick, we would:\n\nSample \\(\\epsilon_1 \\sim \\mathcal{N}(0,1)\\) and \\(\\epsilon_2 \\sim \\mathcal{N}(0,1)\\).\nCalculate \\(z_1 = 0.8 + \\sqrt{0.8187} \\cdot \\epsilon_1\\)\nCalculate \\(z_2 = -1.2 + \\sqrt{1.1052} \\cdot \\epsilon_2\\)\n\nThe resulting \\(\\mathbf{z}=\\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix}\\) is then passed to the decoder.\nThis formulation allows the VAE to learn a distribution for each input, rather than a single point, enabling the desired properties of a continuous and meaningful latent space.\n\n\n\nImagine a very simplified VAE where our latent space \\(\\mathbf{z}\\) is just one-dimensional (\\(D_z=1\\)). Our prior \\(p(\\mathbf{z})\\) is \\(\\mathcal{N}(0,1)\\) (mean 0, variance 1). The VAE‚Äôs job is to learn an encoder (\\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\)) and a decoder (\\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\)) such that:\n\nThe decoder can reconstruct \\(\\mathbf{x}_A\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\).\nThe decoder can reconstruct \\(\\mathbf{x}_B\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\).\nThe KL divergence \\(D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))\\) is minimized for both \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\).\n\nScenario 1: No KL Regularization (like a vanilla Autoencoder)\nIf there were no KL term, the encoder might learn to map \\(\\mathbf{x}_A\\) to a specific point \\(\\mathbf{z}_A\\) and \\(\\mathbf{x}_B\\) to a specific point \\(\\mathbf{z}_B\\).\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(\\mu_A=-5.0, \\sigma_A=0.001\\) (a very tight distribution at -5.0)\nEncoder output for \\(\\mathbf{x}_B\\): \\(\\mu_B=+5.0, \\sigma_B=0.001\\) (a very tight distribution at +5.0)\n\nScenario 2: With KL Regularization (VAE)\nNow, the KL term \\(D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x})||\\mathcal{N}(0,1))\\) is active.\nLet‚Äôs say the encoder tries to map \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) far apart again:\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(\\mu_A=-5.0, \\sigma_A=0.1\\)\nEncoder output for \\(\\mathbf{x}_B\\): \\(\\mu_B=+5.0, \\sigma_B=0.1\\)\n\nLet‚Äôs calculate the KL divergence for \\(\\mathbf{x}_A\\): \\[D_{KL}(\\mathcal{N}(-5.0,0.1^2)||\\mathcal{N}(0,1)) = \\frac{1}{2} (\\exp(2\\log(0.1))+(-5.0)^2-1-2\\log(0.1))\\] \\[= \\frac{1}{2} (0.01+25-1-(-4.6)) = \\frac{1}{2} (24.01+4.6)=14.3\\]\nThis KL value (14.3) is very high! The VAE loss function will strongly penalize this. To minimize this KL term, the encoder is forced to:\n\nPull the means towards 0: \\(\\mu_A\\) and \\(\\mu_B\\) must be closer to 0.\nPush the variances towards 1: \\(\\sigma_A\\) and \\(\\sigma_B\\) must be closer to 1.\n\nSo, for similar inputs \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\), the trained encoder might output:\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)=\\mathcal{N}(\\mathbf{z};\\mu_A=-0.5,\\sigma_A=0.8)\\)\nEncoder output for \\(\\mathbf{x}_B\\): \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)=\\mathcal{N}(\\mathbf{z};\\mu_B=+0.5,\\sigma_B=0.8)\\)\n\nNow, let‚Äôs evaluate the KL divergence again (for \\(\\mathbf{x}_A\\)): \\[D_{KL}(\\mathcal{N}(-0.5,0.8^2)||\\mathcal{N}(0,1)) = \\frac{1}{2} (\\exp(2\\log(0.8))+(-0.5)^2-1-2\\log(0.8))\\] \\[= \\frac{1}{2} (0.64+0.25-1-(-0.446))= \\frac{1}{2} (-0.11+0.446)=0.168\\]\nThis KL value (0.168) is much smaller! The KL regularization successfully compressed the latent distributions closer to the prior.\nWhy this helps:\nThe only way for the encoder to reconstruct accurately AND stay close to the prior when presented with similar inputs is to map them to overlapping distributions.\n\n\n\n\n\nLet‚Äôs consider two distinct but very similar input data points, \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\), both from training distribution\nPressure from KL Divergence: Make them similar\n\nFor \\(\\mathbf{x}_A\\), the encoder will produce \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)=\\mathcal{N}(\\mu_A,\\Sigma_A)\\). The KL term wants \\(\\mu_A \\approx 0\\) and \\(\\Sigma_A \\approx I\\).\nFor \\(\\mathbf{x}_B\\), the encoder will produce \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)=\\mathcal{N}(\\mu_B,\\Sigma_B)\\). The KL term wants \\(\\mu_B \\approx 0\\) and \\(\\Sigma_B \\approx I\\).\n\nThe mathematical consequence of minimizing \\(D_{KL}(Q||P)\\) is that \\(Q\\) is forced to be similar to \\(P\\). Since \\(P\\) is the same prior for all \\(\\mathbf{x}\\), this means all \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) distributions for any \\(\\mathbf{x}\\) are pulled towards the same central region of the latent space (around 0). This inherently forces them to be closer to each other than they might be in a vanilla autoencoder without this regularization.\nPressure from Reconstruction Loss: Make them distinct\nIf the encoder were to map \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) to identical distributions (e.g., \\(\\mu_A=\\mu_B=0\\) and \\(\\Sigma_A=\\Sigma_B=I\\)), the reconstruction loss would be high to penalize that.\nThe Interplay (The ‚ÄúDual Pressure‚Äù):\n\nThe KL term pushes all latent distributions for different \\(\\mathbf{x}\\) towards the same central region of the latent space and encourages them to have a certain ‚Äúspread‚Äù (variance \\(\\approx I\\)). This means they will naturally overlap.\nThe reconstruction term pulls these distributions slightly apart (or adjusts their means and variances) just enough so that the decoder can distinguish between similar inputs and reconstruct them accurately.\n\nThe balance between these two forces is key. The optimal solution is where the encoder maps similar inputs \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) to latent distributions \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\) and \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\) that are:\n\nClose to each other (due to KL regularization towards the common prior).\nSignificantly overlapping (due to the variances being pushed towards 1 by KL, and their means being close).\nSlightly distinct in their means/variances such that the decoder can still reconstruct \\(\\mathbf{x}_A\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\) and \\(\\mathbf{x}_B\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\) with low reconstruction error.\n\n\n\nA challenge arises because sampling \\(\\mathbf{z}\\) from \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is a stochastic operation, which makes backpropagation through the sampling step difficult. The reparameterization trick solves this. Instead of sampling \\(\\mathbf{z} \\sim \\mathcal{N}(\\mu,\\Sigma)\\), we sample an auxiliary random variable \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0,I)\\) and then compute:\n\\[\\mathbf{z} = \\mu_\\phi(\\mathbf{x}) + \\sigma_\\phi(\\mathbf{x}) \\odot \\boldsymbol{\\epsilon}\\] (where \\(\\odot\\) is element-wise multiplication, and \\(\\sigma_\\phi(\\mathbf{x})\\) is the standard deviation, often computed from log-variance to ensure positivity). Now, the stochasticity is moved to \\(\\boldsymbol{\\epsilon}\\), and \\(\\mathbf{z}\\) becomes a deterministic function of \\(\\mu\\), \\(\\sigma\\), and \\(\\boldsymbol{\\epsilon}\\), allowing gradients to flow back through \\(\\mu_\\phi(\\mathbf{x})\\) and \\(\\sigma_\\phi(\\mathbf{x})\\) to the encoder‚Äôs parameters \\(\\phi\\).\n\n\n\n\n\nGenerative Model: By sampling a \\(\\mathbf{z}\\) from the simple prior \\(p(\\mathbf{z})\\) (e.g., standard normal) and passing it through the decoder \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\), we can generate entirely new data that resembles the training data\nVariational Inference & Trade-off between Reconstruction and Regularization: The balance between the reconstruction loss and the KL divergence term is critical. If the KL term is too strong, the model might prioritize learning the prior over accurate reconstruction, leading to blurry reconstructions. If too weak, the latent space might not be well-regularized for generation. This balance is often controlled by a weighting factor (beta-VAE)."
  },
  {
    "objectID": "posts/ml4r-autoencoder-dreamer/index.html#mathematical-formulation-of-vae",
    "href": "posts/ml4r-autoencoder-dreamer/index.html#mathematical-formulation-of-vae",
    "title": "From Autoencoder to Dreamer",
    "section": "",
    "text": "Let‚Äôs denote:\n\n\\(\\mathbf{x}\\): An input data point (e.g., an image)\n\\(\\mathbf{z}\\): A latent variable (vector) in the lower-dimensional latent space\n\\(p(\\mathbf{x})\\): The true, unknown data distribution we want to model\n\\(p(\\mathbf{z})\\): The prior distribution over the latent variables (typically a simple distribution like \\(\\mathcal{N}(0,I)\\))\n\\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\): The decoder (also called generative model or likelihood). This is a neural network parameterized by \\(\\theta\\) that outputs the parameters of the distribution over \\(\\mathbf{x}\\) given \\(\\mathbf{z}\\).\n\\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\): The encoder (also called inference model or approximate posterior). This is a neural network parameterized by \\(\\phi\\) that outputs the parameters of the distribution over \\(\\mathbf{z}\\) given \\(\\mathbf{x}\\).\n\nGoal: is to maximize the marginal likelihood: \\[p(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})}\\] which we do not know the other element to compute, or \\[\\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) d\\mathbf{z}\\] which is intractable\nTherefore, VAEs optimize the Evidence Lower Bound (ELBO), which is a lower bound on the log-likelihood of the data, we will derive it now\n\n\nThe log-likelihood of a data point \\(\\mathbf{x}\\) can be written as:\n\\[\\log p_\\theta(\\mathbf{x}) = \\log p_\\theta(\\mathbf{x})\\] \\[= \\log p_\\theta(\\mathbf{x}) \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) d\\mathbf{z} \\quad \\text{Multiply by 1}\\] \\[= \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) \\log p_\\theta(\\mathbf{x}) d\\mathbf{z} \\quad \\text{Bring inside the integral}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x})] \\quad \\text{Definition of expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{p_\\theta(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Apply the equation } p_\\theta(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{p_\\theta(\\mathbf{z}|\\mathbf{x})}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z}) q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta(\\mathbf{z}|\\mathbf{x}) q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Multiply by 1}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} + \\log \\frac{q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Split the expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] + D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p_\\theta(\\mathbf{z}|\\mathbf{x})) \\quad \\text{KL divergence}\\]\nThe second term is the Kullback-Leibler (KL) divergence, it is non-negative, now we only focus on maximizing the first term ELBO:\n\\[\\log p_\\theta(\\mathbf{x}) \\ge \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Chain rule of probability}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] + \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Split the expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z})) \\quad \\text{KL divergence}\\]\nThis is the objective function for the VAE (Loss Function), also the ELBO to maximize, or minimizing the negative ELBO.\n\\[\\mathcal{L}(\\theta, \\phi, \\mathbf{x}) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z}))\\]\n\n\n\nThis term encourages the decoder to reconstruct the input \\(\\mathbf{x}\\) accurately from a latent sample \\(\\mathbf{z}\\) drawn from the encoder‚Äôs output distribution.\nWe will derive it now\n\n\nWe assume that each data point \\(\\mathbf{x}\\) (e.g., an image, where each pixel is a dimension) is drawn from a Gaussian distribution, given the latent variable \\(\\mathbf{z}\\) that the decoder outputs. For simplicity, let‚Äôs assume the dimensions of \\(\\mathbf{x}\\) are independent given \\(\\mathbf{z}\\), and they share a fixed variance \\(\\sigma^2\\).\nSo, for each dimension \\(j\\) of \\(\\mathbf{x}\\), \\(x_j\\) is distributed as: \\(p_\\theta(x_j|\\mathbf{z}) = \\mathcal{N}(x_j; \\mu_j(\\mathbf{z}), \\sigma^2)\\)\nHere:\n\n\\(\\mu_j(\\mathbf{z})\\) is the mean for the \\(j\\)-th dimension, which is the output of your decoder network for that dimension when given \\(\\mathbf{z}\\).\n\\(\\sigma^2\\) is the variance. For simplicity, we often assume a fixed \\(\\sigma^2\\) (e.g., \\(\\sigma^2=1\\), or a small constant, or even absorbed into the weights). More advanced VAEs can learn this variance as well (the decoder outputs both mean and variance).\n\n\n\n\nThe PDF for a single-dimensional Gaussian variable \\(x_j\\) is:\n\\(f(x_j) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right)\\)\n\n\n\nSince we assume the dimensions of \\(\\mathbf{x}\\) are independent given \\(\\mathbf{z}\\), the joint probability \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\) is the product of the individual probabilities:\n\\(p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\prod_{j=1}^{D_x} p_\\theta(x_j|\\mathbf{z})\\)\nwhere \\(D_x\\) is the dimensionality of \\(\\mathbf{x}\\) (e.g., number of pixels in an image). Now, let‚Äôs take the logarithm of this product:\n\\[\\log p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\log \\left( \\prod_{j=1}^{D_x} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\log\\left(\\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right)\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2} \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) \\right) - \\sum_{j=1}^{D_x} \\left( \\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2} \\right)\\]The first term is a constant multiplied by \\(D_x\\): \\[= -\\frac{D_x}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{D_x} (x_j - \\mu_j(\\mathbf{z}))^2\\] \\[\\text{MSE} = \\frac{1}{D_x} \\sum_{j=1}^{D_x} (x_j - \\mu_j(\\mathbf{z}))^2\\]\n\n\n\n\nThis term acts as a regularizer. It pushes the approximate posterior \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) (the distribution output by the encoder for a given \\(\\mathbf{x}\\)) to be close to the prior distribution \\(p(\\mathbf{z})\\) (e.g., \\(\\mathcal{N}(0,I)\\)).\nIf \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is assumed to be a diagonal Gaussian \\(\\mathcal{N}(\\mu, \\Sigma)\\) (where \\(\\Sigma\\) is diagonal) and \\(p(\\mathbf{z})\\) is \\(\\mathcal{N}(0,I)\\), the KL divergence has a closed-form solution:\n\\[D_{KL}(\\mathcal{N}(\\mu,\\Sigma)||\\mathcal{N}(0,I)) = \\frac{1}{2} \\sum_{j=1}^{D_z} (\\exp(\\sigma_j) + \\mu_j^2 - 1 - \\sigma_j)\\] where \\(D_z\\) is the dimensionality of \\(\\mathbf{z}\\), and \\(\\mu_j\\) and \\(\\sigma_j\\) are the mean and log-variance (diagonal elements of \\(\\Sigma\\)) for the \\(j\\)-th latent dimension, as output by the encoder.\n\n\nLet‚Äôs say our latent space \\(\\mathbf{z}\\) is 2-dimensional (\\(D_z=2\\)). Our input is a specific image \\(\\mathbf{x}_{cat}\\) (an image of a cat).\nThe encoder network takes \\(\\mathbf{x}_{cat}\\) as input. Its output layer (after processing through several hidden layers) has two sets of \\(D_z=2\\) nodes:\n\nMean Output Nodes: For \\(\\mu_\\phi(\\mathbf{x}_{cat})\\)\nLog-Variance Output Nodes: For \\(\\log(\\sigma_\\phi(\\mathbf{x}_{cat})^2)\\) (we use log-variance for numerical stability, as variance must be positive).\n\nSuppose for this specific \\(\\mathbf{x}_{cat}\\), the encoder outputs:\n\\(\\mu_\\phi(\\mathbf{x}_{cat})=\\begin{bmatrix} 0.8 \\\\ -1.2 \\end{bmatrix}\\)\n\\(\\log(\\sigma_\\phi(\\mathbf{x}_{cat})^2)=\\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix}\\)\nFrom the log-variances, we calculate the variances:\n\\(\\sigma_1^2 = \\exp(-0.2) \\approx 0.8187\\) \\(\\sigma_2^2 = \\exp(0.1) \\approx 1.1052\\)\nSo, for this input \\(\\mathbf{x}_{cat}\\), the encoder defines the latent distribution: \\[q_\\phi(\\mathbf{z}|\\mathbf{x}_{cat})=\\mathcal{N}\\left(\\mathbf{z};\\begin{bmatrix} 0.8 \\\\ -1.2 \\end{bmatrix},\\begin{bmatrix} 0.8187 & 0 \\\\ 0 & 1.1052 \\end{bmatrix}\\right)\\]\nThis means:\n\nThe first latent dimension (\\(z_1\\)) is modeled by a Gaussian with mean 0.8 and variance 0.8187.\nThe second latent dimension (\\(z_2\\)) is modeled by a Gaussian with mean \\(-1.2\\) and variance 1.1052.\n\nThese two dimensions are assumed to be independent (because the off-diagonal elements of the covariance matrix are zero).\nWhen we ‚Äúsample \\(\\mathbf{z}\\) from \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_{cat})\\)‚Äù using the reparameterization trick, we would:\n\nSample \\(\\epsilon_1 \\sim \\mathcal{N}(0,1)\\) and \\(\\epsilon_2 \\sim \\mathcal{N}(0,1)\\).\nCalculate \\(z_1 = 0.8 + \\sqrt{0.8187} \\cdot \\epsilon_1\\)\nCalculate \\(z_2 = -1.2 + \\sqrt{1.1052} \\cdot \\epsilon_2\\)\n\nThe resulting \\(\\mathbf{z}=\\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix}\\) is then passed to the decoder.\nThis formulation allows the VAE to learn a distribution for each input, rather than a single point, enabling the desired properties of a continuous and meaningful latent space.\n\n\n\nImagine a very simplified VAE where our latent space \\(\\mathbf{z}\\) is just one-dimensional (\\(D_z=1\\)). Our prior \\(p(\\mathbf{z})\\) is \\(\\mathcal{N}(0,1)\\) (mean 0, variance 1). The VAE‚Äôs job is to learn an encoder (\\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\)) and a decoder (\\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\)) such that:\n\nThe decoder can reconstruct \\(\\mathbf{x}_A\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\).\nThe decoder can reconstruct \\(\\mathbf{x}_B\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\).\nThe KL divergence \\(D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))\\) is minimized for both \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\).\n\nScenario 1: No KL Regularization (like a vanilla Autoencoder)\nIf there were no KL term, the encoder might learn to map \\(\\mathbf{x}_A\\) to a specific point \\(\\mathbf{z}_A\\) and \\(\\mathbf{x}_B\\) to a specific point \\(\\mathbf{z}_B\\).\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(\\mu_A=-5.0, \\sigma_A=0.001\\) (a very tight distribution at -5.0)\nEncoder output for \\(\\mathbf{x}_B\\): \\(\\mu_B=+5.0, \\sigma_B=0.001\\) (a very tight distribution at +5.0)\n\nScenario 2: With KL Regularization (VAE)\nNow, the KL term \\(D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x})||\\mathcal{N}(0,1))\\) is active.\nLet‚Äôs say the encoder tries to map \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) far apart again:\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(\\mu_A=-5.0, \\sigma_A=0.1\\)\nEncoder output for \\(\\mathbf{x}_B\\): \\(\\mu_B=+5.0, \\sigma_B=0.1\\)\n\nLet‚Äôs calculate the KL divergence for \\(\\mathbf{x}_A\\): \\[D_{KL}(\\mathcal{N}(-5.0,0.1^2)||\\mathcal{N}(0,1)) = \\frac{1}{2} (\\exp(2\\log(0.1))+(-5.0)^2-1-2\\log(0.1))\\] \\[= \\frac{1}{2} (0.01+25-1-(-4.6)) = \\frac{1}{2} (24.01+4.6)=14.3\\]\nThis KL value (14.3) is very high! The VAE loss function will strongly penalize this. To minimize this KL term, the encoder is forced to:\n\nPull the means towards 0: \\(\\mu_A\\) and \\(\\mu_B\\) must be closer to 0.\nPush the variances towards 1: \\(\\sigma_A\\) and \\(\\sigma_B\\) must be closer to 1.\n\nSo, for similar inputs \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\), the trained encoder might output:\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)=\\mathcal{N}(\\mathbf{z};\\mu_A=-0.5,\\sigma_A=0.8)\\)\nEncoder output for \\(\\mathbf{x}_B\\): \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)=\\mathcal{N}(\\mathbf{z};\\mu_B=+0.5,\\sigma_B=0.8)\\)\n\nNow, let‚Äôs evaluate the KL divergence again (for \\(\\mathbf{x}_A\\)): \\[D_{KL}(\\mathcal{N}(-0.5,0.8^2)||\\mathcal{N}(0,1)) = \\frac{1}{2} (\\exp(2\\log(0.8))+(-0.5)^2-1-2\\log(0.8))\\] \\[= \\frac{1}{2} (0.64+0.25-1-(-0.446))= \\frac{1}{2} (-0.11+0.446)=0.168\\]\nThis KL value (0.168) is much smaller! The KL regularization successfully compressed the latent distributions closer to the prior.\nWhy this helps:\nThe only way for the encoder to reconstruct accurately AND stay close to the prior when presented with similar inputs is to map them to overlapping distributions."
  },
  {
    "objectID": "posts/ml4r-autoencoder-dreamer/index.html#how-two-terms-work-with-each-other-to-ensure-a-smooth-generative-model-for-unseen-data",
    "href": "posts/ml4r-autoencoder-dreamer/index.html#how-two-terms-work-with-each-other-to-ensure-a-smooth-generative-model-for-unseen-data",
    "title": "From Autoencoder to Dreamer",
    "section": "",
    "text": "Let‚Äôs consider two distinct but very similar input data points, \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\), both from training distribution\nPressure from KL Divergence: Make them similar\n\nFor \\(\\mathbf{x}_A\\), the encoder will produce \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)=\\mathcal{N}(\\mu_A,\\Sigma_A)\\). The KL term wants \\(\\mu_A \\approx 0\\) and \\(\\Sigma_A \\approx I\\).\nFor \\(\\mathbf{x}_B\\), the encoder will produce \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)=\\mathcal{N}(\\mu_B,\\Sigma_B)\\). The KL term wants \\(\\mu_B \\approx 0\\) and \\(\\Sigma_B \\approx I\\).\n\nThe mathematical consequence of minimizing \\(D_{KL}(Q||P)\\) is that \\(Q\\) is forced to be similar to \\(P\\). Since \\(P\\) is the same prior for all \\(\\mathbf{x}\\), this means all \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) distributions for any \\(\\mathbf{x}\\) are pulled towards the same central region of the latent space (around 0). This inherently forces them to be closer to each other than they might be in a vanilla autoencoder without this regularization.\nPressure from Reconstruction Loss: Make them distinct\nIf the encoder were to map \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) to identical distributions (e.g., \\(\\mu_A=\\mu_B=0\\) and \\(\\Sigma_A=\\Sigma_B=I\\)), the reconstruction loss would be high to penalize that.\nThe Interplay (The ‚ÄúDual Pressure‚Äù):\n\nThe KL term pushes all latent distributions for different \\(\\mathbf{x}\\) towards the same central region of the latent space and encourages them to have a certain ‚Äúspread‚Äù (variance \\(\\approx I\\)). This means they will naturally overlap.\nThe reconstruction term pulls these distributions slightly apart (or adjusts their means and variances) just enough so that the decoder can distinguish between similar inputs and reconstruct them accurately.\n\nThe balance between these two forces is key. The optimal solution is where the encoder maps similar inputs \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) to latent distributions \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\) and \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\) that are:\n\nClose to each other (due to KL regularization towards the common prior).\nSignificantly overlapping (due to the variances being pushed towards 1 by KL, and their means being close).\nSlightly distinct in their means/variances such that the decoder can still reconstruct \\(\\mathbf{x}_A\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\) and \\(\\mathbf{x}_B\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\) with low reconstruction error.\n\n\n\nA challenge arises because sampling \\(\\mathbf{z}\\) from \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is a stochastic operation, which makes backpropagation through the sampling step difficult. The reparameterization trick solves this. Instead of sampling \\(\\mathbf{z} \\sim \\mathcal{N}(\\mu,\\Sigma)\\), we sample an auxiliary random variable \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0,I)\\) and then compute:\n\\[\\mathbf{z} = \\mu_\\phi(\\mathbf{x}) + \\sigma_\\phi(\\mathbf{x}) \\odot \\boldsymbol{\\epsilon}\\] (where \\(\\odot\\) is element-wise multiplication, and \\(\\sigma_\\phi(\\mathbf{x})\\) is the standard deviation, often computed from log-variance to ensure positivity). Now, the stochasticity is moved to \\(\\boldsymbol{\\epsilon}\\), and \\(\\mathbf{z}\\) becomes a deterministic function of \\(\\mu\\), \\(\\sigma\\), and \\(\\boldsymbol{\\epsilon}\\), allowing gradients to flow back through \\(\\mu_\\phi(\\mathbf{x})\\) and \\(\\sigma_\\phi(\\mathbf{x})\\) to the encoder‚Äôs parameters \\(\\phi\\)."
  },
  {
    "objectID": "posts/ml4r-autoencoder-dreamer/index.html#key-points-of-vae",
    "href": "posts/ml4r-autoencoder-dreamer/index.html#key-points-of-vae",
    "title": "From Autoencoder to Dreamer",
    "section": "",
    "text": "Generative Model: By sampling a \\(\\mathbf{z}\\) from the simple prior \\(p(\\mathbf{z})\\) (e.g., standard normal) and passing it through the decoder \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\), we can generate entirely new data that resembles the training data\nVariational Inference & Trade-off between Reconstruction and Regularization: The balance between the reconstruction loss and the KL divergence term is critical. If the KL term is too strong, the model might prioritize learning the prior over accurate reconstruction, leading to blurry reconstructions. If too weak, the latent space might not be well-regularized for generation. This balance is often controlled by a weighting factor (beta-VAE)."
  },
  {
    "objectID": "posts/ml4r-autoencoder-dreamer/index.html#modules-involved",
    "href": "posts/ml4r-autoencoder-dreamer/index.html#modules-involved",
    "title": "From Autoencoder to Dreamer",
    "section": "Modules Involved:",
    "text": "Modules Involved:\n\nEncoder (Representation Model): \\(q_\\phi(s_t|h_t,o_t)\\) maps observation \\(o_t\\) and recurrent state \\(h_t\\) to posterior distribution parameters (\\(\\mu_{post},\\sigma_{post}\\)) for \\(s_t\\).\nDecoder (Observation Model): \\(p_\\theta(o_t|s_t)\\) maps stochastic state \\(s_t\\) to observation reconstruction parameters.\nReward Model: \\(p_\\theta(r_t|s_t)\\) maps stochastic state \\(s_t\\) to reward prediction parameters.\nRecurrent Model (Deterministic Dynamics): \\(f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})\\) updates \\(h_t\\).\nPrior Model (Transition Dynamics): \\(p(s_t|h_t)\\) maps recurrent state \\(h_t\\) to prior distribution parameters (\\(\\mu_{prior},\\sigma_{prior}\\)) for \\(s_t\\).\nActor (Policy Network): \\(p_\\psi(a_t|s_t)\\) maps stochastic state \\(s_t\\) to action distribution parameters.\nCritic (Value Network): \\(V_\\psi(s_t)\\) predicts the expected future value from state \\(s_t\\)."
  },
  {
    "objectID": "posts/ml4r-autoencoder-dreamer/index.html#training-steps-one-iteration",
    "href": "posts/ml4r-autoencoder-dreamer/index.html#training-steps-one-iteration",
    "title": "From Autoencoder to Dreamer",
    "section": "Training Steps (One Iteration):",
    "text": "Training Steps (One Iteration):\n\nPhase 1: Data Collection (Real-World Interaction)\n\nObserve Current State: The agent receives the current observation \\(o_t\\) from the real environment.\nInfer Latent State (Encoder): Using the Encoder and Recurrent Model, infer the current stochastic latent state \\(s_t\\) and update the deterministic hidden state \\(h_t\\):\n\n\\(h_t=f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})\\) (using the last actual state and action from the environment)\n\\(q(s_t|h_t,o_t)\\) is computed by the Encoder. A sample \\(s_t\\) is drawn (using the reparameterization trick) from this posterior distribution.\n\nChoose Action (Actor): Feed the inferred latent state \\(s_t\\) to the Actor (Policy Network) \\(p_\\psi(a_t|s_t)\\) to get an action distribution. Sample an action \\(a_t\\) from this distribution (e.g., using \\(\\epsilon\\)-greedy for exploration or pure sampling for stochastic policies).\nExecute Action: Execute action \\(a_t\\) in the real environment.\nReceive Feedback: Get the next observation \\(o_{t+1}\\) and reward \\(r_{t+1}\\) from the environment.\nStore Experience: Store the tuple \\((o_t,a_t,r_{t+1},o_{t+1})\\) in a replay buffer.\n\n\n\nPhase 2: Model Training (World Model Update)\nThis phase happens after collecting a batch of new experiences (e.g., a short trajectory or a few steps) or can be done continuously from the replay buffer.\n\nSample Batch: Sample a batch of (e.g., 50-step) sequences from the replay buffer.\nProcess Sequence (RSSM Pass): For each sequence in the batch, process it through the Recurrent State-Space Model (RSSM):\n\nFor each step \\(t\\) in the sequence:\n\nUpdate Recurrent State: \\(h_t=f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})\\)\nCompute Prior: \\(p(s_t|h_t)\\) (using the Prior Model)\nCompute Posterior: \\(q(s_t|h_t,o_t)\\) (using the Encoder and the observed \\(o_t\\))\nSample Latent State: Sample \\(s_t\\) from the posterior \\(q(s_t|h_t,o_t)\\) (using reparameterization trick for gradients).\n\n\nCalculate World Model Loss: The total world model loss is calculated for the entire batch. It typically combines three terms for each step \\(t\\):\n\nReconstruction Loss: \\(Loss_{obs} = -\\log p_\\theta(o_t|s_t)\\) (from Decoder). This encourages \\(s_t\\) to contain enough information to reconstruct the observation.\nReward Loss: \\(Loss_{reward} = -\\log p_\\theta(r_t|s_t)\\) (from Reward Model). This encourages \\(s_t\\) to contain enough information to predict the reward.\nKL Divergence (Consistency Loss): \\(Loss_{KL} = D_{KL}(q(s_t|h_t,o_t)||p(s_t|h_t))\\). This is crucial:\n\nIt regularizes the posterior \\(q\\) towards the prior \\(p\\).\nIt also forces the Prior Model to learn to accurately predict the next stochastic state before seeing the actual observation, by pulling it closer to the posterior that does see the observation. This is key for effective imagination.\n\nTotal World Model Loss: \\(L_{WM} = \\sum_t (Loss_{obs} + Loss_{reward} + \\beta \\cdot Loss_{KL})\\) (where \\(\\beta\\) is a KL weight, often 1 or adjusted).\n\nOptimize World Model: Perform an optimization step (e.g., Adam) to update the parameters of the Encoder, Decoder, Reward Model, Recurrent Model, and Prior Model based on \\(L_{WM}\\).\n\n\n\nPhase 3: Behavior Learning (Policy & Value Update by Imagination)\nThis phase also happens concurrently with World Model training, typically after a few world model updates. It leverages the current learned world model.\n\nSample Initial States: Sample a batch of latent states \\(s_k\\) from recent past experiences in the replay buffer. These serve as starting points for imagined trajectories.\nImagine Trajectories: For each sampled \\(s_k\\):\n\nRollout: Use the learned Prior Model (\\(p(s_{t+1}|h_{t+1})\\)) and Recurrent Model (\\(h_{t+1}=f_{recurrent}(\\dots)\\)) to simulate forward in the latent space for \\(H\\) steps (the imagination horizon).\nAct in Imagination: At each step \\(t'\\) in the imagination, the Actor \\(p_\\psi(a_{t'}|s_{t'})\\) chooses an action \\(a_{t'}\\).\nPredict Reward: The Reward Model \\(p_\\theta(r_{t'}|s_{t'})\\) predicts the reward for that step.\nPredict Value: The Critic \\(V_\\psi(s_{t'})\\) predicts the value of the future state.\nThis generates an imagined trajectory of states (\\(s_k,s'_{k+1},\\dots,s'_{k+H}\\)) and rewards (\\(r'_k,\\dots,r'_{k+H}\\)).\n\nCalculate Policy Loss:\n\nValue Target: Calculate value targets (e.g., GAE-lambda returns or N-step returns) using the predicted imagined rewards \\(r'_{t'}\\) and the Critic‚Äôs predicted values \\(V_\\psi(s'_{t'})\\) along the imagined trajectory.\nActor Loss: Optimize the Actor to maximize the expected value from imagined trajectories. This typically involves maximizing the expected return from actions chosen by the policy. Gradients flow back through the entire imagined sequence and the dynamics of the world model.\nCritic Loss: Optimize the Critic to accurately predict the value of imagined states, typically using an MSE loss between its prediction and the calculated value targets.\nTotal Policy Loss: \\(L_{Policy} = \\text{Actor Loss} + \\text{Critic Loss}\\)\n\nOptimize Policy & Value Networks: Perform an optimization step (e.g., Adam) to update the parameters of the Actor and Critic Networks based on \\(L_{Policy}\\).\n\n\n1. Parameters We Are Trying to Learn (The \\(\\psi\\) Parameters)\nIn the behavior learning phase, we are trying to learn the parameters of two distinct neural networks:\n\nActor Network (Policy Network): \\(p_\\psi(a_t|s_t)\\)\n\nParameters: These are the weights and biases of the neural network that takes a latent state \\(s_t\\) as input and outputs a distribution over actions \\(a_t\\). Let‚Äôs denote these specific parameters as \\(\\psi_{actor}\\).\nPurpose: To learn a policy that chooses actions which maximize expected future rewards within the imagined world.\n\nCritic Network (Value Network): \\(V_\\psi(s_t)\\)\n\nParameters: These are the weights and biases of the neural network that takes a latent state \\(s_t\\) as input and outputs a single scalar value, representing the estimated expected future return (value) from that state. Let‚Äôs denote these specific parameters as \\(\\psi_{critic}\\).\nPurpose: To learn to accurately predict the ‚Äúgoodness‚Äù (value) of a given latent state. This value prediction is then used as a baseline and target for training the Actor.\n\n\nSo, when we talk about optimizing \\(\\psi\\), we are jointly optimizing \\(\\psi_{actor}\\) and \\(\\psi_{critic}\\).\n\n\n2. Loss Functions Summed Up (Mathematical Formulation)\nThe total policy loss, \\(L_{Policy}(\\psi)\\), is composed of two main parts, each designed to update its respective network:\n\\[L_{Policy}(\\psi) = \\underbrace{L_{Critic}(\\psi_{critic})}_{\\text{Value Estimation Loss}} + \\underbrace{L_{Actor}(\\psi_{actor})}_{\\text{Policy Improvement Loss}}\\]\nLet‚Äôs detail each:\n\nCritic Loss: \\(L_{Critic}(\\psi_{critic})\\)\nGoal: Make the Critic‚Äôs value predictions \\(V_{\\psi_{critic}}(s''_{t'})\\) as accurate as possible for the imagined states \\(s''_{t'}\\). ‚ÄúAccurate‚Äù here means matching the calculated value targets (or \\(\\lambda\\)-returns) \\(V_{target}(s''_{t'})\\).\nFormulation: It‚Äôs a standard Mean Squared Error (MSE) loss.\n\\[L_{Critic}(\\psi_{critic}) = \\frac{1}{M} \\sum_{m=1}^{M} (V_{\\psi_{critic}}(s_m'') - V_{target}(s_m''))^2\\]\nWhere:\n\n\\(M\\) is the total number of (state, target) pairs from all imagined trajectories in the current batch.\n\\(s_m''\\) is an imagined latent state at some time step \\(t'\\) from an imagined trajectory.\n\\(V_{\\psi_{critic}}(s_m'')\\): The Critic‚Äôs Prediction (The Thing We Want to Improve)\n\nWhat it is: This is the current output of your Critic neural network. It‚Äôs the network‚Äôs best guess or estimate of the expected sum of future discounted rewards that the agent will receive if it starts from the imagined latent state \\(s_m''\\) and then follows its current policy \\(p_{\\psi_{actor}}\\) thereafter.\nSource: It comes directly from the forward pass of the Critic network (\\(V_{\\psi_{critic}}\\)) with \\(s_m''\\) as input.\nPurpose in Loss: This is the value that we are trying to adjust during training. The Critic loss will compute how ‚Äúwrong‚Äù this prediction is compared to the target, and then use that error to update the Critic‚Äôs parameters \\(\\psi_{critic}\\) via backpropagation. We want to make this prediction get closer to the target value.\n\n\\(V_{target}(s_m'')\\) is the computed target value for \\(s_m''\\) (Ground Truth). This target is crucial and calculated using the imagined rewards \\(r''_{t'+1}\\) and the bootstrapped value predictions from the Critic itself, specifically the \\(\\lambda\\)-return (or GAE-based target): \\[ V_{target}(s_{t'}' ) = \\sum_{j=0}^{H-t'-1} (\\gamma\\lambda)^j r_{t'+j+1}' + (\\gamma\\lambda)^{H-t'} V_{\\psi_{critic}}(s_{k+H}') \\quad \\text{(simplified } \\lambda\\text{-return)}\\] More robustly, using GAE: \\[ V_{target}(s_{t'}' ) = V_{\\psi_{critic}}(s_{t'}' ) + A_{t'}^{\\text{GAE}(\\gamma, \\lambda)}\\] where \\(A_{t'}^{\\text{GAE}(\\gamma,\\lambda)} = \\sum_{j=0}^{H-t'-1} (\\gamma\\lambda)^j \\delta''_{t'+j}\\) and \\(\\delta''_{t'} = r''_{t'+1} + \\gamma V_{\\psi_{critic}}(s''_{t'+1}) - V_{\\psi_{critic}}(s''_{t'})\\).\n\n\n\nActor Loss: \\(L_{Actor}(\\psi_{actor})\\)\nGoal: Adjust the Actor‚Äôs policy parameters \\(\\psi_{actor}\\) so that actions that lead to higher rewards (higher advantages) become more probable. Also, encourage exploration (entropy).\nFormulation: This is a policy gradient loss, typically based on the REINFORCE algorithm but with the addition of a value baseline (the Critic‚Äôs prediction) and entropy regularization.\n\\[L_{Actor}(\\psi_{actor}) = -\\frac{1}{M} \\sum_{m=1}^{M} (A_m^{\\text{GAE}(\\gamma,\\lambda)} \\cdot \\log p_{\\psi_{actor}}(a_m''|s_m'') + \\beta_H \\cdot H(p_{\\psi_{actor}}(a_m''|s_m'')))\\]\nWhere:\n\n\\(M\\) is the total number of (state, action, advantage) triples from all imagined trajectories.\n\\(A_m^{\\text{GAE}(\\gamma,\\lambda)}\\) is the calculated Generalized Advantage Estimate for the imagined state \\(s_m''\\) and action \\(a_m''\\). This term serves as the ‚Äúcredit assignment‚Äù for the action.\n\\(\\log p_{\\psi_{actor}}(a_m''|s_m'')\\) is the log-probability of the imagined action \\(a_m''\\) under the current policy \\(p_{\\psi_{actor}}\\). This is the standard policy gradient term.\n\\(H(p_{\\psi_{actor}}(a_m''|s_m''))\\) is the entropy of the action distribution at state \\(s_m''\\).\n\\(\\beta_H\\) is a hyperparameter for entropy regularization. A positive \\(\\beta_H\\) means we add a negative entropy term to the loss, which encourages maximizing entropy (more exploration).\n\nI will explain more down here\n\n1. The Thing We Want to Improve: \\(\\log p_{\\psi_{actor}}(a_m''|s_m'')\\) (The Policy Itself)\n\nWhat it is: This term represents the log-probability of the action \\(a_m''\\) that was actually chosen by the Actor (policy) network for the imagined latent state \\(s_m''\\). The Actor network \\(p_{\\psi_{actor}}(a|s)\\) outputs an action distribution, and this is the log-likelihood of a specific action under that distribution.\nSource: It comes directly from a forward pass of the Actor network \\(p_{\\psi_{actor}}\\) given input \\(s_m''\\), and then evaluating the log-probability of \\(a_m''\\).\nPurpose in Loss: This is the core part of the policy that we are trying to adjust. By optimizing the Actor loss, we are trying to change the parameters \\(\\psi_{actor}\\) such that the probability of ‚Äúgood‚Äù actions increases, and the probability of ‚Äúbad‚Äù actions decreases.\n\n\n\n2. The ‚ÄúGround Truth‚Äù (The Guidance Signal): \\(A_m^{\\text{GAE}(\\gamma,\\lambda)}\\) (The Advantage)\n\nWhat it is: This is the Generalized Advantage Estimate (GAE) for the imagined state-action pair (\\(s_m'',a_m''\\)). The advantage function tells us how much better (or worse) a specific action \\(a_m''\\) taken in state \\(s_m''\\) was compared to the average expected value of that state \\(V_{\\psi_{critic}}(s_m'')\\).\n\nIf \\(A_m^{\\text{GAE}} &gt; 0\\): The action \\(a_m''\\) led to better-than-expected rewards.\nIf \\(A_m^{\\text{GAE}} &lt; 0\\): The action \\(a_m''\\) led to worse-than-expected rewards.\n\nSource: This is computed based on the imagined rewards \\(r''_{t'}\\) (from the Reward Model) and the Critic‚Äôs value predictions \\(V_{\\psi_{critic}}(s''_{t'})\\) (from the Critic Network).\nPurpose in Loss: This is the signal that tells the Actor whether the action it just took (in imagination) was good or bad. It serves as the ‚Äúground truth‚Äù in the sense that it‚Äôs the target direction and magnitude for policy improvement. We want to increase the likelihood of actions associated with positive advantages and decrease the likelihood of actions associated with negative advantages.\n\n\n\n3. The Entropy Term: \\(\\beta_H \\cdot H(p_{\\psi_{actor}}(a_m''|s_m''))\\)\n\nWhat it is: This is the entropy of the action distribution output by the Actor for state \\(s_m''\\). It measures the ‚Äúrandomness‚Äù or ‚Äúpredictability‚Äù of the policy. High entropy means the policy is more exploratory (less confident in a single action), low entropy means it‚Äôs more deterministic.\nSource: It‚Äôs calculated directly from the action distribution output by the Actor network.\nPurpose in Loss: This is a regularization term. It‚Äôs not about ‚Äúimproving a prediction‚Äù in the same way as the other terms. Instead, we typically want to maximize entropy (hence the negative sign in the loss formulation) to encourage exploration and prevent the policy from collapsing to a single action too quickly. This helps the agent continue to discover better strategies.\n\n\n\n\n\nHow the Loss Helps Learn \\(\\psi_{actor}\\)\nThe Actor loss, when minimized, works as follows:\n\nPolicy Gradient: The core term \\(A_m^{\\text{GAE}(\\gamma,\\lambda)} \\cdot \\log p_{\\psi_{actor}}(a_m''|s_m'')\\) is the standard policy gradient component. When we minimize the negative of this term:\n\nIf \\(A_m^{\\text{GAE}} &gt; 0\\) (good action): We want to increase \\(\\log p_{\\psi_{actor}}(a_m''|s_m'')\\), meaning we increase the probability of taking action \\(a_m''\\) in state \\(s_m''\\).\nIf \\(A_m^{\\text{GAE}} &lt; 0\\) (bad action): We want to decrease \\(\\log p_{\\psi_{actor}}(a_m''|s_m'')\\), meaning we decrease the probability of taking action \\(a_m''\\) in state \\(s_m''\\).\nThe magnitude of the advantage dictates the strength of this probability adjustment.\n\nEntropy Regularization: The \\(-\\beta_H \\cdot H(\\dots)\\) term ensures that even while the policy is being pushed towards high-advantage actions, it doesn‚Äôt become overly deterministic. It retains some level of randomness, which is beneficial for continued exploration."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "‡ªí(‚äô·¥ó‚äô)‡•≠‚úé‚ñ§",
    "section": "",
    "text": "Functions of a Random Variable\n\n\n\n\n\n\n\n\nFeb 18, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Variables\n\n\n\n\n\n\n\n\nFeb 13, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Modelling\n\n\n\n\n\n\n\n\nAug 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSurface Analysis\n\n\n\n\n\n\n\n\nAug 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPolygonal Mesh\n\n\n\n\n\n\n\n\nJul 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Autoencoder to Dreamer\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Control to Model-based Learning\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process & Bayesian Optimization\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nModel-free Reinforcement Learning\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\nNo matching items"
  }
]