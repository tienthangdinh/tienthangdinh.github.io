[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "໒(⊙ᴗ⊙)७✎▤",
    "section": "",
    "text": "From Control to Model-based Learning\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process & Bayesian Optimization\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nModel-free Reinforcement Learning\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/control/index.html",
    "href": "posts/control/index.html",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "A PID controller is a feedback control loop that continuously calculates an “error” value \\(e(t)\\) as the difference between a desired setpoint \\(r(t)\\) and a measured process variable \\(y(t)\\):\n\\[e(t) = r(t) - y(t)\\]\nBased on this error, the PID controller generates a control output \\(u(t)\\) by combining three distinct terms:\n\nProportional Term (\\(P\\)-term): Accounts for the current error.\nIntegral Term (\\(I\\)-term): Accounts for the accumulation of past errors.\nDerivative Term (\\(D\\)-term): Accounts for the rate of change of the error.\n\nCombining these, the continuous-time PID control law is given by:\n\\[u(t) = K_p e(t) + K_i \\int e(t) dt + K_d \\frac{de(t)}{dt}\\]\nWhere:\n\n\\(u(t)\\) is the controller’s output.\n\\(e(t)\\) is the error at time \\(t\\).\n\\(K_p\\) is the proportional gain.\n\\(K_i\\) is the integral gain.\n\\(K_d\\) is the derivative gain.\n\n\n\n\n\n\n\\[u_P(t) = K_p e(t)\\]\n\nPresent: If the error is large, the controller acts strongly; if the error is small, it acts weakly. =&gt; quickly drive the system towards the setpoint.\nGradual Loss Problem: There are some system where the output just naturally decays over time (like heat loss from a room, or friction in a motor). A simplified model could be, notice that for the output \\(y\\) to be maintained at a constant setpoint \\(r\\) (i.e., \\(\\frac{dy}{dt}=0\\)), the control input \\(u\\) must provide a continuous effort to compensate: \\(u_{required} = \\frac{ay}{b}\\). This system is the core problem:\n\\[\\frac{dy}{dt} = -ay + bu\\]\nSo what we require is that for this type of model to be at steady state, \\(\\frac{dy}{dt}=0 \\implies ay_{ss} = bu_{ss}\\).\nBut the problem is here!!! With P-control, we can only have \\(u_{ss} = K_p (r - y_{ss})\\) that kinda only acts based on the last timestep error. \\[ay_{ss} = bK_p (r - y_{ss})\\] \\[ay_{ss} = bK_pr - bK_py_{ss}\\] \\[y_{ss} = \\frac{K_p r}{(K_p + a/b)}\\]\nSince \\(a,b,K_p\\) are positive, \\(y_{ss}\\) will always be less than \\(r\\), meaning there will always be a non-zero steady-state error: \\(e_{ss} = r - y_{ss} \\ne 0\\).\nTherefore In this time of time-decaying systems, a sole P-Term simply cannot provide a sustained, non-zero output, simply because it only acts with the current error, and never act for the upcoming decay\n\n\n\n\n\\[u_I(t) = K_i \\int e(t) dt\\]\n\nCompensate the gradual Loss: Probably now you know what to do… we push a little more, in such time-decaying system, we need continuous effort to to maintain the setpoint. It does this by continuously accumulating errors over time.\nAnalogy: You’re driving at 99 km/h when the limit is 100 km/h (small error). The P-term might give only a tiny gas pedal press. But you know, car on the street is exactly this type of time-decaying system (\\(\\frac{dy}{dt} = -ay + bu\\)) =&gt; To maintain 99 km/h for a long time, the I-term “notices” this persistent deficit and gradually pushes the gas pedal a little harder and holds it there until you finally reach 100 km/h.\nDrawback: The integral term can make the system slower to respond and potentially cause overshoot or oscillations if its gain \\(K_i\\) is set too high, because it’s reacting to past errors, not current or future ones.\nWhen does this accumulated stop? I would say almost never, because we have a time-decaying system, so we always need it.\nBUT, of course sometimes we want do stop overshooting it, therefore we have another term down here…\n\n\n\n\n\\[u_D(t) = K_d \\frac{de(t)}{dt}\\]\n\nAnticipation and Damping :This is really nice.\n\nIf the error is rapidly increasing (either negative or positive quantitatively), the D-term will counteract it quickly.\nIf the error is rapidly decreasing (meaning the system is approaching the setpoint quickly), the D-term will reduce the control action to prevent overshoot.\n\nAnalogy: You see a sharp turn (error changing rapidly) approaching in your car. You start braking before the turn to slow down smoothly and avoid overshooting the curve. Or, you’re speeding towards the 100 km/h limit; as you get closer, the D-term will gradually ease off the gas, preventing you from overshooting.\nBenefits: Reduces overshoot, reduces oscillations, and improves the transient response (how quickly and smoothly the system reaches the setpoint).\nDrawback: The D-term is very sensitive to noise in the measurement signal. Rapid changes in noisy signals can lead to large, jerky control actions."
  },
  {
    "objectID": "posts/control/index.html#mathematical-formulation",
    "href": "posts/control/index.html#mathematical-formulation",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "A PID controller is a feedback control loop that continuously calculates an “error” value \\(e(t)\\) as the difference between a desired setpoint \\(r(t)\\) and a measured process variable \\(y(t)\\):\n\\[e(t) = r(t) - y(t)\\]\nBased on this error, the PID controller generates a control output \\(u(t)\\) by combining three distinct terms:\n\nProportional Term (\\(P\\)-term): Accounts for the current error.\nIntegral Term (\\(I\\)-term): Accounts for the accumulation of past errors.\nDerivative Term (\\(D\\)-term): Accounts for the rate of change of the error.\n\nCombining these, the continuous-time PID control law is given by:\n\\[u(t) = K_p e(t) + K_i \\int e(t) dt + K_d \\frac{de(t)}{dt}\\]\nWhere:\n\n\\(u(t)\\) is the controller’s output.\n\\(e(t)\\) is the error at time \\(t\\).\n\\(K_p\\) is the proportional gain.\n\\(K_i\\) is the integral gain.\n\\(K_d\\) is the derivative gain."
  },
  {
    "objectID": "posts/control/index.html#but-why-p-i-and-d-why-not-just-use-the-current-error",
    "href": "posts/control/index.html#but-why-p-i-and-d-why-not-just-use-the-current-error",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "\\[u_P(t) = K_p e(t)\\]\n\nPresent: If the error is large, the controller acts strongly; if the error is small, it acts weakly. =&gt; quickly drive the system towards the setpoint.\nGradual Loss Problem: There are some system where the output just naturally decays over time (like heat loss from a room, or friction in a motor). A simplified model could be, notice that for the output \\(y\\) to be maintained at a constant setpoint \\(r\\) (i.e., \\(\\frac{dy}{dt}=0\\)), the control input \\(u\\) must provide a continuous effort to compensate: \\(u_{required} = \\frac{ay}{b}\\). This system is the core problem:\n\\[\\frac{dy}{dt} = -ay + bu\\]\nSo what we require is that for this type of model to be at steady state, \\(\\frac{dy}{dt}=0 \\implies ay_{ss} = bu_{ss}\\).\nBut the problem is here!!! With P-control, we can only have \\(u_{ss} = K_p (r - y_{ss})\\) that kinda only acts based on the last timestep error. \\[ay_{ss} = bK_p (r - y_{ss})\\] \\[ay_{ss} = bK_pr - bK_py_{ss}\\] \\[y_{ss} = \\frac{K_p r}{(K_p + a/b)}\\]\nSince \\(a,b,K_p\\) are positive, \\(y_{ss}\\) will always be less than \\(r\\), meaning there will always be a non-zero steady-state error: \\(e_{ss} = r - y_{ss} \\ne 0\\).\nTherefore In this time of time-decaying systems, a sole P-Term simply cannot provide a sustained, non-zero output, simply because it only acts with the current error, and never act for the upcoming decay\n\n\n\n\n\\[u_I(t) = K_i \\int e(t) dt\\]\n\nCompensate the gradual Loss: Probably now you know what to do… we push a little more, in such time-decaying system, we need continuous effort to to maintain the setpoint. It does this by continuously accumulating errors over time.\nAnalogy: You’re driving at 99 km/h when the limit is 100 km/h (small error). The P-term might give only a tiny gas pedal press. But you know, car on the street is exactly this type of time-decaying system (\\(\\frac{dy}{dt} = -ay + bu\\)) =&gt; To maintain 99 km/h for a long time, the I-term “notices” this persistent deficit and gradually pushes the gas pedal a little harder and holds it there until you finally reach 100 km/h.\nDrawback: The integral term can make the system slower to respond and potentially cause overshoot or oscillations if its gain \\(K_i\\) is set too high, because it’s reacting to past errors, not current or future ones.\nWhen does this accumulated stop? I would say almost never, because we have a time-decaying system, so we always need it.\nBUT, of course sometimes we want do stop overshooting it, therefore we have another term down here…\n\n\n\n\n\\[u_D(t) = K_d \\frac{de(t)}{dt}\\]\n\nAnticipation and Damping :This is really nice.\n\nIf the error is rapidly increasing (either negative or positive quantitatively), the D-term will counteract it quickly.\nIf the error is rapidly decreasing (meaning the system is approaching the setpoint quickly), the D-term will reduce the control action to prevent overshoot.\n\nAnalogy: You see a sharp turn (error changing rapidly) approaching in your car. You start braking before the turn to slow down smoothly and avoid overshooting the curve. Or, you’re speeding towards the 100 km/h limit; as you get closer, the D-term will gradually ease off the gas, preventing you from overshooting.\nBenefits: Reduces overshoot, reduces oscillations, and improves the transient response (how quickly and smoothly the system reaches the setpoint).\nDrawback: The D-term is very sensitive to noise in the measurement signal. Rapid changes in noisy signals can lead to large, jerky control actions."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html",
    "href": "posts/reinforcement-learning/index.html",
    "title": "Model-free Reinforcement Learning",
    "section": "",
    "text": "An agent’s interaction with the environment is usually modeled as a Markov Decision Process (MDP):\n\\(s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2,...\\)\nwhere:\n\n\\(t \\in \\{0, 1, 2, \\dots\\}\\)\n\\(s \\in \\mathcal{S}\\): a state\n\\(a \\in \\mathcal{A}(s)\\): an action available in state \\(s\\)\n\\(r \\in \\mathcal{R} \\subseteq \\mathbb{R}\\): a scalar reward\n\nThe environment dynamics (transition model) are given by:\n\\[\np(s', r \\mid s, a) = \\text{Prob}(S_{t+1} = s',\\ R_{t+1} = r \\mid S_t = s,\\ A_t = a)\n\\]\n\n\n\nThe probability of the next state depends only on the current state and action — not the full history: \\[\nP(s_{t+1} \\mid s_t, a_t) = P(s_{t+1} \\mid s_1, a_1, ..., s_t, a_t)\n\\]\n\n\n\n\n\nStochastic: \\(\\pi(a \\mid s)\\)\nDeterministic: \\(a = \\pi(s)\\)\n\n\n\n\nThe return \\(G_t\\) is the total discounted reward from time \\(t+1\\) to final time \\(T\\):\n\\[\nG_t = \\sum_{k = t+1}^{T} \\gamma^{k - t - 1} R_k\n\\]\nExpanded:\n\\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\]\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{T - t - 1} R_T\n\\]\n\n\n\n\\[\n\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\n\\]\nHow can we determine a policy that accumulates a high reward?"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#value-functions",
    "href": "posts/reinforcement-learning/index.html#value-functions",
    "title": "Model-free Reinforcement Learning",
    "section": "Value Functions",
    "text": "Value Functions\nVery similar to return \\(\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\\).\nValue functions are used to estimate expected returns:\n\nState-value function: \\(V_\\pi(s_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\)\nAction-value function: \\(Q_\\pi(s_t, a_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\nRelationship: \\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) and \\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)\nDerivation of this relationship:\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) (semantically true because)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} \\mid s_t = s, a_t = a]\\) (because \\(\\mathbb{E}[G_{t+1} \\mid s, a] = \\mathbb{E}[V_\\pi(S_{t+1}) \\mid s, a]\\))\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a)[r + \\gamma V_\\pi(s')]\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "href": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "title": "Model-free Reinforcement Learning",
    "section": "The Bellman Equations",
    "text": "The Bellman Equations\nFor any policy \\(\\pi\\), all \\(s \\in \\mathcal{S}\\), and all \\(a \\in \\mathcal{A}(s)\\):\nState-value function:\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\, q_\\pi(s, a) \\tag{1}\n\\]\nAction-value function:\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right] \\tag{2}\n\\]\nAfter substitution as derived above:\n\nBellman Equation for \\(v_\\pi(s)\\)\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]\n\n\nBellman Equation for \\(q_\\pi(s, a)\\)\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a' \\in \\mathcal{A}(s')} \\pi(a' \\mid s') q_\\pi(s', a') \\right]\n\\]\n\n\nOptimal Bellman\n\\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "href": "posts/reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "title": "Model-free Reinforcement Learning",
    "section": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement",
    "text": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement\nUsing the Bellman Equation above and an initial estimate (e.g. all states are randomly initialized or uniformly negative, except the terminal state which is 0), we iteratively update:\nWe apply the Bellman equations above for every single state (for V), or (state, action) pair (for Q, which forms a finer table). Over many runs, it will slowly converge. \\[\n\\pi'(s) = \\arg\\max_a q_\\pi(s, a)\n\\] Where: \\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#monte-carlo-state-value",
    "href": "posts/reinforcement-learning/index.html#monte-carlo-state-value",
    "title": "Model-free Reinforcement Learning",
    "section": "Monte Carlo (State Value)",
    "text": "Monte Carlo (State Value)\nGoal: Given samples under \\(\\pi\\), estimate \\(q_\\pi\\).\n\nWe can express \\(q_\\pi\\)-estimation as \\(v_\\pi\\)-estimation. Imagine a new problem where: \\[\nS_t^{\\text{new}} = (S_t, A_t)\n\\]\n\nAny evaluation algorithm estimating \\(v(s^{\\text{new}})\\) would be estimating \\(q_\\pi(s, a)\\).\nSo basically what happening is that we do not have a policy anymore, instead, policy is now a part of the environment. But why do we want to do that? Because: - This is a generalization anyway, something applied to state value function V, also applied for action value function Q. - It simplifies our analysis, reduces the problem to a simpler problem Markov Reward Process (MRP), which is MDP without actions (s1,r1,s2,r2,s3,r3,…)\nOK, but still, how to do it?\nStart with a random value function: \\(V(s) = \\frac{1}{|S|}\\)\nCollect sampling trajectories \\(M\\) trajectory samples: \\[\ns_0^m \\quad r_1^m \\quad s_1^m \\quad \\cdots \\quad s_{T_m}^m \\qquad m = 1, \\ldots, M\n\\]\nThe Goal: Use averages to approximate \\(v_\\pi(s)\\): \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s] \\approx \\frac{1}{C(s)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[s_\\tau^m = s] \\, g_\\tau^m\n\\] where: \\[\n\\mathbb{I}[s_\\tau^m = s] =\n\\begin{cases}\n1 & \\text{if } s_\\tau^m = s \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] \\[\ng_\\tau^m = R_{t+1}^m + \\gamma R_{t+2}^m + \\gamma^2 R_{t+3}^m + \\dots + \\gamma^{T - t - 1} R_T^m\n\\]\nFor every sample trajectory \\(m\\), at any step \\(\\tau\\) in that trajectory, check if the state \\(g_\\tau^m\\) of that step is the \\(s\\) we are interested in, then include its return \\(g_\\tau^m\\) in the sum, then normalize by \\(C(s)\\), the total number of times state \\(s\\) was visited.\nAt this moment I just realized that: the state will get higher return, if its nearer to the beginning of a trajectory, if u dont believe, have a look at \\(g_\\tau^m\\) again ^^.\nBtw, to calculate return \\(g_\\tau^m\\), maybe you already know, we have to calculate from the terminate state first \\(R_T^m\\), where we know if the reward \\(R_T^m\\) is 0 or 1 (reached the goal or not), then slowly trace backward with addding \\(\\gamma\\)\nAnd to make sure you understand it, \\(v_\\pi(s)\\) is just like \\(G\\), but \\(G\\) is mostly binded to the trajectory and a policy, therefore the function \\(v_\\pi(s)\\) is actually \\(G\\)!!!\nHow to get to that Goal? to apply after the \\(m\\)-th sample: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha \\left( g_t^m - V(s_t^m) \\right)\n\\]\n… then it will slowly converge to the Goal above …\nBUT, how do we extend this to update our action ?"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#monte-carlo-action-value",
    "href": "posts/reinforcement-learning/index.html#monte-carlo-action-value",
    "title": "Model-free Reinforcement Learning",
    "section": "Monte Carlo (Action Value)",
    "text": "Monte Carlo (Action Value)\nSince we also have a deterministic set of action \\(a \\in \\mathcal{A}(s)\\), therefore we can extend the state value above to action value like this, it is equivalent:\nStart also with \\(Q(s,a) = \\frac{1}{|SxA|}\\) or just simply 0\nBasically it just create a finer Q-table.\nThe Goal \\[\nQ_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a] \\approx \\frac{1}{C(s, a)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[(s,a)_\\tau^m = (s,a)] \\, g_\\tau^m\n\\]\nHow to get to that goal? \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha \\left( g_t^m - Q(s_t^m, a_t^m) \\right)\n\\]\n… Then it will slowly converge the Goal above …\nThen we just argmax over action at each state, thats how we get optimal action."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-constant-α-mc-algorithm-pi-approx-pi",
    "href": "posts/reinforcement-learning/index.html#sum-up-constant-α-mc-algorithm-pi-approx-pi",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: Constant-α MC Algorithm \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Constant-α MC Algorithm \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(\\epsilon &gt; 0\\), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some \\(\\epsilon\\)-soft policy\n\n\\(Q(s,a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S},\\ a \\in \\mathcal{A}(s)\\) (like a random Q-Table hehe)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample a trajectory under policy \\(\\pi\\):\n\\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\)\nFor (literally EACH - EVERY SINGLE) \\(t = 0, \\dots, T_m - 1\\):\n\nCompute return (the best way is just to calculate backwards then slowly add \\(\\gamma\\) like Gonkee ^^):\n\\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nUpdate Q-value:\n\\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\n\\]\n\nUpdate policy:\n\\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\)\n\nWhere \\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\) is specified as follows: \\[\na^* \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a) \\quad \\text{(ties broken arbitrarily)}\n\\]\nFor all \\(a \\in \\mathcal{A}(s_t^m)\\): (this means to balance the policy to avoid a local optimal) \\[\n\\pi(a|s_t^m) \\leftarrow\n\\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a = a^* \\\\\n\\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a \\neq a^*\n\\end{cases}\n\\]\n(\\(|\\mathcal{A}(s_t^m)| = \\text{number of actions in } \\mathcal{A}(s_t^m)\\))\nthen back to the loop For \\(m = 1, \\dots, M\\) again and again …"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#off-policy",
    "href": "posts/reinforcement-learning/index.html#off-policy",
    "title": "Model-free Reinforcement Learning",
    "section": "Off-Policy",
    "text": "Off-Policy\nThe problem is, as seen in the Blackjack example, lack of sample leads to local-optimal bias model\n=&gt; Goal: more variance\nSample a trajectory under a different policy \\(b\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\). But the rest of the algorithm stays the same.\nOK, but how to make sampling policy \\(b\\) effect the main behavior policy \\(\\pi\\)?\n\nRelationship between sampling policy \\(b\\) vs main behavior policy \\(\\pi\\)?\nWe want: \\[\nq_\\pi(s, a) = \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]\n\\]\nSampled data under \\(b\\) means this is what we actually estimated: \\[\n\\mathbb{E}_b[G_t|S_t = s, A_t = a]\n\\]\nTherefore we use Importance Sampling to bring them to \\(\\pi\\): \\[\nq_\\pi(s, a) = \\mathbb{E}_b\\left[\\frac{p_\\pi(G_t)}{p_b(G_t)}G_t|S_t = s, A_t = a\\right]\n\\] where \\(\\rho\\) is the importance sampling ratio: \\[\n\\frac{p_\\pi(G_t)}{p_b(G_t)} = \\rho = \\prod_{\\tau=t+1}^{T-1} \\frac{\\pi(A_\\tau|S_\\tau)}{b(A_\\tau|S_\\tau)}\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "href": "posts/reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(b\\) (behavior policy), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some policy\n\\(Q(s, a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\) (also the random Q-Table above hehe)\n\nFor \\(m = 1, \\dots, M\\):\nUnder \\(b\\) sample: \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\)\n\nFor \\(t = 0, \\dots, T_m - 1\\):\n\n\\(\\rho_t^m \\leftarrow \\prod_{\\tau=t+1}^{T_m-1} \\frac{\\pi(a_\\tau^m|s_\\tau^m)}{b(a_\\tau^m|s_\\tau^m)}\\) (or 1 if \\(t+1 &gt; T_m-1\\))\nCompute return: \\(g_t^m \\leftarrow \\rho_t^m(r_{t+1}^m + \\gamma r_{t+2}^m + \\dots)\\)\nUpdate Q-Value: \\(Q(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\\)\nUpdate policy: \\(\\pi(s_t^m) \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a)\\) (ties broken arbitrarily)\n\nNote that at Update Policy: we do not need the \\(\\pi\\)-greedy as above, because now using behavior-policy \\(b\\), we could already diverse out for a more global view\n\nBUT, off policy MC has too much variance, therefore the next technique … Temporal Difference\nBefore we continue, let’s see where is exactly the point of model-free MC learning:"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "href": "posts/reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "title": "Model-free Reinforcement Learning",
    "section": "n-step Temporal Difference Learning",
    "text": "n-step Temporal Difference Learning\nRecall from the MC approach: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\nwhere: \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nn-STEP TD: Replace the target, \\(g_t^m\\), with: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n V(s_{t+n}^m)\n\\]\nwhere \\(V(s_{t+n}^m)\\) is actually no different than \\(g_{t+n}^m\\), but instead of waiting for the trajectory to finish the episode then calculate backward, we just need to wait for \\(n\\) steps to BOOTSTRAPPING the existing \\(V(s_{t+n})\\) calculated from older trajectories, think a little bit, it means the same thing with \\(g_{t+n}\\) (accumulated return). If \\(n = \\infty\\): TD is identical to MC."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#why-is-td-better",
    "href": "posts/reinforcement-learning/index.html#why-is-td-better",
    "title": "Model-free Reinforcement Learning",
    "section": "Why is TD better?",
    "text": "Why is TD better?\n\nMarkov property: The RL environment has Markov property, that means the future depends only on the current state, not the history. And TD works based on that design, the update is just between neighboring states. Not like waiting for the whole trajectory like MC, \\(g_t^m\\) needs backward calculation for the whole trajectory =&gt; strongly history based. \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\n\nFor example in TD(0) the use of \\(V(s_{t+1}^m)\\) is very Markov property: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(r_{t+1}^m + \\gamma V(s_{t+1}^m) - V(s_t^m))\n\\]\n\nReduced Variance: The effect of MC is calculating accumulated result at each single trajectory, then averaging then in the end, this makes the result very specific to the sampled trajectories. But TD directly add the already smoothed \\(+ \\gamma^n V(s_{t+n}^m)\\)\nOnline-learning we all know what that means"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#what-does-larger-n-means",
    "href": "posts/reinforcement-learning/index.html#what-does-larger-n-means",
    "title": "Model-free Reinforcement Learning",
    "section": "What does larger n means?",
    "text": "What does larger n means?\nIncrease the bias = utilize more from the knowledge from this trajectory, instead of keep learning from the average.\nE.g., for a single episode with TD(8):\n\nAll states from \\(S_0\\) up to \\(S_{k-8}\\) (if \\(k \\ge 8\\)): Will be updated using a full 8-step return, bootstrapping from \\(V(S_{t+8})\\) =&gt; very average in the beginning\nThe last 7 states (\\(S_{k-7}, \\dots, S_{k-1}\\)): Will be updated using a return that effectively “runs out of steps” before 8. Their targets will be a mix of actual rewards leading to the terminal state, like Monte Carlo =&gt; direct reward in the end"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-on-policy-temporal-difference-n-step-sarsa",
    "href": "posts/reinforcement-learning/index.html#sum-up-on-policy-temporal-difference-n-step-sarsa",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: On-Policy Temporal Difference: n-step Sarsa",
    "text": "Sum up: On-Policy Temporal Difference: n-step Sarsa\nModel-free control \\(\\rightarrow\\) use \\(Q(s, a)\\), not \\(V(s)\\).\nRedefine: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n Q(s_{t+n}^m, a_{t+n}^m)\n\\]\nUpdate rule: \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_{t:t+n}^m - Q(s_t^m, a_t^m))\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning",
    "href": "posts/reinforcement-learning/index.html#q-learning",
    "title": "Model-free Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\n1-step TD Control—-target adjustment—-&gt; Q-Learning (off-policy).\nInstead of using \\(r_{t+1}^m + \\gamma Q(s_{t+1}^m, a_{t+1}^m)\\) (which is used in SARSA and relies on the next action taken by the policy), Q-Learning uses: \\[\nr_{t+1}^m + \\gamma \\max_{a} Q(s_{t+1}^m, a)\n\\]\nThe \\(\\max\\) operator means, regardless of which action the behavior policy \\(b\\) actually took, this target is formed by the best possible action from the next state \\(s_{t+1}^m\\) =&gt; Q-Learning an off-policy.\nTo describe what actually happens, it is like this: 1-step TD (SARSA-like): \\[\n\\dots s_0^m, \\underset{\\uparrow}{\\underline{a_0^m}}, r_1^m, s_1^m, \\underset{\\uparrow}{\\underline{a_1^m}}, r_2^m, s_2^m, \\underset{\\uparrow}{\\underline{a_2^m}}, r_3^m, s_3^m, \\dots\n\\] Updates for \\(Q(s_t, a_t)\\) occur after observing \\(s_{t+1}, a_{t+1}\\), using the target \\(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\) where \\(a_{t+1}\\) is the action taken by the current policy.\n1-step Q-Learning: \\[\n\\dots  \\underset{\\uparrow}{\\underline{s_0^m}},a_0^m, r_1^m, \\underset{\\uparrow}{\\underline{s_1^m}},a_1^m, r_2^m, \\underset{\\uparrow}{\\underline{s_2^m}},a_2^m, r_3^m, \\dots\n\\] Updates for \\(Q(s_t, a_t)\\) occur after observing \\(s_{t+1}\\), using the target \\(r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a)\\), where action \\(a_{t+1}\\) is observed but not used in forming the target for \\(Q(s_t, a_t)\\)."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#expected-sarsa",
    "href": "posts/reinforcement-learning/index.html#expected-sarsa",
    "title": "Model-free Reinforcement Learning",
    "section": "Expected Sarsa",
    "text": "Expected Sarsa\n1-step Q-Learning —– \\(\\max\\) operator-&gt;average operator—&gt; Expected Sarsa \\[\nr_{t+1}^m + \\gamma \\max_{a} Q(s_{t+1}^m, a)\n\\] to using an expectation over all possible actions, weighted by the policy \\(\\pi\\): \\[\nr_{t+1}^m + \\gamma \\sum_{a} Q(s_{t+1}^m, a) \\pi(a|s_{t+1}^m)\n\\]\nAs presented (when the policy \\(\\pi\\) used in the target is the same as the behavior policy generating the data), this is an on-policy method.\nBut to make it off-policy, just need \\(\\text{policy generating the trajectory} \\neq \\pi \\text{ in target}\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#compare",
    "href": "posts/reinforcement-learning/index.html#compare",
    "title": "Model-free Reinforcement Learning",
    "section": "Compare",
    "text": "Compare\n\nSarsa has longer path, because it just took the action it actually took, which was most of the time exploratory, the one with \\(\\epsilon \\text{exploration}\\) policy that balance out\nQ-Learning does not use this \\(\\epsilon \\text{exploration}\\) policy, it uses \\(\\max\\) operator\nExpected Sarsa use weighted average, so yeah, always a safe choice."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#summary-of-td",
    "href": "posts/reinforcement-learning/index.html#summary-of-td",
    "title": "Model-free Reinforcement Learning",
    "section": "Summary of TD",
    "text": "Summary of TD\nGoal of Q-Learning is updating Q-Table to optimal where: \\[\nQ_*(s_t, a_t) = \\mathbb{E} \\left[ r_t + \\gamma \\max_a Q_*(s_{t+1}, a) \\right]\n\\]\nAlso called Bellman Optimality Equation: every subsequence of a optimal sequence must also be optimal\nLearning Q-values:\n\nSARSA:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n\\]\nExpected SARSA:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\sum_a \\pi(a \\mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\nQ-Learning:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\n\nLearning V-values:\n\\[\nV(s_t) \\leftarrow V(s_t) + \\alpha \\left[ r_t + \\gamma V(s_{t+1}) - V(s_t) \\right]\n\\]\nWhere \\(\\alpha\\) is the learning rate."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#on-policy-evaluation-with-function-approximation",
    "href": "posts/reinforcement-learning/index.html#on-policy-evaluation-with-function-approximation",
    "title": "Model-free Reinforcement Learning",
    "section": "On-Policy Evaluation with Function Approximation",
    "text": "On-Policy Evaluation with Function Approximation\nGoal remains to approximate the state-value function \\(v_\\pi(s)\\). Data generated from a given fixed policy \\(\\pi\\).\nWe now learn a parameterized function \\(\\hat{v}(s, \\mathbf{w})\\), where:\n\n\\(\\mathbf{w} \\in \\mathbb{R}^d\\) is a vector of parameters\na state \\(s\\) as input\n\nWe learn \\(w\\) and hope that \\(v_\\pi(s) \\approx \\hat{v}(s, \\mathbf{w})\\)\nSince \\(d \\ll |\\mathcal{S}|\\), any change to \\(\\mathbf{w}\\) can simultaneously change \\(\\hat{v}(s, \\mathbf{w})\\) for many (or all) states \\(s\\). Different from tabular methods, where an update to \\(V(s)\\) for a specific state \\(s\\) affects only that state’s value."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-represent-mathbfxs",
    "href": "posts/reinforcement-learning/index.html#how-to-represent-mathbfxs",
    "title": "Model-free Reinforcement Learning",
    "section": "How to represent \\(\\mathbf{x}(s)\\)?",
    "text": "How to represent \\(\\mathbf{x}(s)\\)?\nExample 1: two simple features for given image state \\(s\\):\n\n\\(x_1(s)\\): The average of all pixel values in the image.\n\\(x_2(s)\\): The standard deviation of all pixel values in the image.\n\n=&gt; feature vector \\(\\mathbf{x}(s) = \\begin{bmatrix} x_1(s) \\\\ x_2(s) \\end{bmatrix}\\)\nWith these features, we can construct a linear value function to approximate \\(v_\\pi(s)\\): \\[\n\\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s)^T \\mathbf{w}\n\\]\nExample 2: Mountain Car below with a dynamic model of velocity \\(dx\\) and position \\(x\\)\nExample 3: Proto Points and Radius Basis Function will be discussed in next chapter Policy Gradient Method"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#goal-how-to-get-mathbfw",
    "href": "posts/reinforcement-learning/index.html#goal-how-to-get-mathbfw",
    "title": "Model-free Reinforcement Learning",
    "section": "Goal: How to get \\(\\mathbf{w}\\)?",
    "text": "Goal: How to get \\(\\mathbf{w}\\)?\nThe ‘best’ \\(\\mathbf{w}\\) minimizes: \\[\n\\overline{VE}(\\mathbf{w}) = \\sum_{s \\in \\mathcal{S}} \\mu(s)[v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]^2\n\\] where \\(\\mu(\\cdot)\\) is a distribution over states (frequency of visiting each state).\n\nWe observe a surrogate for \\(v_\\pi(S_t)\\): \\(U_t\\): Since we don’t know \\(v_\\pi(S_t)\\) exactly, we use a sample-based estimate or target, \\(U_t\\), as a stand-in. This \\(U_t\\) could be the Monte Carlo return \\(G_t\\), or an n-step TD target \\(g_{t:t+n}\\), or a 1-step TD target \\((R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}))\\).\n\nUpdate Rule we don’t have direct access to \\(v_\\pi(s)\\) for all states =&gt; Stochastic Gradient Descent (SGD) for updating our parameters \\(\\mathbf{w}\\): \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\] Where:\n\n\\(\\alpha\\) is the learning rate (step size).\n\\(\\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right]\\) is the TD error (or prediction error) based on our sample \\(U_t\\).\n\\(\\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\\) is the gradient of our estimated value function with respect to its parameters \\(\\mathbf{w}\\), evaluated at state \\(S_t\\). This gradient tells us how to adjust \\(\\mathbf{w}\\) to change \\(\\hat{v}(S_t, \\mathbf{w})\\) in the desired direction."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-obtain-the-target-u_t",
    "href": "posts/reinforcement-learning/index.html#how-to-obtain-the-target-u_t",
    "title": "Model-free Reinforcement Learning",
    "section": "How to Obtain the Target \\(U_t\\)",
    "text": "How to Obtain the Target \\(U_t\\)\nIn the Stochastic Gradient Descent update rule: \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\] The term \\(U_t\\) serves as our sample-based target for the true value \\(v_\\pi(S_t)\\). Since \\(v_\\pi(S_t)\\) is unknown, we must derive \\(U_t\\) from our observed experience. The choice of \\(U_t\\) determines whether our method leans towards Monte Carlo or Temporal Difference approaches:\n1. Monte Carlo Target: If the target \\(U_t\\) is the full Monte Carlo return from state \\(S_t\\) to the end of the episode, then we are using a Gradient Monte Carlo method: \\[\nU_t = G_t\n\\] where \\(G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1}R_T\\) is the total discounted return observed from time step \\(t\\) until the terminal state \\(T\\).\n\nCharacteristics:\n\nUnbiased: If \\(G_t\\) is an unbiased estimate of \\(v_\\pi(S_t)\\) (which it is, on average), then using it as \\(U_t\\) can lead to the parameters \\(\\mathbf{w}\\) converging to a local optimum of the Mean Squared Value Error (\\(\\overline{VE}(\\mathbf{w})\\)).\nHigh Variance: \\(G_t\\) can be noisy due to the sum of many random rewards.\nRequires complete episodes: We must wait until the episode ends to compute \\(G_t\\).\n\n\n2. Temporal Difference (TD) Target: If the target \\(U_t\\) is derived using bootstrapping (i.e., using an estimate of the value of a future state), then we are using a Semi-Gradient TD method. The most common is the 1-step TD target: \\[\nU_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\n\\] Here, \\(R_{t+1}\\) is the actual reward observed, and \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\) is our current estimate of the value of the next state \\(S_{t+1}\\). For this specific update, \\(\\mathbf{w}\\) in \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\) is usually the online network’s weights, not a target network’s weights in this basic formulation.\n\nCharacteristics:\n\nBiased: especially in the beginning with crappy initialized value \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\).\nLower Variance: It typically has lower variance than the Monte Carlo target because it depends on only one actual reward and then uses a smoothed estimate for the rest of the future.\nOnline Learning: Updates can be performed after each single time step, without waiting for the end of an episode.\nSemi-Gradient: Since \\(U_t\\) depends on \\(\\mathbf{w}\\), our update rule is not a true gradient step. The gradient \\(\\nabla_{\\mathbf{w}} L(\\mathbf{w})\\) for the loss \\(\\left( (R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})) - \\hat{v}(S_t, \\mathbf{w}) \\right)^2\\) would actually involve the derivative of \\(U_t\\) (\\(\\hat{v}(S_{t+1}, \\mathbf{w})\\)) with respect to \\(\\mathbf{w}\\). =&gt; semi-gradient means: \\(\\nabla_{\\mathbf{w}} \\hat{v}(S_{t+1}, \\mathbf{w})\\) is omitted for simplicity and stability, only taking \\(\\nabla_{\\mathbf{w}} \\hat{v}(S_{t}, \\mathbf{w})\\).\nNo Guarantee of Convergence (to Global Optimum): Because it’s not a true gradient of the overall \\(\\overline{VE}(\\mathbf{w})\\), we generally don’t guarantee convergence to the global optimum of the Mean Squared Value Error, even if the optimal \\(\\mathbf{w}\\) is unique. However, for linear function approximation, it can still converge to a local optimum. For non-linear approximators like neural networks, theoretical guarantees are weaker, but these methods still perform very well in practice.\n\n\nThe update rule remains: \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation",
    "href": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation",
    "title": "Model-free Reinforcement Learning",
    "section": "Extension to On-Policy Control with Function Approximation",
    "text": "Extension to On-Policy Control with Function Approximation\nSo far is just policy evaluation (approximating \\(v_\\pi(s)\\)). Now extend directly to control problems (finding an optimal policy), typically by approximating the action-value function \\(q_\\pi(s,a)\\) or \\(q_*(s,a)\\). \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{q}(S_t, A_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})\n\\] Here, \\(\\hat{q}(S_t, A_t, \\mathbf{w})\\) is our function approximator’s estimate of the action-value for the state-action pair \\((S_t, A_t)\\) using parameters \\(\\mathbf{w}\\). The term \\(\\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})\\) is the gradient of this estimate with respect to \\(\\mathbf{w}\\).\nFor Semi-gradient 1-step Sarsa, the target \\(U_t\\) is defined as: \\[\nU_t = R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\mathbf{w})\n\\]\nSince both the action being evaluated (\\(A_t\\)) and the action used to construct the target (\\(A_{t+1}\\)) are chosen according to the same behavior policy (which is actively being improved based on \\(\\hat{q}\\)), this method is on-policy. The agent learns the value of the policy it is currently following, and this policy itself is improved through action selection methods (e.g., \\(\\epsilon\\)-greedy) based on the learned \\(\\hat{q}\\) values.\nExample: Linear Action-Value Function for the Mountain Car Task\nTo illustrate how function approximation can be used for action-value functions, let’s consider a scenario like The Mountain Car Task. We approximate the action-value function \\(\\hat{q}(s, a, \\mathbf{w})\\) using a linear function approximator instead of a Q-Table: \\[\n\\hat{q}(s, a, \\mathbf{w}) = \\begin{cases}\n    \\mathbf{w}_{-1}^T \\mathbf{x}(s) & \\text{if } a = -1 \\\\\n    \\mathbf{w}_{0}^T \\mathbf{x}(s) & \\text{if } a = 0 \\\\\n    \\mathbf{w}_{1}^T \\mathbf{x}(s) & \\text{if } a = 1\n\\end{cases}\n\\] Where:\n\naction \\(a\\) (-1, 0 -1)\n\\(\\mathbf{x}(s)\\) is the feature representation of the state \\(s\\) (length 120 decoded from position & velocity)\n\\(\\mathbf{w}_{-1}\\), \\(\\mathbf{w}_{0}\\), and \\(\\mathbf{w}_{1}\\) are distinct weight vectors, each corresponding to one of the possible actions.\nThe overall parameter vector \\(\\mathbf{w}\\) for the entire function approximator would be the concatenation of these individual action-specific weight vectors (i.e., \\(\\mathbf{w} = [\\mathbf{w}_{-1}, \\mathbf{w}_{0}, \\mathbf{w}_{1}]\\))."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation-1",
    "href": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation-1",
    "title": "Model-free Reinforcement Learning",
    "section": "Extension to On-Policy Control with Function Approximation",
    "text": "Extension to On-Policy Control with Function Approximation\nWe all know what that is, just that when we combine three things together:\n\noff-policy\nfunction approximation\nbootstrapping\n\nwe will have problem with convergence, which may be solved by the next topic …"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-represent-xs",
    "href": "posts/reinforcement-learning/index.html#how-to-represent-xs",
    "title": "Model-free Reinforcement Learning",
    "section": "How to represent \\(x(s)\\)?",
    "text": "How to represent \\(x(s)\\)?\n\nProto Points and Radial Basis Functions (RBFs)\ninstead of having a neural network directly compute an output, the state is first transformed into a set of features by measuring its “similarity” or “proximity” to a predefined set of “proto points” or “centers.”\n\nDefine Proto Points (Centers):\nFirst, you select a number of “proto points” or “centers” in your state space. Let’s call these centers \\(c_1, c_2, \\dots, c_k\\). These centers are essentially fixed, representative points in your environment’s state space. They could be chosen: manually or randomly\n\n\nDefine Basis Functions (e.g., Radial Basis Functions):\nFor each center \\(c_j\\), you define a basis function \\(\\phi_j(s)\\). A common choice is a Gaussian Radial Basis Function: \\[\n\\phi_j(s) = \\exp \\left( -\\frac{\\|s - c_j\\|^2}{2\\sigma_j^2} \\right)\n\\]\nwhere:\n\n\\(\\|s - c_j\\|^2\\) is the squared Euclidean distance between the current state \\(s\\) and the center \\(c_j\\).\n\\(\\sigma_j^2\\) is a variance or width parameter for that basis function, controlling how broad its “influence” is.\n\nWhat does \\(\\phi_j(s)\\) mean? It’s a measure of how “similar” or “close” the current state \\(s\\) is to the center \\(c_j\\). It peaks at 1 when \\(s = c_j\\) and decays to 0 as \\(s\\) moves away from \\(c_j\\).\n\n\nConstruct the Feature Vector:\nFor any given state \\(s\\), you compute the value of each basis function: \\[\n\\mathbf{x}(s) = \\begin{bmatrix} \\phi_1(s) \\\\ \\phi_2(s) \\\\ \\vdots \\\\ \\phi_k(s) \\end{bmatrix}\n\\] This \\(\\mathbf{x}(s)\\) is your feature vector, where each feature represents the activation of a “proto point.”\n\n\nLinear Function Approximation:\nNow, these features are used in a linear function approximator. The parameters you learn are the weights associated with each of these basis functions.\n\nFor a value function: \\[\n  \\hat{V}(s, \\mathbf{w}) = \\sum_{j=1}^k w_j \\phi_j(s) = \\mathbf{w}^T \\mathbf{x}(s)\n  \\] Here, \\(\\mathbf{w}\\) is the vector \\([w_1, \\dots, w_k]\\), and \\(w_j\\) is the weight for the \\(j\\)-th proto point’s influence.\nFor a policy (e.g., logits in a softmax): For each action \\(a\\), you’d have a separate weight vector \\(\\mathbf{\\theta}_a\\), and the logits for the policy could be: \\[\n  h(s,a,\\mathbf{\\theta}_a) = \\mathbf{\\theta}_a^T \\mathbf{x}(s)\n  \\] Then, \\(\\pi(a|s,\\mathbf{\\theta}) = \\text{softmax}(h(s,a,\\mathbf{\\theta}_a))\\).\n\n\n\n\nExample: Policy Gradient with Proto Points (RBFs) for Mountain Car\nImagine our Mountain Car environment again. The state \\(s\\) is (position, velocity), which are continuous. We want to learn a policy \\(\\pi(a|s, \\mathbf{\\theta})\\) directly.\n1. Define Proto Points (Centers) in State Space:\nLet’s say we define \\(K=4\\) proto points (centers) \\(c_j\\) in our 2D (position, velocity) state space. For simplicity, let’s pick them:\n\n\\(c_1 = (-0.5, 0.0)\\) (Mid-left, still)\n\\(c_2 = (0.0, 0.0)\\) (Center, still)\n\\(c_3 = (0.5, 0.0)\\) (Mid-right, still)\n\\(c_4 = (-0.2, 0.05)\\) (Slightly left, moving right)\n\nWe also define a width \\(\\sigma^2\\) for all RBFs (or separate \\(\\sigma_j^2\\) values).\n2. Create the State Feature Vector \\(\\mathbf{x}(s)\\) using RBFs:\nFor any state \\(s = (\\text{pos}, \\text{vel})\\), we calculate its similarity to each of these 4 proto points using a Gaussian RBF. Our feature vector \\(\\mathbf{x}(s)\\) will have 4 dimensions:\n\\[\n\\mathbf{x}(s) = \\begin{bmatrix}\n\\phi_1(s) = \\exp \\left( -\\frac{\\|s - c_1\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_2(s) = \\exp \\left( -\\frac{\\|s - c_2\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_3(s) = \\exp \\left( -\\frac{\\|s - c_3\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_4(s) = \\exp \\left( -\\frac{\\|s - c_4\\|^2}{2\\sigma^2} \\right)\n\\end{bmatrix}\n\\] This \\(\\mathbf{x}(s)\\) now serves as our “meaningful” representation of the state, telling us how similar \\(s\\) is to certain key points in the environment.\n3. Define the Policy \\(\\pi(a|s, \\mathbf{\\theta})\\) using these Features:\nFor each action \\(a \\in \\{-1, 0, 1\\}\\), we define a linear combination of these features to get a “score” or “logit” for that action.\nLet \\(\\mathbf{\\theta}_{-1}\\), \\(\\mathbf{\\theta}_{0}\\), \\(\\mathbf{\\theta}_{1}\\) be our learnable parameter vectors (weights), each of size \\(K=4\\). The total policy parameters \\(\\mathbf{\\theta}\\) would be the concatenation of these three vectors.\nThe logits for each action are: * \\(h(s, a=-1, \\mathbf{\\theta}) = \\mathbf{\\theta}_{-1}^T \\mathbf{x}(s)\\) * \\(h(s, a=0, \\mathbf{\\theta}) = \\mathbf{\\theta}_{0}^T \\mathbf{x}(s)\\) * \\(h(s, a=1, \\mathbf{\\theta}) = \\mathbf{\\theta}_{1}^T \\mathbf{x}(s)\\)\nAnd the policy probabilities are then given by the softmax function: \\[\\pi(a|s, \\mathbf{\\theta}) = \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_{a'} e^{h(s,a',\\mathbf{\\theta})}}\\]\n4. Update the Policy Parameters (\\(\\mathbf{\\theta}\\)) using Policy Gradients (REINFORCE):\nWhen we collect a trajectory and compute returns \\(G_t\\), the REINFORCE update rule applies: \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha G_t \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t|s_t, \\mathbf{\\theta})\\]\nLet’s look at a specific component of this update, say for \\(\\mathbf{\\theta}_{-1}\\): \\[\\mathbf{\\theta}_{-1} \\leftarrow \\mathbf{\\theta}_{-1} + \\alpha G_t \\nabla_{\\mathbf{\\theta}_{-1}} \\ln \\pi(a_t|s_t, \\mathbf{\\theta})\\]\nIf \\(a_t = -1\\), and \\(G_t\\) is high, the gradient \\(\\nabla_{\\mathbf{\\theta}_{-1}} \\ln \\pi(-1|s_t, \\mathbf{\\theta})\\) will push \\(\\mathbf{\\theta}_{-1}\\) to increase the score for action -1 when in state \\(s_t\\). Since \\(h(s, a, \\mathbf{\\theta}_a) = \\mathbf{\\theta}_a^T \\mathbf{x}(s)\\), the gradient with respect to \\(\\mathbf{\\theta}_a\\) is simply \\(\\mathbf{x}(s)\\). So, the update to \\(\\mathbf{\\theta}_{-1}\\) will be proportional to \\(\\mathbf{x}(s_t)\\). This means:\n\nIf \\(s_t\\) is very similar to \\(c_1\\) (so \\(\\phi_1(s_t)\\) is high), then \\(w_{-1,1}\\) (the weight for \\(c_1\\) and action -1) will be adjusted significantly.\nIf \\(s_t\\) is far from \\(c_1\\) (so \\(\\phi_1(s_t)\\) is near zero), then \\(w_{-1,1}\\) will be adjusted very little by this particular sample.\n\nThis means the learning process adjusts the weights for each proto point for each action based on the returns received.\n\n\nAre Probabilities Fixed When Starting with Proto Points?\nNo, the probabilities of actions are not fixed when you start with proto points.\n\nFixed: The proto points \\(c_j\\) themselves are fixed in the state space, and the basis functions \\(\\phi_j(s)\\) are fixed (their shape and location are determined at the start).\nLearned: However, the parameters \\(\\mathbf{\\theta}\\) (the weights associated with each proto point for each action) are learnable.\n\nThe idea of relying action parameter \\(\\theta\\) on fixed state representation \\(x(s)\\) is because action \\(a\\) depends on the state \\((a|s)\\), so we need to fix states representation first, use it as a basis so that action parameter can learn."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#a-monte-carlo-style-policy-gradient-algorithm",
    "href": "posts/reinforcement-learning/index.html#a-monte-carlo-style-policy-gradient-algorithm",
    "title": "Model-free Reinforcement Learning",
    "section": "A Monte Carlo Style Policy Gradient Algorithm",
    "text": "A Monte Carlo Style Policy Gradient Algorithm\nThe core idea is to increase the probability of actions that lead to high returns and decrease the probability of actions that lead to low returns.\nInitialize:\n\nFunctional form for the policy: \\(\\pi(a|s, \\mathbf{\\theta})\\) (e.g., a neural network that outputs action probabilities given a state, parameterized by \\(\\mathbf{\\theta}\\)).\nInitial parameters: \\(\\mathbf{\\theta}\\) (e.g., randomly initialized weights for a neural network).\nStep size (learning rate): \\(\\alpha\\)\n\nAlgorithm:\nFor \\(m = 1, \\dots, M\\) (for each episode):\n\nSample a trajectory under the current policy \\(\\pi(\\cdot|\\cdot, \\mathbf{\\theta})\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\) (where \\(T_m\\) is the length of the episode).\nFor \\(t = 0, \\dots, T_m - 1\\) (for each time step in the trajectory):\n\nCompute the return from time \\(t\\): \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{T_m - t - 1} r_{T_m}^m\\) (This is the total discounted reward from \\(t+1\\) until the end of the episode).\nUpdate the policy parameters: \\[\n  \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t g_t^m \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\n  \\]\n\n\nExplanation of the Update Rule:\n\n\\(\\alpha\\): The learning rate, controlling the step size of the update.\n\\(\\gamma^t\\): The discount factor raised to the power of \\(t\\). This term accounts for the discounting of future rewards, ensuring that immediate rewards have a stronger influence on early actions.\n\\(g_t^m\\): Scaling stepsize with the Monte Carlo return from time step \\(t\\). e.g. If return high =&gt; big step size, if return negative =&gt; step backward\n\\(\\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\): This is the gradient of the log-probability of the action taken.\n\n\\(\\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\): The logarithm of the probability that the policy \\(\\pi\\) would choose action \\(a_t^m\\) in state \\(s_t^m\\) with current parameters \\(\\mathbf{\\theta}\\). This is very important!!! It is a normalizer for cases like a random policy \\(\\pi\\) pick a positive (but low return) action too often, leading to pushing the probability too much, so by taking the gradient of \\(ln\\) of it, it limit down it, (and also the opposite for high reward actions but the behavior policy did not pick as many time to push enough higher). These situations usually happen with initialization of that lower reward already too high, and then add up that the policy random pick it too often pushing it even further.\n\\(\\nabla_{\\mathbf{\\theta}}\\): The gradient operator with respect to the policy parameters \\(\\mathbf{\\theta}\\). This term tells us how to change \\(\\mathbf{\\theta}\\) to increase the log-probability of taking action \\(a_t^m\\) in state \\(s_t^m\\).\n\n\nIntuition:\nThe update rule essentially says: if action \\(a_t^m\\) taken in state \\(s_t^m\\) leads to a high return (\\(g_t^m\\) is large and positive), then increase the probability of taking that action in that state. If it leads to a low (or negative) return, decrease its probability. The \\(\\gamma^t\\) term ensures that actions taken earlier in the trajectory, which influence more of the subsequent rewards, are given appropriate credit.\nREINFORCE is a fundamental algorithm that demonstrates the direct optimization of a policy, laying the groundwork for more advanced policy gradient methods."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#variance-problem",
    "href": "posts/reinforcement-learning/index.html#variance-problem",
    "title": "Model-free Reinforcement Learning",
    "section": "Variance Problem",
    "text": "Variance Problem\nReturn \\(g_t^m\\) rewards the action when it is positive, and punish the action when it is negative. But some cases all actions can be positive, just some are less positive than other =&gt; it should not be encouraged.\nSolution: With baseline (e.g., \\(V_\\pi(s,w)\\)) =&gt; then we have \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t (g_t^m - b(s_t^m)) \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\] so that:\n\nif \\(g_t^m &gt; V_\\pi(s,w)\\), the scaling factor is positive =&gt; increase probability of action \\(a\\)\nif \\(g_t^m &lt; V_\\pi(s,w)\\), the scaling factor is negative =&gt; decrease probability of action \\(a\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-reinforce-gradient-policy-with-baseline",
    "href": "posts/reinforcement-learning/index.html#sum-up-reinforce-gradient-policy-with-baseline",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: REINFORCE Gradient Policy with Baseline",
    "text": "Sum up: REINFORCE Gradient Policy with Baseline\nTo specify upfront:\n\nFunctional forms \\(\\pi(a|s, \\mathbf{\\theta})\\), \\(\\hat{v}(s, \\mathbf{w})\\)\nInitial \\(\\mathbf{\\theta}, \\mathbf{w}\\)\nStep sizes \\(\\alpha^{\\theta}, \\alpha^{w}\\)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample: \\(s_0^m, a_0^m, r_1^m \\dots a_{T_m-1}^m, r_{T_m}^m\\)\nFor \\(t = 0, \\dots, T_m - 1\\): \\[g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\]\n\n\\[\\delta \\leftarrow g_t^m - \\hat{v}(s_t^m, \\mathbf{w})\\]\n\\[\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^w \\delta \\nabla_{\\mathbf{w}} \\hat{v}(s_t^m, \\mathbf{w})\\]\n\\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^{\\theta} \\gamma^t \\delta \\nabla \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\]"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html",
    "href": "posts/bayesian-optimization/index.html",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "\\(P(f | \\mathcal{D}) \\propto P(\\mathcal{D} | f) P(f)\\) is a real multiplication of probability densities. However, when dealing with functions and multivariate Gaussians, this multiplication is implicitly handled by the properties of joint and conditional Gaussian distributions.\n\n\nThe key to the GP’s tractability is that while it’s a distribution over infinite-dimensional functions, any finite collection of function values drawn from a GP jointly follow a multivariate Gaussian distribution.\nConsider our observed input points \\(\\mathbf{X}_t = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_t\\}\\) and a new test input point \\(\\mathbf{x}^*\\). Under the GP prior, the vector of true (but unobserved) function values at these points, \\(\\mathbf{f}_t = [f(\\mathbf{x}_1), \\dots, f(\\mathbf{x}_t)]^T\\) and \\(f^* = f(\\mathbf{x}^*)\\), has a joint multivariate Gaussian distribution:\n\\[\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}, \\begin{pmatrix} \\mathbf{K}_t & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix} \\right)\\]\nHere:\n\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) is the prior mean at observed points.\n\\(\\mathbf{K}_t\\) is the \\(t \\times t\\) covariance matrix of the observed points, where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_* = [k(\\mathbf{x}^*, \\mathbf{x}_1), \\dots, k(\\mathbf{x}^*, \\mathbf{x}_t)]^T\\) is the \\(t \\times 1\\) vector of covariances between the test point and observed points.\n\\(k(\\mathbf{x}^*, \\mathbf{x}^*)\\) is the prior variance at the test point itself.\n\nThis joint prior distribution for \\(\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix}\\) represents the \\(P(f)\\) term in Bayes’ Theorem. We do not know what that exactly is, but it does tell us about the plausible relationships between function values at observed and unobserved locations before we see any data.\n\n\n\nWe observe the data \\(\\mathcal{D}_t = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^t\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\). This can be written in vector form as:\n\\[\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\]\nwhere \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 \\mathbf{I})\\).\nThe likelihood \\(P(\\mathcal{D}_t | f)\\) (or more precisely, \\(P(\\mathbf{y}_t | \\mathbf{f}_t)\\)) is a Gaussian centered at the true function values \\(\\mathbf{f}_t\\):\n\\[P(\\mathbf{y}_t | \\mathbf{f}_t) = \\mathcal{N}(\\mathbf{y}_t | \\mathbf{f}_t, \\sigma_n^2 \\mathbf{I})\\]\nThis explicitly defines the \\(P(\\mathcal{D}|f)\\) term (using \\(\\mathbf{f}_t\\) as the “specific function” part for the observed data).\n\n\n\nNow, the “multiplication” that yields the posterior GP is achieved by a fundamental property of multivariate Gaussian distributions:\nIf you have two random variables (or vectors of variables) \\(A\\) and \\(B\\) that are jointly Gaussian (note that \\(\\Sigma = Cov\\)):\n\\[\\begin{pmatrix} A \\\\ B \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_A \\\\ \\boldsymbol{\\mu}_B \\end{pmatrix}, \\begin{pmatrix} \\boldsymbol{\\Sigma}_{AA} & \\boldsymbol{\\Sigma}_{AB} \\\\ \\boldsymbol{\\Sigma}_{BA} & \\boldsymbol{\\Sigma}_{BB} \\end{pmatrix} \\right)\\]\nThen, the conditional distribution of \\(B\\) given \\(A\\) (i.e., \\(P(B|A)\\)) is also Gaussian with:\n\nConditional Mean: \\(\\boldsymbol{\\mu}_{B|A} = \\boldsymbol{\\mu}_B + \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} (\\mathbf{A} - \\boldsymbol{\\mu}_A)\\)\nConditional Covariance: \\(\\boldsymbol{\\Sigma}_{B|A} = \\boldsymbol{\\Sigma}_{BB} - \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} \\boldsymbol{\\Sigma}_{AB}\\)\n\nHow this maps to GPs:\n\nForm the Joint Distribution of \\((\\mathbf{y}_t, f^*)\\) under the Prior: We need the joint prior distribution of our observed outputs \\(\\mathbf{y}_t\\) and our unobserved test point function value \\(f^*\\). We know \\(\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\). Since \\(\\mathbf{f}_t\\) and \\(f^*\\) are jointly Gaussian (from the GP prior) and \\(\\boldsymbol{\\epsilon}\\) is Gaussian (noise), their sum is also jointly Gaussian. The joint prior mean of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is \\(\\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}\\). The joint prior covariance of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is:\n\\[\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\right) = \\begin{pmatrix} \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I} & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix}\\]\n(Here, the \\(\\sigma_n^2 \\mathbf{I}\\) term comes from the noise added to \\(\\mathbf{f}_t\\) in the \\(\\mathbf{y}_t\\) block).\nConditioning (Applying Bayes’ Rule): Now, we have observed \\(\\mathbf{y}_t\\). We want to find the posterior distribution of \\(f^*\\) given \\(\\mathbf{y}_t\\), which is \\(P(f^* | \\mathbf{y}_t)\\). Using the conditional Gaussian formulas, let:\n\n\\(A = \\mathbf{y}_t\\) (our observed data)\n\\(B = f^*\\) (the function value we want to predict)\n\\(\\boldsymbol{\\mu}_A = \\mathbf{m}_t\\)\n\\(\\boldsymbol{\\mu}_B = m(\\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AA} = \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\)\n\\(\\boldsymbol{\\Sigma}_{BB} = k(\\mathbf{x}^*, \\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AB} = \\mathbf{k}_*\\)\n\\(\\boldsymbol{\\Sigma}_{BA} = \\mathbf{k}_*^T\\)\n\nPlugging these into the conditional mean and covariance formulas gives exactly the GP posterior predictive mean and variance:\nPosterior Mean \\(\\mu_t(\\mathbf{x}^*)\\):\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\]\nPosterior Variance \\(\\sigma_t^2(\\mathbf{x}^*)\\):\n\\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\n\nIn essence:\nThe “multiplication” \\(P(\\mathcal{D} | f) P(f)\\) is handled internally by the mathematical properties of Gaussian distributions. The GP framework sets up a joint Gaussian prior over all relevant function values (observed and unobserved). The likelihood then specifies how our observed data \\(\\mathbf{y}_t\\) relates to the true function values \\(\\mathbf{f}_t\\). By conditioning this joint prior on the observed data \\(\\mathbf{y}_t\\), we directly derive the exact posterior distribution for the unobserved function values \\(f^*\\), which turns out to also be Gaussian with the mean and variance formulas."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-gp-prior-pf-joint-gaussian-over-all-relevant-points",
    "href": "posts/bayesian-optimization/index.html#the-gp-prior-pf-joint-gaussian-over-all-relevant-points",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "The key to the GP’s tractability is that while it’s a distribution over infinite-dimensional functions, any finite collection of function values drawn from a GP jointly follow a multivariate Gaussian distribution.\nConsider our observed input points \\(\\mathbf{X}_t = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_t\\}\\) and a new test input point \\(\\mathbf{x}^*\\). Under the GP prior, the vector of true (but unobserved) function values at these points, \\(\\mathbf{f}_t = [f(\\mathbf{x}_1), \\dots, f(\\mathbf{x}_t)]^T\\) and \\(f^* = f(\\mathbf{x}^*)\\), has a joint multivariate Gaussian distribution:\n\\[\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}, \\begin{pmatrix} \\mathbf{K}_t & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix} \\right)\\]\nHere:\n\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) is the prior mean at observed points.\n\\(\\mathbf{K}_t\\) is the \\(t \\times t\\) covariance matrix of the observed points, where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_* = [k(\\mathbf{x}^*, \\mathbf{x}_1), \\dots, k(\\mathbf{x}^*, \\mathbf{x}_t)]^T\\) is the \\(t \\times 1\\) vector of covariances between the test point and observed points.\n\\(k(\\mathbf{x}^*, \\mathbf{x}^*)\\) is the prior variance at the test point itself.\n\nThis joint prior distribution for \\(\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix}\\) represents the \\(P(f)\\) term in Bayes’ Theorem. We do not know what that exactly is, but it does tell us about the plausible relationships between function values at observed and unobserved locations before we see any data."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-likelihood-pmathcald-f-adding-gaussian-noise",
    "href": "posts/bayesian-optimization/index.html#the-likelihood-pmathcald-f-adding-gaussian-noise",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "We observe the data \\(\\mathcal{D}_t = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^t\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\). This can be written in vector form as:\n\\[\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\]\nwhere \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 \\mathbf{I})\\).\nThe likelihood \\(P(\\mathcal{D}_t | f)\\) (or more precisely, \\(P(\\mathbf{y}_t | \\mathbf{f}_t)\\)) is a Gaussian centered at the true function values \\(\\mathbf{f}_t\\):\n\\[P(\\mathbf{y}_t | \\mathbf{f}_t) = \\mathcal{N}(\\mathbf{y}_t | \\mathbf{f}_t, \\sigma_n^2 \\mathbf{I})\\]\nThis explicitly defines the \\(P(\\mathcal{D}|f)\\) term (using \\(\\mathbf{f}_t\\) as the “specific function” part for the observed data)."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-multiplication-and-conditioning-to-get-the-posterior-pf-mathcald",
    "href": "posts/bayesian-optimization/index.html#the-multiplication-and-conditioning-to-get-the-posterior-pf-mathcald",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "Now, the “multiplication” that yields the posterior GP is achieved by a fundamental property of multivariate Gaussian distributions:\nIf you have two random variables (or vectors of variables) \\(A\\) and \\(B\\) that are jointly Gaussian (note that \\(\\Sigma = Cov\\)):\n\\[\\begin{pmatrix} A \\\\ B \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_A \\\\ \\boldsymbol{\\mu}_B \\end{pmatrix}, \\begin{pmatrix} \\boldsymbol{\\Sigma}_{AA} & \\boldsymbol{\\Sigma}_{AB} \\\\ \\boldsymbol{\\Sigma}_{BA} & \\boldsymbol{\\Sigma}_{BB} \\end{pmatrix} \\right)\\]\nThen, the conditional distribution of \\(B\\) given \\(A\\) (i.e., \\(P(B|A)\\)) is also Gaussian with:\n\nConditional Mean: \\(\\boldsymbol{\\mu}_{B|A} = \\boldsymbol{\\mu}_B + \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} (\\mathbf{A} - \\boldsymbol{\\mu}_A)\\)\nConditional Covariance: \\(\\boldsymbol{\\Sigma}_{B|A} = \\boldsymbol{\\Sigma}_{BB} - \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} \\boldsymbol{\\Sigma}_{AB}\\)\n\nHow this maps to GPs:\n\nForm the Joint Distribution of \\((\\mathbf{y}_t, f^*)\\) under the Prior: We need the joint prior distribution of our observed outputs \\(\\mathbf{y}_t\\) and our unobserved test point function value \\(f^*\\). We know \\(\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\). Since \\(\\mathbf{f}_t\\) and \\(f^*\\) are jointly Gaussian (from the GP prior) and \\(\\boldsymbol{\\epsilon}\\) is Gaussian (noise), their sum is also jointly Gaussian. The joint prior mean of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is \\(\\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}\\). The joint prior covariance of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is:\n\\[\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\right) = \\begin{pmatrix} \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I} & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix}\\]\n(Here, the \\(\\sigma_n^2 \\mathbf{I}\\) term comes from the noise added to \\(\\mathbf{f}_t\\) in the \\(\\mathbf{y}_t\\) block).\nConditioning (Applying Bayes’ Rule): Now, we have observed \\(\\mathbf{y}_t\\). We want to find the posterior distribution of \\(f^*\\) given \\(\\mathbf{y}_t\\), which is \\(P(f^* | \\mathbf{y}_t)\\). Using the conditional Gaussian formulas, let:\n\n\\(A = \\mathbf{y}_t\\) (our observed data)\n\\(B = f^*\\) (the function value we want to predict)\n\\(\\boldsymbol{\\mu}_A = \\mathbf{m}_t\\)\n\\(\\boldsymbol{\\mu}_B = m(\\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AA} = \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\)\n\\(\\boldsymbol{\\Sigma}_{BB} = k(\\mathbf{x}^*, \\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AB} = \\mathbf{k}_*\\)\n\\(\\boldsymbol{\\Sigma}_{BA} = \\mathbf{k}_*^T\\)\n\nPlugging these into the conditional mean and covariance formulas gives exactly the GP posterior predictive mean and variance:\nPosterior Mean \\(\\mu_t(\\mathbf{x}^*)\\):\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\]\nPosterior Variance \\(\\sigma_t^2(\\mathbf{x}^*)\\):\n\\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\n\nIn essence:\nThe “multiplication” \\(P(\\mathcal{D} | f) P(f)\\) is handled internally by the mathematical properties of Gaussian distributions. The GP framework sets up a joint Gaussian prior over all relevant function values (observed and unobserved). The likelihood then specifies how our observed data \\(\\mathbf{y}_t\\) relates to the true function values \\(\\mathbf{f}_t\\). By conditioning this joint prior on the observed data \\(\\mathbf{y}_t\\), we directly derive the exact posterior distribution for the unobserved function values \\(f^*\\), which turns out to also be Gaussian with the mean and variance formulas."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-gaussian-process-gp-as-the-surrogate-model",
    "href": "posts/bayesian-optimization/index.html#the-gaussian-process-gp-as-the-surrogate-model",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "1. The Gaussian Process (GP) as the Surrogate Model",
    "text": "1. The Gaussian Process (GP) as the Surrogate Model\nAs established, the Gaussian Process models our unknown objective function \\(f(\\mathbf{x})\\) as a probability distribution over functions:\n\\[f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))\\]\n\n\\(m(\\mathbf{x})\\): Mean function (often assumed to be \\(m(\\mathbf{x})=0\\) or the mean of observed data for simplicity).\n\\(k(\\mathbf{x}, \\mathbf{x}')\\): Kernel (covariance) function, defining similarity between function values at different points. A common choice is the Squared Exponential (RBF) kernel:\n\\[k(\\mathbf{x}, \\mathbf{x}') = \\sigma_f^2 \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}'\\|^2}{2l^2}\\right)\\]\nwhere \\(\\sigma_f^2\\) is the signal variance (amplitude) and \\(l\\) is the length-scale.\n\nGiven a set of \\(t\\) observed data points \\(\\mathcal{D}_t = \\{(\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_t, y_t)\\}\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) (with additive Gaussian noise \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\)), the posterior predictive distribution for a new point \\(\\mathbf{x}^*\\) is Gaussian:\n\\[f(\\mathbf{x}^*) | \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\mathbf{x}^*), \\sigma_t^2(\\mathbf{x}^*))\\]\nThe predictive mean \\(\\mu_t(\\mathbf{x}^*)\\) and variance \\(\\sigma_t^2(\\mathbf{x}^*)\\) are given by:\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\] \\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\nWhere:\n\n\\(\\mathbf{y}_t = [y_1, \\dots, y_t]^T\\) (vector of observed values)\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) (mean function evaluated at observed points)\n\\(\\mathbf{K}_t\\): \\(t \\times t\\) covariance matrix where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_*\\): \\(t \\times 1\\) vector where \\([\\mathbf{k}_*]_i = k(\\mathbf{x}^*, \\mathbf{x}_i)\\).\n\\(\\mathbf{I}\\): Identity matrix.\n\\(\\sigma_n^2\\): Noise variance."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#bayesian-optimization-iteration-using-an-acquisition-function",
    "href": "posts/bayesian-optimization/index.html#bayesian-optimization-iteration-using-an-acquisition-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "2. Bayesian Optimization Iteration using an Acquisition Function",
    "text": "2. Bayesian Optimization Iteration using an Acquisition Function\nThe goal of Bayesian Optimization is to find \\(\\mathbf{x}^* = \\arg\\max_{\\mathbf{x} \\in \\mathcal{X}} f(\\mathbf{x})\\), where \\(\\mathcal{X}\\) is the search domain.\nThe iterative process involves:\n\nUpdate GP: Use the current dataset \\(\\mathcal{D}_t\\) to compute the posterior mean \\(\\mu_t(\\mathbf{x})\\) and variance \\(\\sigma_t^2(\\mathbf{x})\\) for the entire search space \\(\\mathcal{X}\\).\nMaximize Acquisition Function: Select the next point \\(\\mathbf{x}_{next}\\) by maximizing an acquisition function \\(a(\\mathbf{x})\\), which intelligently balances exploration (sampling in uncertain regions) and exploitation (sampling in promising regions). We’ll use Expected Improvement (EI) for our example:\n\\[\\text{EI}(\\mathbf{x}) = \\mathbb{E}[\\max(0, f(\\mathbf{x}) - y_{\\text{max}}^*)]\\]\nWhere \\(y_{\\text{max}}^* = \\max_{i=1 \\dots t} y_i\\) is the current best observed value.\nThe analytical form of EI (assuming \\(\\sigma_t(\\mathbf{x}) &gt; 0\\)) is:\n\\[\\text{EI}(\\mathbf{x}) = \\sigma_t(\\mathbf{x}) \\left[\\phi(Z) + Z\\Phi(Z)\\right]\\]\nwhere \\(Z = \\frac{\\mu_t(\\mathbf{x}) - y_{\\text{max}}^*}{\\sigma_t(\\mathbf{x})}\\). If \\(\\sigma_t(\\mathbf{x}) = 0\\), then \\(\\text{EI}(\\mathbf{x}) = 0\\). \\(\\phi(\\cdot)\\) is the PDF and \\(\\Phi(\\cdot)\\) is the CDF of the standard normal distribution.\nEvaluate True Function: Obtain \\(y_{next} = f(\\mathbf{x}_{next})\\).\nAdd to Data: \\(\\mathcal{D}_{t+1} = \\mathcal{D}_t \\cup \\{(\\mathbf{x}_{next}, y_{next})\\}\\)."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#numerical-example-optimizing-a-simple-1d-function",
    "href": "posts/bayesian-optimization/index.html#numerical-example-optimizing-a-simple-1d-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "3. Numerical Example: Optimizing a Simple 1D Function",
    "text": "3. Numerical Example: Optimizing a Simple 1D Function\nLet’s use a very simple 1D objective function \\(f(x) = -(x-2)^2 + 4\\) over the domain \\(x \\in [0,4]\\). The true maximum is at \\(x=2\\) with \\(f(2)=4\\).\nGP Hyperparameters (Fixed for Simplicity):\n\nMean function \\(m(x)=0\\).\nSquared Exponential Kernel: \\(\\sigma_f^2=1.0\\), \\(l=1.0\\). So, \\(k(x,x') = 1.0 \\cdot \\exp\\left(-\\frac{(x-x')^2}{2 \\cdot 1.0^2}\\right) = \\exp\\left(-\\frac{(x-x')^2}{2}\\right)\\).\nNoise variance \\(\\sigma_n^2=0.01\\).\n\nInitial Data Points (\\(\\mathcal{D}_2\\)): Let’s say we randomly selected two points and evaluated the true function (with no noise for simplicity in the example, so \\(\\epsilon_i=0\\)):\n\n\\(x_1=1.0 \\Rightarrow y_1 = -(1.0-2)^2 + 4 = -(-1)^2 + 4 = 3.0\\)\n\\(x_2=3.0 \\Rightarrow y_2 = -(3.0-2)^2 + 4 = -(1)^2 + 4 = 3.0\\)\n\nSo, our initial dataset is \\(\\mathcal{D}_2 = \\{(1.0, 3.0), (3.0, 3.0)\\}\\). Current best observed value: \\(y_{\\text{max}}^* = 3.0\\).\n\nIteration 1: Find the Next Point to Evaluate\nStep 1: Compute \\(\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\) and its inverse\nFirst, calculate the kernel matrix \\(\\mathbf{K}_2\\) for \\(x_1=1.0\\) and \\(x_2=3.0\\):\n\n\\(k(x_1,x_1) = \\exp\\left(-\\frac{(1-1)^2}{2}\\right) = \\exp(0) = 1.0\\)\n\\(k(x_1,x_2) = \\exp\\left(-\\frac{(1-3)^2}{2}\\right) = \\exp\\left(-\\frac{(-2)^2}{2}\\right) = \\exp\\left(-\\frac{4}{2}\\right) = \\exp(-2) \\approx 0.1353\\)\n\\(k(x_2,x_1) = k(x_1,x_2) \\approx 0.1353\\)\n\\(k(x_2,x_2) = \\exp\\left(-\\frac{(3-3)^2}{2}\\right) = \\exp(0) = 1.0\\)\n\nSo, \\(\\mathbf{K}_2 = \\begin{bmatrix} 1.0 & 0.1353 \\\\ 0.1353 & 1.0 \\end{bmatrix}\\)\nNow add the noise variance \\(\\sigma_n^2 \\mathbf{I} = 0.01 \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix}\\): \\(\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I} = \\begin{bmatrix} 1.01 & 0.1353 \\\\ 0.1353 & 1.01 \\end{bmatrix}\\)\nCalculate the inverse \\((\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1}\\): Determinant \\(= (1.01 \\times 1.01) - (0.1353 \\times 0.1353) = 1.0201 - 0.0183 \\approx 1.0018\\) Inverse \\(\\approx \\frac{1}{1.0018} \\begin{bmatrix} 1.01 & -0.1353 \\\\ -0.1353 & 1.01 \\end{bmatrix} \\approx \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix}\\)\nStep 2: Calculate \\(\\mu_t(x^*)\\) and \\(\\sigma_t^2(x^*)\\) for candidate points\nLet’s pick a few candidate points \\(x^*\\) to evaluate our GP at:\n\n\\(x_A^* = 0.5\\)\n\\(x_B^* = 2.0\\) (Near the true optimum, but previously unobserved)\n\\(x_C^* = 3.5\\)\n\nFor each \\(x^*\\), we need \\(\\mathbf{k}_* = [k(x^*,x_1), k(x^*,x_2)]^T\\):\nFor \\(x_A^* = 0.5\\):\n\n\\(k(0.5,1.0) = \\exp\\left(-\\frac{(0.5-1.0)^2}{2}\\right) = \\exp\\left(-\\frac{(-0.5)^2}{2}\\right) = \\exp(-0.125) \\approx 0.8825\\)\n\\(k(0.5,3.0) = \\exp\\left(-\\frac{(0.5-3.0)^2}{2}\\right) = \\exp\\left(-\\frac{(-2.5)^2}{2}\\right) = \\exp(-3.125) \\approx 0.0440\\) So, \\(\\mathbf{k}_* = \\begin{bmatrix} 0.8825 \\\\ 0.0440 \\end{bmatrix}\\)\n\nNow compute \\(\\mu_t(0.5)\\) and \\(\\sigma_t^2(0.5)\\): (\\(\\mathbf{y}_t - \\mathbf{m}_t = \\mathbf{y}_t = [3.0, 3.0]^T\\) since \\(m(x)=0\\))\n\\(\\mu_t(0.5) = \\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_t\\) (since \\(\\mathbf{m}_t=0\\)) \\(\\mu_t(0.5) \\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix} \\begin{bmatrix} 3.0 \\\\ 3.0 \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} (1.0082 \\times 3) + (-0.1351 \\times 3) \\\\ (-0.1351 \\times 3) + (1.0082 \\times 3) \\end{bmatrix} = \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 2.6193 \\\\ 2.6193 \\end{bmatrix}\\) \\(\\approx (0.8825 \\times 2.6193) + (0.0440 \\times 2.6193) \\approx 2.3106 + 0.1151 \\approx \\mathbf{2.4257}\\)\n\\(\\sigma_t^2(0.5) = k(0.5,0.5) - \\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\) \\(k(0.5,0.5) = 1.0\\) \\(\\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_* \\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix} \\begin{bmatrix} 0.8825 \\\\ 0.0440 \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} (1.0082 \\times 0.8825) + (-0.1351 \\times 0.0440) \\\\ (-0.1351 \\times 0.8825) + (1.0082 \\times 0.0440) \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 0.8837 \\\\ -0.0762 \\end{bmatrix} \\approx (0.8825 \\times 0.8837) + (0.0440 \\times -0.0762) \\approx 0.7797 - 0.0033 \\approx 0.7764\\) \\(\\sigma_t^2(0.5) \\approx 1.0 - 0.7764 = \\mathbf{0.2236}\\)\nLet’s summarize for our candidates (using a more precise calculator for speed for \\(x_B^*\\) and \\(x_C^*\\)):\n\nAt \\(x_A^* = 0.5\\): \\(\\mu_t(0.5) \\approx 2.425\\) \\(\\sigma_t(0.5) = \\sqrt{0.2236} \\approx 0.473\\)\nAt \\(x_B^* = 2.0\\): (This point is exactly in the middle of our two observed points, so we expect high uncertainty as it’s unobserved but also perhaps a good mean due to interpolation) \\(\\mu_t(2.0) \\approx 3.0\\) \\(\\sigma_t(2.0) \\approx 0.995\\) (High uncertainty because it’s far from observed data in terms of kernel distance, but interpolated mean is high)\nAt \\(x_C^* = 3.5\\): \\(\\mu_t(3.5) \\approx 2.425\\) \\(\\sigma_t(3.5) \\approx 0.473\\)\n\nStep 3: Calculate Expected Improvement (EI) for candidate points\nCurrent best \\(y_{\\text{max}}^* = 3.0\\). We will use \\(\\xi=0\\) (the default for simple EI, meaning no exploration-exploitation trade-off parameter).\nFor \\(x_A^* = 0.5\\):\n\\(Z = \\frac{\\mu_t(0.5) - y_{\\text{max}}^*}{\\sigma_t(0.5)} = \\frac{2.425 - 3.0}{0.473} = \\frac{-0.575}{0.473} \\approx -1.215\\) \\(\\phi(-1.215) \\approx 0.1804\\) (PDF value)\n\\(\\Phi(-1.215) \\approx 0.1122\\) (CDF value)\n\\(\\text{EI}(0.5) = 0.473 [0.1804 + (-1.215) \\cdot 0.1122] = 0.473 [0.1804 - 0.1364] = 0.473 [0.044] \\approx \\mathbf{0.0208}\\)\nFor \\(x_B^* = 2.0\\):\n\\(Z = \\frac{\\mu_t(2.0) - y_{\\text{max}}^*}{\\sigma_t(2.0)} = \\frac{3.0 - 3.0}{0.995} = 0\\) \\(\\phi(0) \\approx 0.3989\\) \\(\\Phi(0) = 0.5\\) \\(\\text{EI}(2.0) = 0.995 [0.3989 + 0 \\cdot 0.5] = 0.995 [0.3989] \\approx \\mathbf{0.3964}\\)\nFor \\(x_C^* = 3.5\\):\n\\(Z = \\frac{\\mu_t(3.5) - y_{\\text{max}}^*}{\\sigma_t(3.5)} = \\frac{2.425 - 3.0}{0.473} \\approx -1.215\\) \\(\\text{EI}(3.5) \\approx \\mathbf{0.0208}\\) (same as \\(x_A^*\\) due to symmetry in this specific example setup)\nStep 4: Identify \\(\\mathbf{x}_{next}\\)\nComparing the EI values:\n\n\\(\\text{EI}(0.5) \\approx 0.0208\\)\n\\(\\text{EI}(2.0) \\approx 0.3964\\)\n\\(\\text{EI}(3.5) \\approx 0.0208\\)\n\nThe point \\(\\mathbf{x}_{next}=\\mathbf{2.0}\\) has the highest Expected Improvement. This makes sense: it’s centrally located relative to the observed points, and while its predicted mean is only 3.0 (same as observed), its uncertainty is very high, suggesting a high potential for improvement.\nStep 5: Evaluate the true function at \\(\\mathbf{x}_{next}\\) and update data\nWe evaluate \\(f(2.0) = -(2.0-2)^2 + 4 = 4.0\\). Our updated dataset becomes \\(\\mathcal{D}_3 = \\{(1.0, 3.0), (3.0, 3.0), (2.0, 4.0)\\}\\). The new best observed value \\(y_{\\text{max}}^* = 4.0\\).\n\n\nIteration 2 (Conceptual)\nWith the new point \\((2.0, 4.0)\\), the GP model would be updated. The uncertainty around \\(x=2.0\\) would drastically decrease, as we now know its value precisely (or with very low noise). The acquisition function would then be maximized again. Given that \\(y_{\\text{max}}^*\\) is now 4.0 (the true optimum), the EI will be very low at \\(x=2.0\\). The algorithm would likely explore regions further away from \\(x=2.0\\) to ensure no other maxima exist, or converge as no significant improvement is expected elsewhere."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#formulation",
    "href": "posts/bayesian-optimization/index.html#formulation",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Formulation",
    "text": "Formulation\nBayes’ Theorem:\n\\[P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)}\\]\nWhere:\n\n\\(P(\\theta | D)\\): Posterior (Our updated belief in the parameters \\(\\theta\\) after seeing the Data \\(D\\)). This is what we want to find in Bayesian inference.\n\\(P(D | \\theta)\\): Likelihood (The probability of observing the Data \\(D\\) given the parameters \\(\\theta\\)). This is the core term that MLE maximizes.\n\\(P(\\theta)\\): Prior Probability (Our initial belief in the parameters \\(\\theta\\) before seeing any data).\n\\(P(D)\\): Evidence / Marginal Likelihood (The total probability of the data, across all possible parameters. This is a normalizing constant).\n\n\nMaximum Likelihood Estimation (MLE)\nMLE is purely data-driven (no belief about parameters needed). It asks: “Given this data, what are the parameters that make this data most probable?”\n\\[\\theta_{MLE} = \\arg \\max_{\\theta} P(D | \\theta)\\]\n\n\nMaximum A Posteriori (MAP) Estimation\nMAP is the inverse one, working on posterior:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(\\theta | D)\\]\nApplying Bayes’ Theorem:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} \\frac{P(D | \\theta) P(\\theta)}{P(D)}\\]\nSince \\(P(D)\\) is a constant with respect to \\(\\theta\\) ( doesn’t depend on \\(\\theta\\)):\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(D | \\theta) P(\\theta)\\]\nKey characteristic: MAP is a blend of data and prior beliefs. It asks: “Given this data and my prior beliefs about the parameters, what are the parameters that are most probable?”\n\n\nMLE is a Special Case of MAP\n\\(P(\\theta)\\) is constant, because parameter space is uniformly distributed:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(D | \\theta)\\]\nWow, MAP is reduced to MLE when parameters are uniformly distributed\nIn simple terms:\n\nMLE: “What parameters best explain the data I observed?” \\(\\rightarrow P(D | \\theta)\\)\nMAP: “What parameters are most plausible overall, considering both the data and what I already believed before seeing the data?” \\(\\rightarrow P(D | \\theta) \\times P(\\theta)\\)\nBayes’ Theorem (Full Inference): “What’s the complete updated probability distribution for my parameters, given everything I know?”"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#naive-bayes-classifier---the-naive-assumption-conditional-independence",
    "href": "posts/bayesian-optimization/index.html#naive-bayes-classifier---the-naive-assumption-conditional-independence",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Naive Bayes Classifier - The “Naive” Assumption: Conditional Independence",
    "text": "Naive Bayes Classifier - The “Naive” Assumption: Conditional Independence\nRecall Bayes’ Theorem for a hypothesis \\(H\\) (which is our class \\(C\\)) and evidence \\(E\\) (which are our features \\(F_1, F_2, \\dots, F_n\\)):\n\\[P(C | F_1, F_2, \\dots, F_n) = \\frac{P(F_1, F_2, \\dots, F_n | C) \\cdot P(C)}{P(F_1, F_2, \\dots, F_n)}\\]\nComputing the joint probability of correlated evidences likelihood term \\(P(F_1, F_2, \\dots, F_n | C)\\) can be very complex.\nThe “Naive” assumption: conditional independence, the joint likelihood = the product of individual feature likelihoods:\n\\[P(F_1, F_2, \\dots, F_n | C) \\approx \\prod_{i=1}^n P(F_i | C)\\]\nThus, we reduced the Naive Bayes Classifier to finding \\(C\\) that maximizes the posterior probability:\n\\(C_{predicted} = \\arg \\max_{C} \\left( P(C) \\cdot \\prod_{i=1}^n P(F_i | C) \\right)\\)\nJust the same as above, the denominator \\(P(E) = \\sum_i P(E|H_i)P(H_i) = P(F_1, F_2, \\dots, F_n)\\) is omitted during prediction because it’s a constant for all classes, acting only as a normalizer so that the sum of all posterior probabilities for all possible hypotheses equals 1."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#covariance-between-two-random-variables-1d",
    "href": "posts/bayesian-optimization/index.html#covariance-between-two-random-variables-1d",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Covariance between Two Random Variables (1D)",
    "text": "Covariance between Two Random Variables (1D)\nLet’s start with the simplest case: two scalar random variables \\(X\\) and \\(Y\\). The covariance between \\(X\\) and \\(Y\\), denoted \\(Cov(X, Y)\\) or \\(\\sigma_{XY}\\), measures the degree to which they vary together.\nThe formal definition of covariance is:\n\\[Cov(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\]\nWhere:\n\n\n\\(\\mathbb{E}[X]\\) is the expected value (mean) of a vector that contains some samples but only taking the \\(X\\) dimension.\n\n\n\\(\\mathbb{E}[Y]\\) is the expected value (mean) of a vector that contains the samples but only taking the \\(Y\\) dimension.\n\n\nDot product between vectors \\((X - \\mathbb{E}[X]) and (Y - \\mathbb{E}[Y])\\)\n\n\nThe outer operation \\(\\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\) basically just eventually normalized by dividing the number of samples\n\n\nIntuition:\n\nIf \\(X\\) mostly distributed above its mean AND \\(Y\\) is also above its mean, the product \\((X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\) will often be positive.\nIf \\(X\\) mostly distributed above its mean AND \\(Y\\) is below its mean (and vice-versa), the product will often be negative.\nOtherwise the distributed \\(X\\) and \\(y\\) cancel out on average =&gt; zero covariance only implies no linear relationship; variables can still have a non-linear relationship."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#covariance-of-a-single-random-variable-with-itself-variance",
    "href": "posts/bayesian-optimization/index.html#covariance-of-a-single-random-variable-with-itself-variance",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Covariance of a Single Random Variable with Itself (Variance)",
    "text": "Covariance of a Single Random Variable with Itself (Variance)\nVariance measures how much a single random variable deviates from its mean, or its spread, \\(Cov(X, X)\\). This gives us the variance of \\(X\\), denoted \\(Var(X)\\) or \\(\\sigma_X^2\\):\n\\[Cov(X, X) = \\mathbb{E}[(X - \\mathbb{E}[X])(X - \\mathbb{E}[X])] = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = Var(X)\\]\nCalculate the same way as above, basically a dot product of a vector \\((X - \\mathbb{E}[X])\\) with itself, therefore it will be sth like this:\n\\[Var(X) = (Xsample_1 - meanX)² + (Xsample_2 - meanX)² + ... + (Xsample_n - meanX)² = {a positive or negative number}\\]\nIntuition:\n\nmean \\(E[X]\\) could be in the middle, but when most samples are mostly distributed on negative or positive side, then the Medium will be on that side\nremember we are dealing with only one dimensional (scalar) variables"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-covariance-matrix-for-multiple-random-variables-multivariate-case",
    "href": "posts/bayesian-optimization/index.html#the-covariance-matrix-for-multiple-random-variables-multivariate-case",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "The Covariance Matrix for Multiple Random Variables (Multivariate Case)",
    "text": "The Covariance Matrix for Multiple Random Variables (Multivariate Case)\nNow, let’s extend both concepts above to multiple random variables. Suppose we have a random vector \\(\\mathbf{X}\\) consisting of \\(n\\) random variables:\n\\[\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}\\]\nThe covariance matrix \\(\\mathbf{\\Sigma}\\), is an \\(n \\times n\\) matrix where each element \\(\\mathbf{\\Sigma}_{ij}\\) represents the covariance between the \\(i\\)-th random variable \\(X_i\\) and the \\(j\\)-th random variable \\(X_j\\):\n\\[\\mathbf{\\Sigma}_{ij} = Cov(X_i, X_j)\\]\nMore formally, the covariance matrix \\(\\mathbf{\\Sigma}\\) is defined as:\n\\[\\mathbf{\\Sigma} = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])^T]\\]\nLet’s explicitly write out the elements of a \\(3 \\times 3\\) covariance matrix for a random vector \\(\\mathbf{X} = [X_1, X_2, X_3]^T\\):\n\\[\n\\mathbf{\\Sigma} = \\begin{bmatrix}\nVar(X_1) & Cov(X_1, X_2) & Cov(X_1, X_3) \\\\\nCov(X_2, X_1) & Var(X_2) & Cov(X_2, X_3) \\\\\nCov(X_3, X_1) & Cov(X_3, X_2) & Var(X_3)\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#intuition-of-the-covariance-matrix-spread-and-relationships",
    "href": "posts/bayesian-optimization/index.html#intuition-of-the-covariance-matrix-spread-and-relationships",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Intuition of the Covariance Matrix: Spread and Relationships",
    "text": "Intuition of the Covariance Matrix: Spread and Relationships\n\nDiagonal Elements: The variances of each individual random variable a.k.a the individual spread: \\[\\mathbf{\\Sigma}_{ii} = Cov(X_i, X_i) = Var(X_i)\\]\nOff-Diagonal Elements: \\[\\mathbf{\\Sigma}_{ij} = Cov(X_i, X_j) \\quad \\text{for } i \\neq j\\] These tell us how pairs of variables move together.\n\nIf \\(\\mathbf{\\Sigma}_{ij} &gt; 0\\), \\(X_i\\) and \\(X_j\\) tend to increase or decrease together.\nIf \\(\\mathbf{\\Sigma}_{ij} &lt; 0\\), \\(X_i\\) tends to increase when \\(X_j\\) decreases, and vice-versa.\nIf \\(\\mathbf{\\Sigma}_{ij} \\approx 0\\), there is little to no linear relationship between \\(X_i\\) and \\(X_j\\).\n\nSymmetry: The covariance matrix is always symmetric, \\(\\mathbf{\\Sigma}_{ij} = \\mathbf{\\Sigma}_{ji}\\) because \\(Cov(X_i, X_j) = Cov(X_j, X_i)\\), you know how.\n\nIn the next chapter about Gaussian Processes, we will slowly build the kernel function \\(k(x, x')\\) directly defines the elements of this covariance matrix for function values at different input points. This matrix say how much a point x is impacted by x’."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-linear-model-and-gaussian-noise-assumption",
    "href": "posts/bayesian-optimization/index.html#the-linear-model-and-gaussian-noise-assumption",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "The Linear Model and Gaussian Noise Assumption",
    "text": "The Linear Model and Gaussian Noise Assumption\nConsider a simple linear regression model where we try to predict an output \\(y_i\\) based on input features \\(\\mathbf{x}_i\\):\n\\[y_i = \\mathbf{x}_i^T \\boldsymbol{\\beta} + \\epsilon_i\\]\nWhere:\n\n\\(y_i\\) is the \\(i\\)-th observed output.\n\\(\\mathbf{x}_i^T\\) representing the features for the \\(i\\)-th observation.\n\\(\\boldsymbol{\\beta}\\) is the vector of unknown regression coefficients (parameters) we want to estimate.\n\\(\\epsilon_i\\) is the error or noise term for the \\(i\\)-th observation.\n\nCrucial assumption for this derivation is that these error terms \\(\\epsilon_i\\) are independently and identically distributed (i.i.d.) according to a Gaussian (Normal) distribution with a mean of zero and a constant variance \\(\\sigma^2\\):\n\\[\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\]\nBecause \\(\\mathbf{x}_i^T \\boldsymbol{\\beta}\\) is a fixed (non-random) quantity for a given \\(\\mathbf{x}_i\\), this assumption about the error implies that \\(y_i\\) itself is also normally distributed:\n\\[y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2 \\sim \\mathcal{N}(\\mathbf{x}_i^T \\boldsymbol{\\beta}, \\sigma^2)\\]\nThis means the probability density function (PDF) for a single observation \\(y_i\\) is:\n\\[p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\]"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#likelihood-function",
    "href": "posts/bayesian-optimization/index.html#likelihood-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nImportant Definition: For a dataset of \\(N\\) independent observations \\((\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_N, y_N)\\), the likelihood function \\(L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y})\\) is the product of the individual PDFs (due to the independence assumption):\n\\[L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\prod_{i=1}^N p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2)\\]\n\\[L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\]\nGoal Maximum Likelihood Estimation (MLE):\n\nGiven datast \\((\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_N, y_N)\\)\n=&gt; Find \\(\\boldsymbol{\\beta}\\) (and \\(\\sigma^2\\)) that maximize this likelihood function.\n\n\nDerivation\nConverts products into sums, simplifying differentiation. Since the logarithm is a monotonically increasing function, maximizing \\(\\ln L\\) is equivalent to maximizing \\(L\\):\n\\[\\ln L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\ln \\left( \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right) \\right)\\]\nUsing logarithm properties (\\(\\ln(ab) = \\ln a + \\ln b\\) and \\(\\ln(a^b) = b \\ln a\\)):\n\\[\\ln L = \\sum_{i=1}^N \\left[ \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\ln\\left(\\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\right) \\right]\\]\n\\[\\ln L = \\sum_{i=1}^N \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2} \\right]\\]\n\\[\\ln L = -N \\cdot \\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\]\n\n\nMaximizing this function with respect to \\(\\boldsymbol{\\beta}\\)\nTo do that, easy, we take the partial derivative with respect to \\(\\boldsymbol{\\beta}\\) and set it to zero.\nInterestingly, \\(-N \\cdot \\frac{1}{2}\\ln(2\\pi\\sigma^2)\\), does not depend on \\(\\boldsymbol{\\beta}\\). Therefore, when maximizing \\(\\ln L\\) with respect to \\(\\boldsymbol{\\beta}\\), we only need to consider the second term:\n\\[\\text{maximize} \\left( - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2} \\right)\\]\nand voila!: \\[\\text{minimize} \\left( \\sum_{i=1}^N (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 \\right)\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Stalk me",
    "section": "",
    "text": "Xin chào tớ là Thắng, tớ thích ăn bánh cuốn nguội tại vì nó thơm. Tớ quan tâm đến các vấn đề tự hành và ước mơ của tớ là để người máy giúp hết việc cho các bác công nhân đỡ vất vả :3"
  }
]