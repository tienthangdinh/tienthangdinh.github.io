[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html",
    "href": "posts/reinforcement-learning/index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) is a subfield of machine learning concerned with how agents ought to take actions in an environment in order to maximize cumulative reward."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#value-functions",
    "href": "posts/reinforcement-learning/index.html#value-functions",
    "title": "Reinforcement Learning",
    "section": "Value Functions",
    "text": "Value Functions\nVery similar to return \\(\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\\).\nValue functions are used to estimate expected returns:\n\nState-value function: \\(V_\\pi(s_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\)\nAction-value function: \\(Q_\\pi(s_t, a_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\nRelationship: \\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) and \\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)\nDerivation of this relationship:\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) (semantically true because)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} \\mid s_t = s, a_t = a]\\) (because \\(\\mathbb{E}[G_{t+1} \\mid s, a] = \\mathbb{E}[V_\\pi(S_{t+1}) \\mid s, a]\\))\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a)[r + \\gamma V_\\pi(s')]\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "href": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "title": "Reinforcement Learning",
    "section": "The Bellman Equations",
    "text": "The Bellman Equations\nFor any policy \\(\\pi\\), all \\(s \\in \\mathcal{S}\\), and all \\(a \\in \\mathcal{A}(s)\\):\nState-value function:\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\, q_\\pi(s, a) \\tag{1}\n\\]\nAction-value function:\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right] \\tag{2}\n\\]\nAfter substitution as derived above:\n\nBellman Equation for \\(v_\\pi(s)\\)\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]\n\n\nBellman Equation for \\(q_\\pi(s, a)\\)\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a' \\in \\mathcal{A}(s')} \\pi(a' \\mid s') q_\\pi(s', a') \\right]\n\\]\n\n\nOptimal Bellman\n\\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "href": "posts/reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "title": "Reinforcement Learning",
    "section": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement",
    "text": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement\nUsing the Bellman Equation above and an initial estimate (e.g. all states are randomly initialized or uniformly negative, except the terminal state which is 0), we iteratively update:\nWe apply the Bellman equations above for every single state (for V), or (state, action) pair (for Q, which forms a finer table). Over many runs, it will slowly converge. \\[\n\\pi'(s) = \\arg\\max_a q_\\pi(s, a)\n\\] Where: \\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#monte-carlo-state-value",
    "href": "posts/reinforcement-learning/index.html#monte-carlo-state-value",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo (State Value)",
    "text": "Monte Carlo (State Value)\nGoal: Given samples under \\(\\pi\\), estimate \\(q_\\pi\\).\n\nWe can express \\(q_\\pi\\)-estimation as \\(v_\\pi\\)-estimation. Imagine a new problem where: \\[\nS_t^{\\text{new}} = (S_t, A_t)\n\\]\n\nAny evaluation algorithm estimating \\(v(s^{\\text{new}})\\) would be estimating \\(q_\\pi(s, a)\\).\nSo basically what happening is that we do not have a policy anymore, instead, policy is now a part of the environment. But why do we want to do that? Because: - This is a generalization anyway, something applied to state value function V, also applied for action value function Q. - It simplifies our analysis, reduces the problem to a simpler problem Markov Reward Process (MRP), which is MDP without actions (s1,r1,s2,r2,s3,r3,…)\nOK, but still, how to do it?\nStart with a random value function: \\(V(s) = \\frac{1}{|S|}\\)\nCollect sampling trajectories \\(M\\) trajectory samples: \\[\ns_0^m \\quad r_1^m \\quad s_1^m \\quad \\cdots \\quad s_{T_m}^m \\qquad m = 1, \\ldots, M\n\\]\nThe Goal: Use averages to approximate \\(v_\\pi(s)\\): \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s] \\approx \\frac{1}{C(s)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[s_\\tau^m = s] \\, g_\\tau^m\n\\] where: \\[\n\\mathbb{I}[s_\\tau^m = s] =\n\\begin{cases}\n1 & \\text{if } s_\\tau^m = s \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] \\[\ng_\\tau^m = R_{t+1}^m + \\gamma R_{t+2}^m + \\gamma^2 R_{t+3}^m + \\dots + \\gamma^{T - t - 1} R_T^m\n\\]\nFor every sample trajectory \\(m\\), at any step \\(\\tau\\) in that trajectory, check if the state \\(g_\\tau^m\\) of that step is the \\(s\\) we are interested in, then include its return \\(g_\\tau^m\\) in the sum, then normalize by \\(C(s)\\), the total number of times state \\(s\\) was visited.\nAt this moment I just realized that: the state will get higher return, if its nearer to the beginning of a trajectory, if u dont believe, have a look at \\(g_\\tau^m\\) again ^^.\nBtw, to calculate return \\(g_\\tau^m\\), maybe you already know, we have to calculate from the terminate state first \\(R_T^m\\), where we know if the reward \\(R_T^m\\) is 0 or 1 (reached the goal or not), then slowly trace backward with addding \\(\\gamma\\)\nAnd to make sure you understand it, \\(v_\\pi(s)\\) is just like \\(G\\), but \\(G\\) is mostly binded to the trajectory and a policy, therefore the function \\(v_\\pi(s)\\) is actually \\(G\\)!!!\nHow to get to that Goal? to apply after the \\(m\\)-th sample: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha \\left( g_t^m - V(s_t^m) \\right)\n\\]\n… then it will slowly converge to the Goal above …\nBUT, how do we extend this to update our action ?"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#monte-carlo-action-value",
    "href": "posts/reinforcement-learning/index.html#monte-carlo-action-value",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo (Action Value)",
    "text": "Monte Carlo (Action Value)\nSince we also have a deterministic set of action \\(a \\in \\mathcal{A}(s)\\), therefore we can extend the state value above to action value like this, it is equivalent:\nStart also with \\(Q(s,a) = \\frac{1}{|SxA|}\\) or just simply 0\nBasically it just create a finer Q-table.\nThe Goal \\[\nQ_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a] \\approx \\frac{1}{C(s, a)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[(s,a)_\\tau^m = (s,a)] \\, g_\\tau^m\n\\]\nHow to get to that goal? \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha \\left( g_t^m - Q(s_t^m, a_t^m) \\right)\n\\]\n… Then it will slowly converge the Goal above …\nThen we just argmax over action at each state, thats how we get optimal action."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-constant-α-mc-algorithm-pi-approx-pi",
    "href": "posts/reinforcement-learning/index.html#sum-up-constant-α-mc-algorithm-pi-approx-pi",
    "title": "Reinforcement Learning",
    "section": "Sum up: Constant-α MC Algorithm \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Constant-α MC Algorithm \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(\\epsilon &gt; 0\\), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some \\(\\epsilon\\)-soft policy\n\n\\(Q(s,a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S},\\ a \\in \\mathcal{A}(s)\\) (like a random Q-Table hehe)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample a trajectory under policy \\(\\pi\\):\n\\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\)\nFor (literally EACH - EVERY SINGLE) \\(t = 0, \\dots, T_m - 1\\):\n\nCompute return (the best way is just to calculate backwards then slowly add \\(\\gamma\\) like Gonkee ^^):\n\\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nUpdate Q-value:\n\\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\n\\]\n\nUpdate policy:\n\\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\)\n\nWhere \\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\) is specified as follows: \\[\na^* \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a) \\quad \\text{(ties broken arbitrarily)}\n\\]\nFor all \\(a \\in \\mathcal{A}(s_t^m)\\): (this means to balance the policy to avoid a local optimal) \\[\n\\pi(a|s_t^m) \\leftarrow\n\\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a = a^* \\\\\n\\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a \\neq a^*\n\\end{cases}\n\\]\n(\\(|\\mathcal{A}(s_t^m)| = \\text{number of actions in } \\mathcal{A}(s_t^m)\\))\nthen back to the loop For \\(m = 1, \\dots, M\\) again and again …"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#off-policy",
    "href": "posts/reinforcement-learning/index.html#off-policy",
    "title": "Reinforcement Learning",
    "section": "Off-Policy",
    "text": "Off-Policy\nThe problem is, as seen in the Blackjack example, lack of sample leads to local-optimal bias model\n=&gt; Goal: more variance\nSample a trajectory under a different policy \\(b\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\). But the rest of the algorithm stays the same.\nOK, but how to make sampling policy \\(b\\) effect the main behavior policy \\(\\pi\\)?\n\nRelationship between sampling policy \\(b\\) vs main behavior policy \\(\\pi\\)?\nWe want: \\[\nq_\\pi(s, a) = \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]\n\\]\nSampled data under \\(b\\) means this is what we actually estimated: \\[\n\\mathbb{E}_b[G_t|S_t = s, A_t = a]\n\\]\nTherefore we use Importance Sampling to bring them to \\(\\pi\\): \\[\nq_\\pi(s, a) = \\mathbb{E}_b\\left[\\frac{p_\\pi(G_t)}{p_b(G_t)}G_t|S_t = s, A_t = a\\right]\n\\] where \\(\\rho\\) is the importance sampling ratio: \\[\n\\frac{p_\\pi(G_t)}{p_b(G_t)} = \\rho = \\prod_{\\tau=t+1}^{T-1} \\frac{\\pi(A_\\tau|S_\\tau)}{b(A_\\tau|S_\\tau)}\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "href": "posts/reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "title": "Reinforcement Learning",
    "section": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(b\\) (behavior policy), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some policy\n\\(Q(s, a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\) (also the random Q-Table above hehe)\n\nFor \\(m = 1, \\dots, M\\):\nUnder \\(b\\) sample: \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\)\n\nFor \\(t = 0, \\dots, T_m - 1\\):\n\n\\(\\rho_t^m \\leftarrow \\prod_{\\tau=t+1}^{T_m-1} \\frac{\\pi(a_\\tau^m|s_\\tau^m)}{b(a_\\tau^m|s_\\tau^m)}\\) (or 1 if \\(t+1 &gt; T_m-1\\))\nCompute return: \\(g_t^m \\leftarrow \\rho_t^m(r_{t+1}^m + \\gamma r_{t+2}^m + \\dots)\\)\nUpdate Q-Value: \\(Q(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\\)\nUpdate policy: \\(\\pi(s_t^m) \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a)\\) (ties broken arbitrarily)\n\nNote that at Update Policy: we do not need the \\(\\pi\\)-greedy as above, because now using behavior-policy \\(b\\), we could already diverse out for a more global view\n\nBUT, off policy MC has too much variance, therefore the next technique … Temporal Difference\nBefore we continue, let’s see where is exactly the point of model-free MC learning:"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "href": "posts/reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "title": "Reinforcement Learning",
    "section": "n-step Temporal Difference Learning",
    "text": "n-step Temporal Difference Learning\nRecall from the MC approach: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\nwhere: \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nn-STEP TD: Replace the target, \\(g_t^m\\), with: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n V(s_{t+n}^m)\n\\]\nwhere \\(V(s_{t+n}^m)\\) is actually no different than \\(g_{t+n}^m\\), but instead of waiting for the trajectory to finish the episode then calculate backward, we just need to wait for \\(n\\) steps to BOOTSTRAPPING the existing \\(V(s_{t+n})\\) calculated from older trajectories, think a little bit, it means the same thing with \\(g_{t+n}\\) (accumulated return). If \\(n = \\infty\\): TD is identical to MC."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#why-is-td-better",
    "href": "posts/reinforcement-learning/index.html#why-is-td-better",
    "title": "Reinforcement Learning",
    "section": "Why is TD better?",
    "text": "Why is TD better?\n\nMarkov property: The RL environment has Markov property, that means the future depends only on the current state, not the history. And TD works based on that design, the update is just between neighboring states. Not like waiting for the whole trajectory like MC, \\(g_t^m\\) needs backward calculation for the whole trajectory =&gt; strongly history based. \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\n\nFor example in TD(0) the use of \\(V(s_{t+1}^m)\\) is very Markov property: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(r_{t+1}^m + \\gamma V(s_{t+1}^m) - V(s_t^m))\n\\]\n\nReduced Variance: The effect of MC is calculating accumulated result at each single trajectory, then averaging then in the end, this makes the result very specific to the sampled trajectories. But TD directly add the already smoothed \\(+ \\gamma^n V(s_{t+n}^m)\\)\nOnline-learning we all know what that means"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sarsa-on-policy",
    "href": "posts/reinforcement-learning/index.html#sarsa-on-policy",
    "title": "Reinforcement Learning",
    "section": "SARSA (On-policy)",
    "text": "SARSA (On-policy)\nGiven a (state, action), how much better or worse is my return Gt relative to the new state and the SPECIFIC action that I ended up taking?\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n\\] BUT: there are many actions, not just one action a+1 so what action to take? We do not have access to p(s’,a’|s,a) so we do not know But we can calculate what we can expect in the next state after we take an action a ## Expected SARSA Given a (state, action), how much better or worse is my return Gt relative to the new state withh ALL actions AVERAGED OUT acording to policy probability pi? - we can calculate all the possible action at the next state and take the expected average with the policy probability (expected SARSA) \\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\sum_a \\pi(a \\mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning-off-policy",
    "href": "posts/reinforcement-learning/index.html#q-learning-off-policy",
    "title": "Reinforcement Learning",
    "section": "Q-Learning (Off-policy)",
    "text": "Q-Learning (Off-policy)\nGiven a (state, action), how much better or worse is my return Gt relative to the new state with the best action? \\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#credit-assignment-problem",
    "href": "posts/reinforcement-learning/index.html#credit-assignment-problem",
    "title": "Reinforcement Learning",
    "section": "Credit assignment problem",
    "text": "Credit assignment problem\nwithin an episode, figure out what subpath made the episode bad or good a.k.a individual action impact on the episode =&gt;"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning",
    "href": "posts/reinforcement-learning/index.html#q-learning",
    "title": "Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\nDevelop a function \\[ Q \\] to approximate \\[ Q_* \\], by updating:\n\\[\nQ(s_t, a_t) \\rightarrow r_t + \\gamma \\max_a Q(s_{t+1}, a)\n\\]\nRemember, the environment is random (many possible next states/rewards).\nAccording to the world model:\n\\[\np(s', r \\mid s, a)\n\\]\n(You don’t have access to this model directly.)\nTherefore, We do Q-learning with enough sample trajectories, we can average them out Equation that describes what \\[ Q_* \\] is actually supposed to be: Result of samples averaging out \\[\nQ_*(s_t, a_t) = \\mathbb{E} \\left[ r_t + \\gamma \\max_a Q_*(s_{t+1}, a) \\right]\n\\]\n\nNote: \\[ \\mathbb{E}[\\cdot] \\] = expected value = weighted average according to probabilities\n\n=&gt; That means, eventhough we do not have world model, when we do sampling, we still can gather enough information near to it"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "haha",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning\n\n\n\nmachine-learning\n\nRL\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nDinh Tien Thang\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 22, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  }
]