[
  {
    "objectID": "posts/reinforcement-learning/index.html",
    "href": "posts/reinforcement-learning/index.html",
    "title": "Model-free Reinforcement Learning",
    "section": "",
    "text": "An agent‚Äôs interaction with the environment is usually modeled as a Markov Decision Process (MDP):\n\\(s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2,...\\)\nwhere:\n\n\\(t \\in \\{0, 1, 2, \\dots\\}\\)\n\\(s \\in \\mathcal{S}\\): a state\n\\(a \\in \\mathcal{A}(s)\\): an action available in state \\(s\\)\n\\(r \\in \\mathcal{R} \\subseteq \\mathbb{R}\\): a scalar reward\n\nThe environment dynamics (transition model) are given by:\n\\[\np(s', r \\mid s, a) = \\text{Prob}(S_{t+1} = s',\\ R_{t+1} = r \\mid S_t = s,\\ A_t = a)\n\\]\n\n\n\nThe probability of the next state depends only on the current state and action ‚Äî not the full history: \\[\nP(s_{t+1} \\mid s_t, a_t) = P(s_{t+1} \\mid s_1, a_1, ..., s_t, a_t)\n\\]\n\n\n\n\n\nStochastic: \\(\\pi(a \\mid s)\\)\nDeterministic: \\(a = \\pi(s)\\)\n\n\n\n\nThe return \\(G_t\\) is the total discounted reward from time \\(t+1\\) to final time \\(T\\):\n\\[\nG_t = \\sum_{k = t+1}^{T} \\gamma^{k - t - 1} R_k\n\\]\nExpanded:\n\\[\nG_t = R_{t+1} + \\gamma G_{t+1}\n\\]\n\\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots + \\gamma^{T - t - 1} R_T\n\\]\n\n\n\n\\[\n\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\n\\]\nHow can we determine a policy that accumulates a high reward?"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#value-functions",
    "href": "posts/reinforcement-learning/index.html#value-functions",
    "title": "Model-free Reinforcement Learning",
    "section": "Value Functions",
    "text": "Value Functions\nVery similar to return \\(\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\\).\nValue functions are used to estimate expected returns:\n\nState-value function: \\(V_\\pi(s_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\)\nAction-value function: \\(Q_\\pi(s_t, a_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\nRelationship: \\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) and \\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)\nDerivation of this relationship:\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) (semantically true because)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} \\mid s_t = s, a_t = a]\\) (because \\(\\mathbb{E}[G_{t+1} \\mid s, a] = \\mathbb{E}[V_\\pi(S_{t+1}) \\mid s, a]\\))\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a)[r + \\gamma V_\\pi(s')]\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "href": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "title": "Model-free Reinforcement Learning",
    "section": "The Bellman Equations",
    "text": "The Bellman Equations\nFor any policy \\(\\pi\\), all \\(s \\in \\mathcal{S}\\), and all \\(a \\in \\mathcal{A}(s)\\):\nState-value function:\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\, q_\\pi(s, a) \\tag{1}\n\\]\nAction-value function:\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right] \\tag{2}\n\\]\nAfter substitution as derived above:\n\nBellman Equation for \\(v_\\pi(s)\\)\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]\n\n\nBellman Equation for \\(q_\\pi(s, a)\\)\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a' \\in \\mathcal{A}(s')} \\pi(a' \\mid s') q_\\pi(s', a') \\right]\n\\]\n\n\nOptimal Bellman\n\\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "href": "posts/reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "title": "Model-free Reinforcement Learning",
    "section": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement",
    "text": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement\nUsing the Bellman Equation above and an initial estimate (e.g.¬†all states are randomly initialized or uniformly negative, except the terminal state which is 0), we iteratively update:\nWe apply the Bellman equations above for every single state (for V), or (state, action) pair (for Q, which forms a finer table). Over many runs, it will slowly converge. \\[\n\\pi'(s) = \\arg\\max_a q_\\pi(s, a)\n\\] Where: \\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#monte-carlo-state-value",
    "href": "posts/reinforcement-learning/index.html#monte-carlo-state-value",
    "title": "Model-free Reinforcement Learning",
    "section": "Monte Carlo (State Value)",
    "text": "Monte Carlo (State Value)\nGoal: Given samples under \\(\\pi\\), estimate \\(q_\\pi\\).\n\nWe can express \\(q_\\pi\\)-estimation as \\(v_\\pi\\)-estimation. Imagine a new problem where: \\[\nS_t^{\\text{new}} = (S_t, A_t)\n\\]\n\nAny evaluation algorithm estimating \\(v(s^{\\text{new}})\\) would be estimating \\(q_\\pi(s, a)\\).\nSo basically what happening is that we do not have a policy anymore, instead, policy is now a part of the environment. But why do we want to do that? Because: - This is a generalization anyway, something applied to state value function V, also applied for action value function Q. - It simplifies our analysis, reduces the problem to a simpler problem Markov Reward Process (MRP), which is MDP without actions (s1,r1,s2,r2,s3,r3,‚Ä¶)\nOK, but still, how to do it?\nStart with a random value function: \\(V(s) = \\frac{1}{|S|}\\)\nCollect sampling trajectories \\(M\\) trajectory samples: \\[\ns_0^m \\quad r_1^m \\quad s_1^m \\quad \\cdots \\quad s_{T_m}^m \\qquad m = 1, \\ldots, M\n\\]\nThe Goal: Use averages to approximate \\(v_\\pi(s)\\): \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s] \\approx \\frac{1}{C(s)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[s_\\tau^m = s] \\, g_\\tau^m\n\\] where: \\[\n\\mathbb{I}[s_\\tau^m = s] =\n\\begin{cases}\n1 & \\text{if } s_\\tau^m = s \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] \\[\ng_\\tau^m = R_{t+1}^m + \\gamma R_{t+2}^m + \\gamma^2 R_{t+3}^m + \\dots + \\gamma^{T - t - 1} R_T^m\n\\]\nFor every sample trajectory \\(m\\), at any step \\(\\tau\\) in that trajectory, check if the state \\(g_\\tau^m\\) of that step is the \\(s\\) we are interested in, then include its return \\(g_\\tau^m\\) in the sum, then normalize by \\(C(s)\\), the total number of times state \\(s\\) was visited.\nAt this moment I just realized that: the state will get higher return, if its nearer to the beginning of a trajectory, if u dont believe, have a look at \\(g_\\tau^m\\) again ^^.\nBtw, to calculate return \\(g_\\tau^m\\), maybe you already know, we have to calculate from the terminate state first \\(R_T^m\\), where we know if the reward \\(R_T^m\\) is 0 or 1 (reached the goal or not), then slowly trace backward with addding \\(\\gamma\\)\nAnd to make sure you understand it, \\(v_\\pi(s)\\) is just like \\(G\\), but \\(G\\) is mostly binded to the trajectory and a policy, therefore the function \\(v_\\pi(s)\\) is actually \\(G\\)!!!\nHow to get to that Goal? to apply after the \\(m\\)-th sample: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha \\left( g_t^m - V(s_t^m) \\right)\n\\]\n‚Ä¶ then it will slowly converge to the Goal above ‚Ä¶\nBUT, how do we extend this to update our action ?"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#monte-carlo-action-value",
    "href": "posts/reinforcement-learning/index.html#monte-carlo-action-value",
    "title": "Model-free Reinforcement Learning",
    "section": "Monte Carlo (Action Value)",
    "text": "Monte Carlo (Action Value)\nSince we also have a deterministic set of action \\(a \\in \\mathcal{A}(s)\\), therefore we can extend the state value above to action value like this, it is equivalent:\nStart also with \\(Q(s,a) = \\frac{1}{|SxA|}\\) or just simply 0\nBasically it just create a finer Q-table.\nThe Goal \\[\nQ_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a] \\approx \\frac{1}{C(s, a)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[(s,a)_\\tau^m = (s,a)] \\, g_\\tau^m\n\\]\nHow to get to that goal? \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha \\left( g_t^m - Q(s_t^m, a_t^m) \\right)\n\\]\n‚Ä¶ Then it will slowly converge the Goal above ‚Ä¶\nThen we just argmax over action at each state, thats how we get optimal action."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-constant-Œ±-mc-algorithm-pi-approx-pi",
    "href": "posts/reinforcement-learning/index.html#sum-up-constant-Œ±-mc-algorithm-pi-approx-pi",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: Constant-Œ± MC Algorithm \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Constant-Œ± MC Algorithm \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(\\epsilon &gt; 0\\), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some \\(\\epsilon\\)-soft policy\n\n\\(Q(s,a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S},\\ a \\in \\mathcal{A}(s)\\) (like a random Q-Table hehe)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample a trajectory under policy \\(\\pi\\):\n\\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\)\nFor (literally EACH - EVERY SINGLE) \\(t = 0, \\dots, T_m - 1\\):\n\nCompute return (the best way is just to calculate backwards then slowly add \\(\\gamma\\) like Gonkee ^^):\n\\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nUpdate Q-value:\n\\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\n\\]\n\nUpdate policy:\n\\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\)\n\nWhere \\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\) is specified as follows: \\[\na^* \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a) \\quad \\text{(ties broken arbitrarily)}\n\\]\nFor all \\(a \\in \\mathcal{A}(s_t^m)\\): (this means to balance the policy to avoid a local optimal) \\[\n\\pi(a|s_t^m) \\leftarrow\n\\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a = a^* \\\\\n\\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a \\neq a^*\n\\end{cases}\n\\]\n(\\(|\\mathcal{A}(s_t^m)| = \\text{number of actions in } \\mathcal{A}(s_t^m)\\))\nthen back to the loop For \\(m = 1, \\dots, M\\) again and again ‚Ä¶"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#off-policy",
    "href": "posts/reinforcement-learning/index.html#off-policy",
    "title": "Model-free Reinforcement Learning",
    "section": "Off-Policy",
    "text": "Off-Policy\nThe problem is, as seen in the Blackjack example, lack of sample leads to local-optimal bias model\n=&gt; Goal: more variance\nSample a trajectory under a different policy \\(b\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\). But the rest of the algorithm stays the same.\nOK, but how to make sampling policy \\(b\\) effect the main behavior policy \\(\\pi\\)?\n\nRelationship between sampling policy \\(b\\) vs main behavior policy \\(\\pi\\)?\nWe want: \\[\nq_\\pi(s, a) = \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]\n\\]\nSampled data under \\(b\\) means this is what we actually estimated: \\[\n\\mathbb{E}_b[G_t|S_t = s, A_t = a]\n\\]\nTherefore we use Importance Sampling to bring them to \\(\\pi\\): \\[\nq_\\pi(s, a) = \\mathbb{E}_b\\left[\\frac{p_\\pi(G_t)}{p_b(G_t)}G_t|S_t = s, A_t = a\\right]\n\\] where \\(\\rho\\) is the importance sampling ratio: \\[\n\\frac{p_\\pi(G_t)}{p_b(G_t)} = \\rho = \\prod_{\\tau=t+1}^{T-1} \\frac{\\pi(A_\\tau|S_\\tau)}{b(A_\\tau|S_\\tau)}\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "href": "posts/reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(b\\) (behavior policy), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some policy\n\\(Q(s, a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\) (also the random Q-Table above hehe)\n\nFor \\(m = 1, \\dots, M\\):\nUnder \\(b\\) sample: \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\)\n\nFor \\(t = 0, \\dots, T_m - 1\\):\n\n\\(\\rho_t^m \\leftarrow \\prod_{\\tau=t+1}^{T_m-1} \\frac{\\pi(a_\\tau^m|s_\\tau^m)}{b(a_\\tau^m|s_\\tau^m)}\\) (or 1 if \\(t+1 &gt; T_m-1\\))\nCompute return: \\(g_t^m \\leftarrow \\rho_t^m(r_{t+1}^m + \\gamma r_{t+2}^m + \\dots)\\)\nUpdate Q-Value: \\(Q(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\\)\nUpdate policy: \\(\\pi(s_t^m) \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a)\\) (ties broken arbitrarily)\n\nNote that at Update Policy: we do not need the \\(\\pi\\)-greedy as above, because now using behavior-policy \\(b\\), we could already diverse out for a more global view\n\nBUT, off policy MC has too much variance, therefore the next technique ‚Ä¶ Temporal Difference\nBefore we continue, let‚Äôs see where is exactly the point of model-free MC learning:"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "href": "posts/reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "title": "Model-free Reinforcement Learning",
    "section": "n-step Temporal Difference Learning",
    "text": "n-step Temporal Difference Learning\nRecall from the MC approach: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\nwhere: \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nn-STEP TD: Replace the target, \\(g_t^m\\), with: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n V(s_{t+n}^m)\n\\]\nwhere \\(V(s_{t+n}^m)\\) is actually no different than \\(g_{t+n}^m\\), but instead of waiting for the trajectory to finish the episode then calculate backward, we just need to wait for \\(n\\) steps to BOOTSTRAPPING the existing \\(V(s_{t+n})\\) calculated from older trajectories, think a little bit, it means the same thing with \\(g_{t+n}\\) (accumulated return). If \\(n = \\infty\\): TD is identical to MC."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#why-is-td-better",
    "href": "posts/reinforcement-learning/index.html#why-is-td-better",
    "title": "Model-free Reinforcement Learning",
    "section": "Why is TD better?",
    "text": "Why is TD better?\n\nMarkov property: The RL environment has Markov property, that means the future depends only on the current state, not the history. And TD works based on that design, the update is just between neighboring states. Not like waiting for the whole trajectory like MC, \\(g_t^m\\) needs backward calculation for the whole trajectory =&gt; strongly history based. \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\n\nFor example in TD(0) the use of \\(V(s_{t+1}^m)\\) is very Markov property: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(r_{t+1}^m + \\gamma V(s_{t+1}^m) - V(s_t^m))\n\\]\n\nReduced Variance: The effect of MC is calculating accumulated result at each single trajectory, then averaging then in the end, this makes the result very specific to the sampled trajectories. But TD directly add the already smoothed \\(+ \\gamma^n V(s_{t+n}^m)\\)\nOnline-learning we all know what that means"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#what-does-larger-n-means",
    "href": "posts/reinforcement-learning/index.html#what-does-larger-n-means",
    "title": "Model-free Reinforcement Learning",
    "section": "What does larger n means?",
    "text": "What does larger n means?\nIncrease the bias = utilize more from the knowledge from this trajectory, instead of keep learning from the average.\nE.g., for a single episode with TD(8):\n\nAll states from \\(S_0\\) up to \\(S_{k-8}\\) (if \\(k \\ge 8\\)): Will be updated using a full 8-step return, bootstrapping from \\(V(S_{t+8})\\) =&gt; very average in the beginning\nThe last 7 states (\\(S_{k-7}, \\dots, S_{k-1}\\)): Will be updated using a return that effectively ‚Äúruns out of steps‚Äù before 8. Their targets will be a mix of actual rewards leading to the terminal state, like Monte Carlo =&gt; direct reward in the end"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-on-policy-temporal-difference-n-step-sarsa",
    "href": "posts/reinforcement-learning/index.html#sum-up-on-policy-temporal-difference-n-step-sarsa",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: On-Policy Temporal Difference: n-step Sarsa",
    "text": "Sum up: On-Policy Temporal Difference: n-step Sarsa\nModel-free control \\(\\rightarrow\\) use \\(Q(s, a)\\), not \\(V(s)\\).\nRedefine: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n Q(s_{t+n}^m, a_{t+n}^m)\n\\]\nUpdate rule: \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_{t:t+n}^m - Q(s_t^m, a_t^m))\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning",
    "href": "posts/reinforcement-learning/index.html#q-learning",
    "title": "Model-free Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\n1-step TD Control‚Äî-target adjustment‚Äî-&gt; Q-Learning (off-policy).\nInstead of using \\(r_{t+1}^m + \\gamma Q(s_{t+1}^m, a_{t+1}^m)\\) (which is used in SARSA and relies on the next action taken by the policy), Q-Learning uses: \\[\nr_{t+1}^m + \\gamma \\max_{a} Q(s_{t+1}^m, a)\n\\]\nThe \\(\\max\\) operator means, regardless of which action the behavior policy \\(b\\) actually took, this target is formed by the best possible action from the next state \\(s_{t+1}^m\\) =&gt; Q-Learning an off-policy.\nTo describe what actually happens, it is like this: 1-step TD (SARSA-like): \\[\n\\dots s_0^m, \\underset{\\uparrow}{\\underline{a_0^m}}, r_1^m, s_1^m, \\underset{\\uparrow}{\\underline{a_1^m}}, r_2^m, s_2^m, \\underset{\\uparrow}{\\underline{a_2^m}}, r_3^m, s_3^m, \\dots\n\\] Updates for \\(Q(s_t, a_t)\\) occur after observing \\(s_{t+1}, a_{t+1}\\), using the target \\(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\) where \\(a_{t+1}\\) is the action taken by the current policy.\n1-step Q-Learning: \\[\n\\dots  \\underset{\\uparrow}{\\underline{s_0^m}},a_0^m, r_1^m, \\underset{\\uparrow}{\\underline{s_1^m}},a_1^m, r_2^m, \\underset{\\uparrow}{\\underline{s_2^m}},a_2^m, r_3^m, \\dots\n\\] Updates for \\(Q(s_t, a_t)\\) occur after observing \\(s_{t+1}\\), using the target \\(r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a)\\), where action \\(a_{t+1}\\) is observed but not used in forming the target for \\(Q(s_t, a_t)\\)."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#expected-sarsa",
    "href": "posts/reinforcement-learning/index.html#expected-sarsa",
    "title": "Model-free Reinforcement Learning",
    "section": "Expected Sarsa",
    "text": "Expected Sarsa\n1-step Q-Learning ‚Äî‚Äì \\(\\max\\) operator-&gt;average operator‚Äî&gt; Expected Sarsa \\[\nr_{t+1}^m + \\gamma \\max_{a} Q(s_{t+1}^m, a)\n\\] to using an expectation over all possible actions, weighted by the policy \\(\\pi\\): \\[\nr_{t+1}^m + \\gamma \\sum_{a} Q(s_{t+1}^m, a) \\pi(a|s_{t+1}^m)\n\\]\nAs presented (when the policy \\(\\pi\\) used in the target is the same as the behavior policy generating the data), this is an on-policy method.\nBut to make it off-policy, just need \\(\\text{policy generating the trajectory} \\neq \\pi \\text{ in target}\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#compare",
    "href": "posts/reinforcement-learning/index.html#compare",
    "title": "Model-free Reinforcement Learning",
    "section": "Compare",
    "text": "Compare\n\nSarsa has longer path, because it just took the action it actually took, which was most of the time exploratory, the one with \\(\\epsilon \\text{exploration}\\) policy that balance out\nQ-Learning does not use this \\(\\epsilon \\text{exploration}\\) policy, it uses \\(\\max\\) operator\nExpected Sarsa use weighted average, so yeah, always a safe choice."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#summary-of-td",
    "href": "posts/reinforcement-learning/index.html#summary-of-td",
    "title": "Model-free Reinforcement Learning",
    "section": "Summary of TD",
    "text": "Summary of TD\nGoal of Q-Learning is updating Q-Table to optimal where: \\[\nQ_*(s_t, a_t) = \\mathbb{E} \\left[ r_t + \\gamma \\max_a Q_*(s_{t+1}, a) \\right]\n\\]\nAlso called Bellman Optimality Equation: every subsequence of a optimal sequence must also be optimal\nLearning Q-values:\n\nSARSA:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n\\]\nExpected SARSA:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\sum_a \\pi(a \\mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\nQ-Learning:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\n\nLearning V-values:\n\\[\nV(s_t) \\leftarrow V(s_t) + \\alpha \\left[ r_t + \\gamma V(s_{t+1}) - V(s_t) \\right]\n\\]\nWhere \\(\\alpha\\) is the learning rate."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#on-policy-evaluation-with-function-approximation",
    "href": "posts/reinforcement-learning/index.html#on-policy-evaluation-with-function-approximation",
    "title": "Model-free Reinforcement Learning",
    "section": "On-Policy Evaluation with Function Approximation",
    "text": "On-Policy Evaluation with Function Approximation\nGoal remains to approximate the state-value function \\(v_\\pi(s)\\). Data generated from a given fixed policy \\(\\pi\\).\nWe now learn a parameterized function \\(\\hat{v}(s, \\mathbf{w})\\), where:\n\n\\(\\mathbf{w} \\in \\mathbb{R}^d\\) is a vector of parameters\na state \\(s\\) as input\n\nWe learn \\(w\\) and hope that \\(v_\\pi(s) \\approx \\hat{v}(s, \\mathbf{w})\\)\nSince \\(d \\ll |\\mathcal{S}|\\), any change to \\(\\mathbf{w}\\) can simultaneously change \\(\\hat{v}(s, \\mathbf{w})\\) for many (or all) states \\(s\\). Different from tabular methods, where an update to \\(V(s)\\) for a specific state \\(s\\) affects only that state‚Äôs value."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-represent-mathbfxs",
    "href": "posts/reinforcement-learning/index.html#how-to-represent-mathbfxs",
    "title": "Model-free Reinforcement Learning",
    "section": "How to represent \\(\\mathbf{x}(s)\\)?",
    "text": "How to represent \\(\\mathbf{x}(s)\\)?\nExample 1: two simple features for given image state \\(s\\):\n\n\\(x_1(s)\\): The average of all pixel values in the image.\n\\(x_2(s)\\): The standard deviation of all pixel values in the image.\n\n=&gt; feature vector \\(\\mathbf{x}(s) = \\begin{bmatrix} x_1(s) \\\\ x_2(s) \\end{bmatrix}\\)\nWith these features, we can construct a linear value function to approximate \\(v_\\pi(s)\\): \\[\n\\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s)^T \\mathbf{w}\n\\]\nExample 2: Mountain Car below with a dynamic model of velocity \\(dx\\) and position \\(x\\)\nExample 3: Proto Points and Radius Basis Function will be discussed in next chapter Policy Gradient Method"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#goal-how-to-get-mathbfw",
    "href": "posts/reinforcement-learning/index.html#goal-how-to-get-mathbfw",
    "title": "Model-free Reinforcement Learning",
    "section": "Goal: How to get \\(\\mathbf{w}\\)?",
    "text": "Goal: How to get \\(\\mathbf{w}\\)?\nThe ‚Äòbest‚Äô \\(\\mathbf{w}\\) minimizes: \\[\n\\overline{VE}(\\mathbf{w}) = \\sum_{s \\in \\mathcal{S}} \\mu(s)[v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]^2\n\\] where \\(\\mu(\\cdot)\\) is a distribution over states (frequency of visiting each state).\n\nWe observe a surrogate for \\(v_\\pi(S_t)\\): \\(U_t\\): Since we don‚Äôt know \\(v_\\pi(S_t)\\) exactly, we use a sample-based estimate or target, \\(U_t\\), as a stand-in. This \\(U_t\\) could be the Monte Carlo return \\(G_t\\), or an n-step TD target \\(g_{t:t+n}\\), or a 1-step TD target \\((R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}))\\).\n\nUpdate Rule we don‚Äôt have direct access to \\(v_\\pi(s)\\) for all states =&gt; Stochastic Gradient Descent (SGD) for updating our parameters \\(\\mathbf{w}\\): \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\] Where:\n\n\\(\\alpha\\) is the learning rate (step size).\n\\(\\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right]\\) is the TD error (or prediction error) based on our sample \\(U_t\\).\n\\(\\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\\) is the gradient of our estimated value function with respect to its parameters \\(\\mathbf{w}\\), evaluated at state \\(S_t\\). This gradient tells us how to adjust \\(\\mathbf{w}\\) to change \\(\\hat{v}(S_t, \\mathbf{w})\\) in the desired direction."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-obtain-the-target-u_t",
    "href": "posts/reinforcement-learning/index.html#how-to-obtain-the-target-u_t",
    "title": "Model-free Reinforcement Learning",
    "section": "How to Obtain the Target \\(U_t\\)",
    "text": "How to Obtain the Target \\(U_t\\)\nIn the Stochastic Gradient Descent update rule: \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\] The term \\(U_t\\) serves as our sample-based target for the true value \\(v_\\pi(S_t)\\). Since \\(v_\\pi(S_t)\\) is unknown, we must derive \\(U_t\\) from our observed experience. The choice of \\(U_t\\) determines whether our method leans towards Monte Carlo or Temporal Difference approaches:\n1. Monte Carlo Target: If the target \\(U_t\\) is the full Monte Carlo return from state \\(S_t\\) to the end of the episode, then we are using a Gradient Monte Carlo method: \\[\nU_t = G_t\n\\] where \\(G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1}R_T\\) is the total discounted return observed from time step \\(t\\) until the terminal state \\(T\\).\n\nCharacteristics:\n\nUnbiased: If \\(G_t\\) is an unbiased estimate of \\(v_\\pi(S_t)\\) (which it is, on average), then using it as \\(U_t\\) can lead to the parameters \\(\\mathbf{w}\\) converging to a local optimum of the Mean Squared Value Error (\\(\\overline{VE}(\\mathbf{w})\\)).\nHigh Variance: \\(G_t\\) can be noisy due to the sum of many random rewards.\nRequires complete episodes: We must wait until the episode ends to compute \\(G_t\\).\n\n\n2. Temporal Difference (TD) Target: If the target \\(U_t\\) is derived using bootstrapping (i.e., using an estimate of the value of a future state), then we are using a Semi-Gradient TD method. The most common is the 1-step TD target: \\[\nU_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\n\\] Here, \\(R_{t+1}\\) is the actual reward observed, and \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\) is our current estimate of the value of the next state \\(S_{t+1}\\). For this specific update, \\(\\mathbf{w}\\) in \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\) is usually the online network‚Äôs weights, not a target network‚Äôs weights in this basic formulation.\n\nCharacteristics:\n\nBiased: especially in the beginning with crappy initialized value \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\).\nLower Variance: It typically has lower variance than the Monte Carlo target because it depends on only one actual reward and then uses a smoothed estimate for the rest of the future.\nOnline Learning: Updates can be performed after each single time step, without waiting for the end of an episode.\nSemi-Gradient: Since \\(U_t\\) depends on \\(\\mathbf{w}\\), our update rule is not a true gradient step. The gradient \\(\\nabla_{\\mathbf{w}} L(\\mathbf{w})\\) for the loss \\(\\left( (R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})) - \\hat{v}(S_t, \\mathbf{w}) \\right)^2\\) would actually involve the derivative of \\(U_t\\) (\\(\\hat{v}(S_{t+1}, \\mathbf{w})\\)) with respect to \\(\\mathbf{w}\\). =&gt; semi-gradient means: \\(\\nabla_{\\mathbf{w}} \\hat{v}(S_{t+1}, \\mathbf{w})\\) is omitted for simplicity and stability, only taking \\(\\nabla_{\\mathbf{w}} \\hat{v}(S_{t}, \\mathbf{w})\\).\nNo Guarantee of Convergence (to Global Optimum): Because it‚Äôs not a true gradient of the overall \\(\\overline{VE}(\\mathbf{w})\\), we generally don‚Äôt guarantee convergence to the global optimum of the Mean Squared Value Error, even if the optimal \\(\\mathbf{w}\\) is unique. However, for linear function approximation, it can still converge to a local optimum. For non-linear approximators like neural networks, theoretical guarantees are weaker, but these methods still perform very well in practice.\n\n\nThe update rule remains: \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation",
    "href": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation",
    "title": "Model-free Reinforcement Learning",
    "section": "Extension to On-Policy Control with Function Approximation",
    "text": "Extension to On-Policy Control with Function Approximation\nSo far is just policy evaluation (approximating \\(v_\\pi(s)\\)). Now extend directly to control problems (finding an optimal policy), typically by approximating the action-value function \\(q_\\pi(s,a)\\) or \\(q_*(s,a)\\). \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{q}(S_t, A_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})\n\\] Here, \\(\\hat{q}(S_t, A_t, \\mathbf{w})\\) is our function approximator‚Äôs estimate of the action-value for the state-action pair \\((S_t, A_t)\\) using parameters \\(\\mathbf{w}\\). The term \\(\\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})\\) is the gradient of this estimate with respect to \\(\\mathbf{w}\\).\nFor Semi-gradient 1-step Sarsa, the target \\(U_t\\) is defined as: \\[\nU_t = R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\mathbf{w})\n\\]\nSince both the action being evaluated (\\(A_t\\)) and the action used to construct the target (\\(A_{t+1}\\)) are chosen according to the same behavior policy (which is actively being improved based on \\(\\hat{q}\\)), this method is on-policy. The agent learns the value of the policy it is currently following, and this policy itself is improved through action selection methods (e.g., \\(\\epsilon\\)-greedy) based on the learned \\(\\hat{q}\\) values.\nExample: Linear Action-Value Function for the Mountain Car Task\nTo illustrate how function approximation can be used for action-value functions, let‚Äôs consider a scenario like The Mountain Car Task. We approximate the action-value function \\(\\hat{q}(s, a, \\mathbf{w})\\) using a linear function approximator instead of a Q-Table: \\[\n\\hat{q}(s, a, \\mathbf{w}) = \\begin{cases}\n    \\mathbf{w}_{-1}^T \\mathbf{x}(s) & \\text{if } a = -1 \\\\\n    \\mathbf{w}_{0}^T \\mathbf{x}(s) & \\text{if } a = 0 \\\\\n    \\mathbf{w}_{1}^T \\mathbf{x}(s) & \\text{if } a = 1\n\\end{cases}\n\\] Where:\n\naction \\(a\\) (-1, 0 -1)\n\\(\\mathbf{x}(s)\\) is the feature representation of the state \\(s\\) (length 120 decoded from position & velocity)\n\\(\\mathbf{w}_{-1}\\), \\(\\mathbf{w}_{0}\\), and \\(\\mathbf{w}_{1}\\) are distinct weight vectors, each corresponding to one of the possible actions.\nThe overall parameter vector \\(\\mathbf{w}\\) for the entire function approximator would be the concatenation of these individual action-specific weight vectors (i.e., \\(\\mathbf{w} = [\\mathbf{w}_{-1}, \\mathbf{w}_{0}, \\mathbf{w}_{1}]\\))."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation-1",
    "href": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation-1",
    "title": "Model-free Reinforcement Learning",
    "section": "Extension to On-Policy Control with Function Approximation",
    "text": "Extension to On-Policy Control with Function Approximation\nWe all know what that is, just that when we combine three things together:\n\noff-policy\nfunction approximation\nbootstrapping\n\nwe will have problem with convergence, which may be solved by the next topic ‚Ä¶"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-represent-xs",
    "href": "posts/reinforcement-learning/index.html#how-to-represent-xs",
    "title": "Model-free Reinforcement Learning",
    "section": "How to represent \\(x(s)\\)?",
    "text": "How to represent \\(x(s)\\)?\n\nProto Points and Radial Basis Functions (RBFs)\ninstead of having a neural network directly compute an output, the state is first transformed into a set of features by measuring its ‚Äúsimilarity‚Äù or ‚Äúproximity‚Äù to a predefined set of ‚Äúproto points‚Äù or ‚Äúcenters.‚Äù\n\nDefine Proto Points (Centers):\nFirst, you select a number of ‚Äúproto points‚Äù or ‚Äúcenters‚Äù in your state space. Let‚Äôs call these centers \\(c_1, c_2, \\dots, c_k\\). These centers are essentially fixed, representative points in your environment‚Äôs state space. They could be chosen: manually or randomly\n\n\nDefine Basis Functions (e.g., Radial Basis Functions):\nFor each center \\(c_j\\), you define a basis function \\(\\phi_j(s)\\). A common choice is a Gaussian Radial Basis Function: \\[\n\\phi_j(s) = \\exp \\left( -\\frac{\\|s - c_j\\|^2}{2\\sigma_j^2} \\right)\n\\]\nwhere:\n\n\\(\\|s - c_j\\|^2\\) is the squared Euclidean distance between the current state \\(s\\) and the center \\(c_j\\).\n\\(\\sigma_j^2\\) is a variance or width parameter for that basis function, controlling how broad its ‚Äúinfluence‚Äù is.\n\nWhat does \\(\\phi_j(s)\\) mean? It‚Äôs a measure of how ‚Äúsimilar‚Äù or ‚Äúclose‚Äù the current state \\(s\\) is to the center \\(c_j\\). It peaks at 1 when \\(s = c_j\\) and decays to 0 as \\(s\\) moves away from \\(c_j\\).\n\n\nConstruct the Feature Vector:\nFor any given state \\(s\\), you compute the value of each basis function: \\[\n\\mathbf{x}(s) = \\begin{bmatrix} \\phi_1(s) \\\\ \\phi_2(s) \\\\ \\vdots \\\\ \\phi_k(s) \\end{bmatrix}\n\\] This \\(\\mathbf{x}(s)\\) is your feature vector, where each feature represents the activation of a ‚Äúproto point.‚Äù\n\n\nLinear Function Approximation:\nNow, these features are used in a linear function approximator. The parameters you learn are the weights associated with each of these basis functions.\n\nFor a value function: \\[\n  \\hat{V}(s, \\mathbf{w}) = \\sum_{j=1}^k w_j \\phi_j(s) = \\mathbf{w}^T \\mathbf{x}(s)\n  \\] Here, \\(\\mathbf{w}\\) is the vector \\([w_1, \\dots, w_k]\\), and \\(w_j\\) is the weight for the \\(j\\)-th proto point‚Äôs influence.\nFor a policy (e.g., logits in a softmax): For each action \\(a\\), you‚Äôd have a separate weight vector \\(\\mathbf{\\theta}_a\\), and the logits for the policy could be: \\[\n  h(s,a,\\mathbf{\\theta}_a) = \\mathbf{\\theta}_a^T \\mathbf{x}(s)\n  \\] Then, \\(\\pi(a|s,\\mathbf{\\theta}) = \\text{softmax}(h(s,a,\\mathbf{\\theta}_a))\\).\n\n\n\n\nExample: Policy Gradient with Proto Points (RBFs) for Mountain Car\nImagine our Mountain Car environment again. The state \\(s\\) is (position, velocity), which are continuous. We want to learn a policy \\(\\pi(a|s, \\mathbf{\\theta})\\) directly.\n1. Define Proto Points (Centers) in State Space:\nLet‚Äôs say we define \\(K=4\\) proto points (centers) \\(c_j\\) in our 2D (position, velocity) state space. For simplicity, let‚Äôs pick them:\n\n\\(c_1 = (-0.5, 0.0)\\) (Mid-left, still)\n\\(c_2 = (0.0, 0.0)\\) (Center, still)\n\\(c_3 = (0.5, 0.0)\\) (Mid-right, still)\n\\(c_4 = (-0.2, 0.05)\\) (Slightly left, moving right)\n\nWe also define a width \\(\\sigma^2\\) for all RBFs (or separate \\(\\sigma_j^2\\) values).\n2. Create the State Feature Vector \\(\\mathbf{x}(s)\\) using RBFs:\nFor any state \\(s = (\\text{pos}, \\text{vel})\\), we calculate its similarity to each of these 4 proto points using a Gaussian RBF. Our feature vector \\(\\mathbf{x}(s)\\) will have 4 dimensions:\n\\[\n\\mathbf{x}(s) = \\begin{bmatrix}\n\\phi_1(s) = \\exp \\left( -\\frac{\\|s - c_1\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_2(s) = \\exp \\left( -\\frac{\\|s - c_2\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_3(s) = \\exp \\left( -\\frac{\\|s - c_3\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_4(s) = \\exp \\left( -\\frac{\\|s - c_4\\|^2}{2\\sigma^2} \\right)\n\\end{bmatrix}\n\\] This \\(\\mathbf{x}(s)\\) now serves as our ‚Äúmeaningful‚Äù representation of the state, telling us how similar \\(s\\) is to certain key points in the environment.\n3. Define the Policy \\(\\pi(a|s, \\mathbf{\\theta})\\) using these Features:\nFor each action \\(a \\in \\{-1, 0, 1\\}\\), we define a linear combination of these features to get a ‚Äúscore‚Äù or ‚Äúlogit‚Äù for that action.\nLet \\(\\mathbf{\\theta}_{-1}\\), \\(\\mathbf{\\theta}_{0}\\), \\(\\mathbf{\\theta}_{1}\\) be our learnable parameter vectors (weights), each of size \\(K=4\\). The total policy parameters \\(\\mathbf{\\theta}\\) would be the concatenation of these three vectors.\nThe logits for each action are: * \\(h(s, a=-1, \\mathbf{\\theta}) = \\mathbf{\\theta}_{-1}^T \\mathbf{x}(s)\\) * \\(h(s, a=0, \\mathbf{\\theta}) = \\mathbf{\\theta}_{0}^T \\mathbf{x}(s)\\) * \\(h(s, a=1, \\mathbf{\\theta}) = \\mathbf{\\theta}_{1}^T \\mathbf{x}(s)\\)\nAnd the policy probabilities are then given by the softmax function: \\[\\pi(a|s, \\mathbf{\\theta}) = \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_{a'} e^{h(s,a',\\mathbf{\\theta})}}\\]\n4. Update the Policy Parameters (\\(\\mathbf{\\theta}\\)) using Policy Gradients (REINFORCE):\nWhen we collect a trajectory and compute returns \\(G_t\\), the REINFORCE update rule applies: \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha G_t \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t|s_t, \\mathbf{\\theta})\\]\nLet‚Äôs look at a specific component of this update, say for \\(\\mathbf{\\theta}_{-1}\\): \\[\\mathbf{\\theta}_{-1} \\leftarrow \\mathbf{\\theta}_{-1} + \\alpha G_t \\nabla_{\\mathbf{\\theta}_{-1}} \\ln \\pi(a_t|s_t, \\mathbf{\\theta})\\]\nIf \\(a_t = -1\\), and \\(G_t\\) is high, the gradient \\(\\nabla_{\\mathbf{\\theta}_{-1}} \\ln \\pi(-1|s_t, \\mathbf{\\theta})\\) will push \\(\\mathbf{\\theta}_{-1}\\) to increase the score for action -1 when in state \\(s_t\\). Since \\(h(s, a, \\mathbf{\\theta}_a) = \\mathbf{\\theta}_a^T \\mathbf{x}(s)\\), the gradient with respect to \\(\\mathbf{\\theta}_a\\) is simply \\(\\mathbf{x}(s)\\). So, the update to \\(\\mathbf{\\theta}_{-1}\\) will be proportional to \\(\\mathbf{x}(s_t)\\). This means:\n\nIf \\(s_t\\) is very similar to \\(c_1\\) (so \\(\\phi_1(s_t)\\) is high), then \\(w_{-1,1}\\) (the weight for \\(c_1\\) and action -1) will be adjusted significantly.\nIf \\(s_t\\) is far from \\(c_1\\) (so \\(\\phi_1(s_t)\\) is near zero), then \\(w_{-1,1}\\) will be adjusted very little by this particular sample.\n\nThis means the learning process adjusts the weights for each proto point for each action based on the returns received.\n\n\nAre Probabilities Fixed When Starting with Proto Points?\nNo, the probabilities of actions are not fixed when you start with proto points.\n\nFixed: The proto points \\(c_j\\) themselves are fixed in the state space, and the basis functions \\(\\phi_j(s)\\) are fixed (their shape and location are determined at the start).\nLearned: However, the parameters \\(\\mathbf{\\theta}\\) (the weights associated with each proto point for each action) are learnable.\n\nThe idea of relying action parameter \\(\\theta\\) on fixed state representation \\(x(s)\\) is because action \\(a\\) depends on the state \\((a|s)\\), so we need to fix states representation first, use it as a basis so that action parameter can learn."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#a-monte-carlo-style-policy-gradient-algorithm",
    "href": "posts/reinforcement-learning/index.html#a-monte-carlo-style-policy-gradient-algorithm",
    "title": "Model-free Reinforcement Learning",
    "section": "A Monte Carlo Style Policy Gradient Algorithm",
    "text": "A Monte Carlo Style Policy Gradient Algorithm\nThe core idea is to increase the probability of actions that lead to high returns and decrease the probability of actions that lead to low returns.\nInitialize:\n\nFunctional form for the policy: \\(\\pi(a|s, \\mathbf{\\theta})\\) (e.g., a neural network that outputs action probabilities given a state, parameterized by \\(\\mathbf{\\theta}\\)).\nInitial parameters: \\(\\mathbf{\\theta}\\) (e.g., randomly initialized weights for a neural network).\nStep size (learning rate): \\(\\alpha\\)\n\nAlgorithm:\nFor \\(m = 1, \\dots, M\\) (for each episode):\n\nSample a trajectory under the current policy \\(\\pi(\\cdot|\\cdot, \\mathbf{\\theta})\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\) (where \\(T_m\\) is the length of the episode).\nFor \\(t = 0, \\dots, T_m - 1\\) (for each time step in the trajectory):\n\nCompute the return from time \\(t\\): \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{T_m - t - 1} r_{T_m}^m\\) (This is the total discounted reward from \\(t+1\\) until the end of the episode).\nUpdate the policy parameters: \\[\n  \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t g_t^m \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\n  \\]\n\n\nExplanation of the Update Rule:\n\n\\(\\alpha\\): The learning rate, controlling the step size of the update.\n\\(\\gamma^t\\): The discount factor raised to the power of \\(t\\). This term accounts for the discounting of future rewards, ensuring that immediate rewards have a stronger influence on early actions.\n\\(g_t^m\\): Scaling stepsize with the Monte Carlo return from time step \\(t\\). e.g.¬†If return high =&gt; big step size, if return negative =&gt; step backward\n\\(\\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\): This is the gradient of the log-probability of the action taken.\n\n\\(\\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\): The logarithm of the probability that the policy \\(\\pi\\) would choose action \\(a_t^m\\) in state \\(s_t^m\\) with current parameters \\(\\mathbf{\\theta}\\). This is very important!!! It is a normalizer for cases like a random policy \\(\\pi\\) pick a positive (but low return) action too often, leading to pushing the probability too much, so by taking the gradient of \\(ln\\) of it, it limit down it, (and also the opposite for high reward actions but the behavior policy did not pick as many time to push enough higher). These situations usually happen with initialization of that lower reward already too high, and then add up that the policy random pick it too often pushing it even further.\n\\(\\nabla_{\\mathbf{\\theta}}\\): The gradient operator with respect to the policy parameters \\(\\mathbf{\\theta}\\). This term tells us how to change \\(\\mathbf{\\theta}\\) to increase the log-probability of taking action \\(a_t^m\\) in state \\(s_t^m\\).\n\n\nIntuition:\nThe update rule essentially says: if action \\(a_t^m\\) taken in state \\(s_t^m\\) leads to a high return (\\(g_t^m\\) is large and positive), then increase the probability of taking that action in that state. If it leads to a low (or negative) return, decrease its probability. The \\(\\gamma^t\\) term ensures that actions taken earlier in the trajectory, which influence more of the subsequent rewards, are given appropriate credit.\nREINFORCE is a fundamental algorithm that demonstrates the direct optimization of a policy, laying the groundwork for more advanced policy gradient methods."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#variance-problem",
    "href": "posts/reinforcement-learning/index.html#variance-problem",
    "title": "Model-free Reinforcement Learning",
    "section": "Variance Problem",
    "text": "Variance Problem\nReturn \\(g_t^m\\) rewards the action when it is positive, and punish the action when it is negative. But some cases all actions can be positive, just some are less positive than other =&gt; it should not be encouraged.\nSolution: With baseline (e.g., \\(V_\\pi(s,w)\\)) =&gt; then we have \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t (g_t^m - b(s_t^m)) \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\] so that:\n\nif \\(g_t^m &gt; V_\\pi(s,w)\\), the scaling factor is positive =&gt; increase probability of action \\(a\\)\nif \\(g_t^m &lt; V_\\pi(s,w)\\), the scaling factor is negative =&gt; decrease probability of action \\(a\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-reinforce-gradient-policy-with-baseline",
    "href": "posts/reinforcement-learning/index.html#sum-up-reinforce-gradient-policy-with-baseline",
    "title": "Model-free Reinforcement Learning",
    "section": "Sum up: REINFORCE Gradient Policy with Baseline",
    "text": "Sum up: REINFORCE Gradient Policy with Baseline\nTo specify upfront:\n\nFunctional forms \\(\\pi(a|s, \\mathbf{\\theta})\\), \\(\\hat{v}(s, \\mathbf{w})\\)\nInitial \\(\\mathbf{\\theta}, \\mathbf{w}\\)\nStep sizes \\(\\alpha^{\\theta}, \\alpha^{w}\\)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample: \\(s_0^m, a_0^m, r_1^m \\dots a_{T_m-1}^m, r_{T_m}^m\\)\nFor \\(t = 0, \\dots, T_m - 1\\): \\[g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\]\n\n\\[\\delta \\leftarrow g_t^m - \\hat{v}(s_t^m, \\mathbf{w})\\]\n\\[\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^w \\delta \\nabla_{\\mathbf{w}} \\hat{v}(s_t^m, \\mathbf{w})\\]\n\\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^{\\theta} \\gamma^t \\delta \\nabla \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#proximal-policy-optimization-ppo",
    "href": "posts/reinforcement-learning/index.html#proximal-policy-optimization-ppo",
    "title": "Model-free Reinforcement Learning",
    "section": "Proximal Policy Optimization (PPO)",
    "text": "Proximal Policy Optimization (PPO)\nIt‚Äôs an on-policy, actor-critic method. Its main innovation lies in its clipped surrogate objective function, which aims to take the largest possible improvement step on a policy without causing a performance collapse.\nThe PPO policy objective to maximize is: \\[L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t\\left[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)\\right]\\]"
  },
  {
    "objectID": "posts/cg1/index.html",
    "href": "posts/cg1/index.html",
    "title": "Computer Graphics Foundation",
    "section": "",
    "text": "mesh ùëÄ is defined as a tripel (ùëâ, ùê∏, ùêπ)\nTopological type using betti numbers\nincident: different type, adjacent: same type\nvalence (given a vertex: number of incident edge/traingles?)\ndegree (given a face: number of incident edges?)\n\\(\\sum valences = 2edges = \\sum degrees\\) (because mostly one edge is shared by 2 vertices and 2 faces)\nwinding order is used for backface culling (when we add into the program)\n\n\n\nv -1 -1 1\nv 1 -1 1\nv -1 1 1\n...\nvt u1 v1\nvt u2 v2\nvt u3 v3\n...\nvn -1 0 0\nvn 1 0 0\nvn 0 -1 0\n...\nf 1/1/1 3/3/1 7/?/1 5/?/1\nf 2/2/2 6/6/2 8//2 4//2\n...\n\ndeep in hardware could be saved as:\n\narray: very fast for linear traversal, add and delete slower\ndouble linked-list: add and delete faster, but linear traversal slower\n\n\n\n\n\nGoal idea: resembling 2D plane everywhere on the surface:\n\neach edge is incident to 1 or 2 faces (if less: no face, if more: no clear surface definition, if 1: boundary)\neach edge is incident to 2 vertices (no rounded edge with its face in itself)\neach face is bounded by 1 loop over incident edges (if not: its a hole)\neach vertex: the incident faces and edges form a single ‚Äúfan‚Äù which is either open (on the boundary) or a closed disk (in the interior of the mesh), but not multiple fans.\n\nDS are mostly restricted on this because:\n\nReliably deterministic in operation (e.g.¬†each edge only has 1,2 faces at 2 sides which create a clear surface for traversal) =&gt; avoid ambigioiusity\nNaturally it is able to model most of 3D shapes\n\n\n\n\n\nclassify all corners in convex or concave (already knew from internal angles)\nrepeat n-3 times: (n loop)\n\none ear-cut: iterate all convex corners (corner hear means vertex and the edges at two sides): (n loop)\n\ncheck, if corner is an ear (if it contains no concave inside the border line)\nif yes, cut-off ear, reclassify adjacent corners, break this loop to go to the next eat-cut\n\n\n\n\n\n\n\nTriangulated using Ear-Cutting\n\\(2e = 3f\\) or \\(\\frac{3}{2} f = e\\) bc each faces has 3 edges, but each edge is used double\nEuler formula: \\(v-e+f=X\\), for X very small =&gt; \\(v = \\frac{1}{3} e = \\frac{1}{2} f\\)\nvalence: \\(\\sum valences = 2e = 6v\\)\n\n\n\n\n\n\n\n\nFor triangle: \\(\\hat{n} = (p_1 - p_0) \\times (p_2 - p_0)\\) and \\(A = \\frac{1}{2} \\left| p_0 \\times p_1 + p_1 \\times p_2 + p_2 \\times p_0 \\right|\\)\nFor Polygon: \\(\\hat{n} \\propto (p_0 \\times p_1) + (p_1 \\times p_2) + \\ldots + (p_{d-1} \\times p_0)\\) and \\(A = \\frac{1}{2} \\left| (p_0 \\times p_1) + (p_1 \\times p_2) + \\ldots + (p_{d-1} \\times p_0) \\right|\\)\n\nGeneral idea for Area:\n\nChoose a reference point, typically the origin (0, 0).\nIterate through each directed edges (p_i, p_j) of the polygon, calculate the signed area of the triangle it forms with the origin using the z-component of the cross product: \\(\\Delta A = \\frac{1}{2} (x_i y_j - x_j y_i)\\). (It is represented by a normal pointing inside in the winding order aka right-hand rule, some shorter, some longer will sum up to the correct direction)\nSum all these signed areas \\(A = \\sum \\Delta A\\). Concave (negative area) and Convex (positive area)\n\nAdvantage:\n\nNo need ear-cutting: O(n¬≤)\nTaking care of concave triangles\n\n\n\n\nIdea comes from Divergence Theorem: sum of all the ‚Äúsources‚Äù and ‚Äúsinks‚Äù within a volume equal to the total amount of ‚Äústuff‚Äù flowing out (or in) through the surface boundary of that volume. \\(\\iint_S \\mathbf{F} \\cdot d\\mathbf{S} = \\iiint_V (\\nabla \\cdot \\mathbf{F}) dV\\)\nWhere:\n\n\\(\\mathbf{F}\\) is the vector field.\n\\(S\\) is a closed surface.\n\\(V\\) is the volume enclosed by \\(S\\).\n\\(d\\mathbf{S}\\) is an infinitesimal outward normal vector element of the surface.\n\\(\\nabla \\cdot \\mathbf{F}\\) is the divergence of the vector field \\(\\mathbf{F}\\).\n\nIn simpler terms:\n\nFlux (\\(\\iint_S \\mathbf{F} \\cdot d\\mathbf{S}\\)): This represents the ‚Äúflow‚Äù of the vector field out of the enclosed volume through its boundary surface. Imagine a fluid flowing; the flux would be the amount of fluid passing through the surface per unit time.\nDivergence (\\(\\nabla \\cdot \\mathbf{F}\\)): This measures the ‚Äúsource‚Äù or ‚Äúsink‚Äù density of the vector field at each point within the volume. A positive divergence indicates a source (where the field is originating or expanding), and a negative divergence indicates a sink (where the field is converging or disappearing).\n\nNow we apply that to Volume, each tetrahedron volume created by each triangle \\(\\frac{1}{6} \\mathbf{p}_i \\cdot (\\mathbf{p}_j \\times \\mathbf{p}_k)\\) (also called triple product)\nThen the total volume \\(V\\) is sum of tetrahedron volume: \\(V = \\frac{1}{6} \\sum_{(ijk) \\in T} [\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k]\\)\n\n\n\nLet‚Äôs start with the volume contribution of a tetrahedron formed by the origin and a triangle \\((\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k)\\):\n\\(V_{ijk} = \\frac{1}{6} \\mathbf{p}_k \\cdot (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\n\\(\\frac{\\partial V_{ijk}}{\\partial \\mathbf{p}_k} = \\frac{\\partial}{\\partial \\mathbf{p}_k} \\left( \\frac{1}{6} \\mathbf{p}_i \\cdot (\\mathbf{p}_j \\times \\mathbf{p}_k) \\right)\\) (volume with respect to the coordinates of a specific vertex \\(\\frac{\\partial V}{\\partial \\mathbf{p}_k}\\))\n\\(\\frac{\\partial V_{ijk}}{\\partial \\mathbf{p}_k} = \\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\nUntil now what this means?\n\nIt means much the volume of that specific tetrahedron changes if we move vertex \\(\\mathbf{p}_k\\) by a small amount.\nThe result, \\(\\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\), is a vector. This vector is normal to the plane formed by \\(\\mathbf{p}_i\\) and \\(\\mathbf{p}_j\\), and its magnitude is related to the area of the triangle formed by \\(\\mathbf{p}_i\\), \\(\\mathbf{p}_j\\), and the origin.\n\nFor the total volume \\(V\\), the gradient with respect to a vertex \\(\\mathbf{p}_k\\) is the sum of the gradients of all tetrahedra one-ring around \\(\\mathbf{p}_k\\):\n\\(\\nabla_{\\mathbf{p}_k} V = \\sum_{\\text{one-ring } (\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k)} \\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\nThis gradient indicates the direction in which moving the vertex would most effectively increase the total mesh volume.\n\n\n\n\\(\\hat{n}_v \\propto \\sum_i w_i \\hat{n}_i\\)\nImplementation is interestingly reversed: because faces and vertices are jointly incident to each other\nnormals.resize(positions.size()); // init normals array with length equal to number of vertices\nforeach (fi in meshfaces) // iterate over all faces\n  nml = get_face_normal(fi); // get the normal for this face\n  foreach (corner ci in face fi) // iterate over all vertices of this face\n    normals[vi(ci)] += nml *compute_corner_weight(ci); // add the weighted normal to the corresponding normal array index\n\n\n\n\\(c_v = \\frac{1}{n_{onering}} \\sum_i p_i\\) Implementaton also a little reversed, hell yeah, because all vertices are adjacent to each other\ncentroids.resize(positions.size()); //init\nvalences.resize(positions.size());\nforeach (fi in meshfaces) //for each face\n  foreach (halfedge vi,vj in face fi) // iterate three halfedges (vi,vj)\n    centroids[vi] += positions[vj];\n    valences[vi] += 1;\n\n\n\n\n\n\n\nShells are connected components\nBoundary Loops are border/edges\n\n\n\n\nThis DS can be used to find shells three operations: * initialize UF-DS such that each element forms its own subset * union(element1, element2): Merges the sets containing the two elements. * find(element): Returns a unique identifier for the set that the element belongs to.\nAlgorithm to find shells and count shells\nint nr_comp = vertices.size(); \nUF.init(vertices.size()); // init to component/set per vertex\nforeach (fi) // iterate face\n  foreach (halfedge vi,vj in face fi) //iterate HE\n  if (UF.find(vi) != UF.find(vj))\n    UF.union(vi,vj); //unite\n    nr_comp -= 1; //reduce comp by 1"
  },
  {
    "objectID": "posts/cg1/index.html#basics",
    "href": "posts/cg1/index.html#basics",
    "title": "Computer Graphics Foundation",
    "section": "",
    "text": "mesh ùëÄ is defined as a tripel (ùëâ, ùê∏, ùêπ)\nTopological type using betti numbers\nincident: different type, adjacent: same type\nvalence (given a vertex: number of incident edge/traingles?)\ndegree (given a face: number of incident edges?)\n\\(\\sum valences = 2edges = \\sum degrees\\) (because mostly one edge is shared by 2 vertices and 2 faces)\nwinding order is used for backface culling (when we add into the program)\n\n\n\nv -1 -1 1\nv 1 -1 1\nv -1 1 1\n...\nvt u1 v1\nvt u2 v2\nvt u3 v3\n...\nvn -1 0 0\nvn 1 0 0\nvn 0 -1 0\n...\nf 1/1/1 3/3/1 7/?/1 5/?/1\nf 2/2/2 6/6/2 8//2 4//2\n...\n\ndeep in hardware could be saved as:\n\narray: very fast for linear traversal, add and delete slower\ndouble linked-list: add and delete faster, but linear traversal slower\n\n\n\n\n\nGoal idea: resembling 2D plane everywhere on the surface:\n\neach edge is incident to 1 or 2 faces (if less: no face, if more: no clear surface definition, if 1: boundary)\neach edge is incident to 2 vertices (no rounded edge with its face in itself)\neach face is bounded by 1 loop over incident edges (if not: its a hole)\neach vertex: the incident faces and edges form a single ‚Äúfan‚Äù which is either open (on the boundary) or a closed disk (in the interior of the mesh), but not multiple fans.\n\nDS are mostly restricted on this because:\n\nReliably deterministic in operation (e.g.¬†each edge only has 1,2 faces at 2 sides which create a clear surface for traversal) =&gt; avoid ambigioiusity\nNaturally it is able to model most of 3D shapes\n\n\n\n\n\nclassify all corners in convex or concave (already knew from internal angles)\nrepeat n-3 times: (n loop)\n\none ear-cut: iterate all convex corners (corner hear means vertex and the edges at two sides): (n loop)\n\ncheck, if corner is an ear (if it contains no concave inside the border line)\nif yes, cut-off ear, reclassify adjacent corners, break this loop to go to the next eat-cut\n\n\n\n\n\n\n\nTriangulated using Ear-Cutting\n\\(2e = 3f\\) or \\(\\frac{3}{2} f = e\\) bc each faces has 3 edges, but each edge is used double\nEuler formula: \\(v-e+f=X\\), for X very small =&gt; \\(v = \\frac{1}{3} e = \\frac{1}{2} f\\)\nvalence: \\(\\sum valences = 2e = 6v\\)"
  },
  {
    "objectID": "posts/cg1/index.html#geometry-analysis",
    "href": "posts/cg1/index.html#geometry-analysis",
    "title": "Computer Graphics Foundation",
    "section": "",
    "text": "For triangle: \\(\\hat{n} = (p_1 - p_0) \\times (p_2 - p_0)\\) and \\(A = \\frac{1}{2} \\left| p_0 \\times p_1 + p_1 \\times p_2 + p_2 \\times p_0 \\right|\\)\nFor Polygon: \\(\\hat{n} \\propto (p_0 \\times p_1) + (p_1 \\times p_2) + \\ldots + (p_{d-1} \\times p_0)\\) and \\(A = \\frac{1}{2} \\left| (p_0 \\times p_1) + (p_1 \\times p_2) + \\ldots + (p_{d-1} \\times p_0) \\right|\\)\n\nGeneral idea for Area:\n\nChoose a reference point, typically the origin (0, 0).\nIterate through each directed edges (p_i, p_j) of the polygon, calculate the signed area of the triangle it forms with the origin using the z-component of the cross product: \\(\\Delta A = \\frac{1}{2} (x_i y_j - x_j y_i)\\). (It is represented by a normal pointing inside in the winding order aka right-hand rule, some shorter, some longer will sum up to the correct direction)\nSum all these signed areas \\(A = \\sum \\Delta A\\). Concave (negative area) and Convex (positive area)\n\nAdvantage:\n\nNo need ear-cutting: O(n¬≤)\nTaking care of concave triangles\n\n\n\n\nIdea comes from Divergence Theorem: sum of all the ‚Äúsources‚Äù and ‚Äúsinks‚Äù within a volume equal to the total amount of ‚Äústuff‚Äù flowing out (or in) through the surface boundary of that volume. \\(\\iint_S \\mathbf{F} \\cdot d\\mathbf{S} = \\iiint_V (\\nabla \\cdot \\mathbf{F}) dV\\)\nWhere:\n\n\\(\\mathbf{F}\\) is the vector field.\n\\(S\\) is a closed surface.\n\\(V\\) is the volume enclosed by \\(S\\).\n\\(d\\mathbf{S}\\) is an infinitesimal outward normal vector element of the surface.\n\\(\\nabla \\cdot \\mathbf{F}\\) is the divergence of the vector field \\(\\mathbf{F}\\).\n\nIn simpler terms:\n\nFlux (\\(\\iint_S \\mathbf{F} \\cdot d\\mathbf{S}\\)): This represents the ‚Äúflow‚Äù of the vector field out of the enclosed volume through its boundary surface. Imagine a fluid flowing; the flux would be the amount of fluid passing through the surface per unit time.\nDivergence (\\(\\nabla \\cdot \\mathbf{F}\\)): This measures the ‚Äúsource‚Äù or ‚Äúsink‚Äù density of the vector field at each point within the volume. A positive divergence indicates a source (where the field is originating or expanding), and a negative divergence indicates a sink (where the field is converging or disappearing).\n\nNow we apply that to Volume, each tetrahedron volume created by each triangle \\(\\frac{1}{6} \\mathbf{p}_i \\cdot (\\mathbf{p}_j \\times \\mathbf{p}_k)\\) (also called triple product)\nThen the total volume \\(V\\) is sum of tetrahedron volume: \\(V = \\frac{1}{6} \\sum_{(ijk) \\in T} [\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k]\\)\n\n\n\nLet‚Äôs start with the volume contribution of a tetrahedron formed by the origin and a triangle \\((\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k)\\):\n\\(V_{ijk} = \\frac{1}{6} \\mathbf{p}_k \\cdot (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\n\\(\\frac{\\partial V_{ijk}}{\\partial \\mathbf{p}_k} = \\frac{\\partial}{\\partial \\mathbf{p}_k} \\left( \\frac{1}{6} \\mathbf{p}_i \\cdot (\\mathbf{p}_j \\times \\mathbf{p}_k) \\right)\\) (volume with respect to the coordinates of a specific vertex \\(\\frac{\\partial V}{\\partial \\mathbf{p}_k}\\))\n\\(\\frac{\\partial V_{ijk}}{\\partial \\mathbf{p}_k} = \\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\nUntil now what this means?\n\nIt means much the volume of that specific tetrahedron changes if we move vertex \\(\\mathbf{p}_k\\) by a small amount.\nThe result, \\(\\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\), is a vector. This vector is normal to the plane formed by \\(\\mathbf{p}_i\\) and \\(\\mathbf{p}_j\\), and its magnitude is related to the area of the triangle formed by \\(\\mathbf{p}_i\\), \\(\\mathbf{p}_j\\), and the origin.\n\nFor the total volume \\(V\\), the gradient with respect to a vertex \\(\\mathbf{p}_k\\) is the sum of the gradients of all tetrahedra one-ring around \\(\\mathbf{p}_k\\):\n\\(\\nabla_{\\mathbf{p}_k} V = \\sum_{\\text{one-ring } (\\mathbf{p}_i, \\mathbf{p}_j, \\mathbf{p}_k)} \\frac{1}{6} (\\mathbf{p}_i \\times \\mathbf{p}_j)\\)\nThis gradient indicates the direction in which moving the vertex would most effectively increase the total mesh volume.\n\n\n\n\\(\\hat{n}_v \\propto \\sum_i w_i \\hat{n}_i\\)\nImplementation is interestingly reversed: because faces and vertices are jointly incident to each other\nnormals.resize(positions.size()); // init normals array with length equal to number of vertices\nforeach (fi in meshfaces) // iterate over all faces\n  nml = get_face_normal(fi); // get the normal for this face\n  foreach (corner ci in face fi) // iterate over all vertices of this face\n    normals[vi(ci)] += nml *compute_corner_weight(ci); // add the weighted normal to the corresponding normal array index\n\n\n\n\\(c_v = \\frac{1}{n_{onering}} \\sum_i p_i\\) Implementaton also a little reversed, hell yeah, because all vertices are adjacent to each other\ncentroids.resize(positions.size()); //init\nvalences.resize(positions.size());\nforeach (fi in meshfaces) //for each face\n  foreach (halfedge vi,vj in face fi) // iterate three halfedges (vi,vj)\n    centroids[vi] += positions[vj];\n    valences[vi] += 1;"
  },
  {
    "objectID": "posts/cg1/index.html#connectivity-analysis",
    "href": "posts/cg1/index.html#connectivity-analysis",
    "title": "Computer Graphics Foundation",
    "section": "",
    "text": "Shells are connected components\nBoundary Loops are border/edges\n\n\n\n\nThis DS can be used to find shells three operations: * initialize UF-DS such that each element forms its own subset * union(element1, element2): Merges the sets containing the two elements. * find(element): Returns a unique identifier for the set that the element belongs to.\nAlgorithm to find shells and count shells\nint nr_comp = vertices.size(); \nUF.init(vertices.size()); // init to component/set per vertex\nforeach (fi) // iterate face\n  foreach (halfedge vi,vj in face fi) //iterate HE\n  if (UF.find(vi) != UF.find(vj))\n    UF.union(vi,vj); //unite\n    nr_comp -= 1; //reduce comp by 1"
  },
  {
    "objectID": "posts/cg1/index.html#ds-construction",
    "href": "posts/cg1/index.html#ds-construction",
    "title": "Computer Graphics Foundation",
    "section": "DS Construction",
    "text": "DS Construction\n\nAdjacent relationship: next, prev, inv\nIncident relationship: origin, loop/face, edge\n\nstruct vertex {\n  vec3d position;\n  int halfedge;  //stores only 1 HE, the rest can be found by inv(next) like below\n};\nstruct halfedge {\n  int origin,ni,ti; (pointer-based trade off sequential access vs single access vs memory)\n  int next,inv,li;\n};\nstruct loop {\n  int halfedge; //stores only 1 HE, the rest can be found by next\n  bool is_border;\n};\nstruct halfedge_mesh {\nvector&lt;vertex&gt; vertices;\nvector&lt;vec3d&gt; normals;\nvector&lt;vec2d&gt; texCoords;\nvector&lt;halfedge&gt; halfedges;\nvector&lt;loop&gt; loops;\n};\n\nSo, from an OBJ. file, what of these relationship can be directly inferred?\n\nactually everything when reading the face: origin, next, loop BUT EXCEPT FOR inv, that neads to find what face adjacent to current face\n\n\n\nVertex Circulator:\nint h0 = inv(vertexHalfEdge(vi)); //usually start with the inverse\nint hi = h0;\ndo {\n  useNeighbor(origin(hi));\n  hi = inv(next(hi)); //always inv(next)\n} while (hi != h0); //until back to beginning\n\n\nInverse Matching\n\nGoal: finding the inv pointer for every halfedge in the mesh by placing them next to each other\nIdea: Easy:\n\nSort internally v1 -&gt; v2 vs v2 -&gt; v1 in format smaller-&gt;bigger\nSort the indices along the smaller column\nEnd effect:\n\n\nHE exists twice are matched, they are internal\nHE exists once are boundary\nHE exists more than twice are non-manifold\n\nThis way we can extract Boundary Loop by for each unmatched HE, define all the inverse, and the inverse keep traversing next until it come back to the beginning"
  },
  {
    "objectID": "posts/cg1/index.html#mesh-analysis",
    "href": "posts/cg1/index.html#mesh-analysis",
    "title": "Computer Graphics Foundation",
    "section": "Mesh Analysis",
    "text": "Mesh Analysis\n\nDesign Pattern for Design Mesh Processing Algorithm\n\nCirculator (around vertex, face, vertex in face)\nTagging (mark processed elements or store id/flag)\nRegion Growing (triangle strip, compressed segmentation)\n\n\n\nWhy use Triangle Strips for Rendering?\n\nTransfer: only need to transfer n+2 vertices for a strip of n triangles (the first one all 3, after that only 1 for each new triangle)\nAccess: for triangle mesh we all know \\(f = 2v\\), triangle strip can utilize this optimum, but without strip, each face individually require separate 3 times access to their vertices. So \\(f = 2.3v\\)\nDownside? The algorithm is not always perfect (its just greedy)\n\n\n\nHow to compute Triangle Strip?\nOutput Goal: strip index per face 1. Sample some seed 2. For each seed, generate a stripification 3. Here is how to stripification: * 2 types of HE, (even - odd) using parity (0-1) depending on how the origin aligned. * Forward: prev(inv(vi)) or next(inv(vi)) * Backward: ‚Ä¶ This run alternatively‚Ä¶change parity 0-1-0-1-0-1 Until Border or Comeback to beginning 4. Choose the longest strip from seed 5. Repeat with new set of seeds\n\n\nHow to do Orientability Check?\nRegion Growing: Start at a seed to queue 1. Take the first face from queue 2. Check if orientable? 1. check if all neighboring processed faces have consistent orientation 2. If no, give up. But if yes, swap next and inv 3. Add all neighbors to queue"
  },
  {
    "objectID": "posts/exam-ml4r/index.html",
    "href": "posts/exam-ml4r/index.html",
    "title": "Machine Learning 4 Robotics",
    "section": "",
    "text": "Forward dynamics: given torque vector -&gt; angular position, velocity, acceleration at each joint\nBackward dynamics: the inverse is more interesting\n\n\n\n3 terms: Proportional Gain (Kp), Integral (Ki), Differential (Kd) * Proportional: we all know, current error strong -&gt; more gas, weak-&gt;less gas * Integral? for gradual decaying system (like car) -&gt; continuously effort a little more gas to compensate the persistent decay, otherwise P controller keep chasing it and never reach. BUT, it needs careful choose, because it is slow reaction, this is a drawback, and a term to overcome this drawback is‚Ä¶ * Differential: fast change in error: it will correct! -&gt; can react on time (sharp turn, break,‚Ä¶), BUT from my experience this should not be too high bc it can be noisy\n\n\n\n\njust generate bunch of data, then fit this data with any type regression model\nlinear or non-linear (then needs linearization around interested equilibrium like the cart pole)\n\n\n\n\nGeneral idea is like this, implemented by a dot product: \\[u(t) = K e(t) = K_x (x(t) - x_{\\text{target}}(t)) + K_\\theta \\theta(t) + K_{\\dot{x}} \\dot{x}(t) + K_{\\dot{\\theta}} \\dot{\\theta}(t)\\]\nu = mj_data.ctrl = - np.dot(gain, error)\nwe just adapt the gain K to see which one is good, but this is a manual work, so how to find the optimal gain automatically? LQR\n\n\n\n\n\n\\(J = \\int_{0}^{\\infty} (\\mathbf{s}(t)^T \\mathbf{Q} \\mathbf{s}(t) + u(t)^T R u(t)) \\, dt\\)\n\nQ (State Weighting Matrix): Penalize state deviation \\[\\mathbf{Q} = \\text{diag}([1, 10, 0, 0]) = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 10 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\]\nThis means:\n\npenalize the square of the cart‚Äôs position (\\(x^2\\)) with a weight of 1.\nheavily penalize the square of the pendulum‚Äôs angle (\\(\\theta^2\\)) with a weight of 10, indicating it‚Äôs crucial to keep the pendulum upright.\ndo not directly penalize the squared cart velocity (\\(\\dot{x}^2\\)) or pendulum angular velocity (\\(\\dot{\\theta}^2\\)) in the cost function.\n\nR (Control Weighting Matrix/Scalar): penalizes the magnitude of the control input. A larger \\(R\\) encourage smoother control, but slower. smaller \\(R\\) canverges faster, but may not always good for human and gearbox \\[R = 0.1\\] This means the square of the control input (\\(u^2\\)) is penalized with a weight of 0.1.\n\n\n\n\nwe use Lagrange dynamics by hand to define matrices A and B manually then put in\n\\(\\dot{\\mathbf{s}}(t) = \\mathbf{A}\\mathbf{s}(t) + \\mathbf{B}u(t)\\)\n\nA (State Matrix): \\[\\mathbf{A} = \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & -\\frac{g m}{M} & 0 & 0 \\\\ 0 & \\frac{g}{l}\\left(1 + \\frac{m}{M}\\right) & 0 & 0 \\end{bmatrix}\\]\nB (Input Matrix): \\[\\mathbf{B} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{k}{M} \\\\ -\\frac{k}{M l} \\end{bmatrix}\\]\n\nWhere:\n* $g$: acceleration due to gravity\n* $m$: mass of the pendulum (pole)\n* $M$: mass of the cart\n* $l$: length from the pivot to the center of mass of the pendulum\n* $k$: gear ratio, converting the control signal $u$ into a physical force $F = k \\cdot u$ applied to the cart.\n\n\n\nK (State Feedback Gain Matrix):\n\nFor a LTI-System with infinite horizon, we can set derivation of cost function = 0 and find out that \\(u(t) = - \\mathbf{K}\\mathbf{s}(t)\\) at every time point the same \\(K\\).\nFor an LTI system with a finite horizon, K is time-varying because we set extra another cost at the end of horizont(calculated via DRE).\nThis is achieved by rephrasing the cost function above, then we end up with a function including two terms, both are quadratic terms (first one totally dependent on x, the second term where x and u are involved).\nFrom the first one we found out that x and u are in linear relationship \\[\\mathbf{u}^*(t) = -\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}(t)\\]\nBut since we do not know how much is this K gain, so we also set the first term to 0, which is Algebraic Ricatti Equation, to find \\(P\\), and eventually find \\(K = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\)\nThe control.lqr(A,B,Q,R) computes this: \\[\\mathbf{K} = \\begin{bmatrix} K_x & K_\\theta & K_{\\dot{x}} & K_{\\dot{\\theta}} \\end{bmatrix}\\] \\[u(t) = - (K_x x(t) + K_\\theta \\theta(t) + K_{\\dot{x}} \\dot{x}(t) + K_{\\dot{\\theta}} \\dot{\\theta}(t))\\]\n\n\n\n\n\n\n\n\n\nFeature\nFinite-Horizon LQR\nModel Predictive Control (MPC)\n\n\n\n\nSystems\nLimited to local linearization around an equilibrium.\nCan handle non-linear system, because it is mostly just sampling different trajectories at each timestemp and choose the most optimal one\n\n\nCalculation\nCalculate once, offline\nCalculate repeatedly, online at each time step\n\n\nGain K\nPre-computed time-varying schedule \\(\\mathbf{K}(t)\\) for the full horizon. Even for finite horizont, at K(0), can calculate K(1), K(2), K(3),‚Ä¶ arranging recursive relationship in a linear equation system, then substitue back into cost function, we can even solve it using either DRE or transforming in Quadratic Format\na sequence of open-loop controls is generated, and only the first is applied. If the system is linear, can apply LQR easily then no need trajectory sampling, but if not we need sampling\n\n\n\nComputational Load | Low online load (just lookup) | High (solves an optimization problem at each step)\n\n\n\n\n\nstep 1: taking dynamics A, B as input, formulate finite horizont stateinput sequences in a dynamic programmed (matrix) way \\(X_k = \\mathbf{\\Phi} x(k) + \\mathbf{\\Gamma} U_k\\)\nstep 2: substituting into J cost function (just the same as LQR), now we have \\(J(U_k,x(k))\\) into \\(\\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}\\).\nusing QP solver, because this is the format that the solution method is known\n(only difference in MPC) - only apply first one, then re-calculate at the next step"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#pid",
    "href": "posts/exam-ml4r/index.html#pid",
    "title": "Machine Learning 4 Robotics",
    "section": "",
    "text": "3 terms: Proportional Gain (Kp), Integral (Ki), Differential (Kd) * Proportional: we all know, current error strong -&gt; more gas, weak-&gt;less gas * Integral? for gradual decaying system (like car) -&gt; continuously effort a little more gas to compensate the persistent decay, otherwise P controller keep chasing it and never reach. BUT, it needs careful choose, because it is slow reaction, this is a drawback, and a term to overcome this drawback is‚Ä¶ * Differential: fast change in error: it will correct! -&gt; can react on time (sharp turn, break,‚Ä¶), BUT from my experience this should not be too high bc it can be noisy"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#dynamics-system-id",
    "href": "posts/exam-ml4r/index.html#dynamics-system-id",
    "title": "Machine Learning 4 Robotics",
    "section": "",
    "text": "just generate bunch of data, then fit this data with any type regression model\nlinear or non-linear (then needs linearization around interested equilibrium like the cart pole)"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#exploring-a-linear-feedback-state-control-in-lti-system",
    "href": "posts/exam-ml4r/index.html#exploring-a-linear-feedback-state-control-in-lti-system",
    "title": "Machine Learning 4 Robotics",
    "section": "",
    "text": "General idea is like this, implemented by a dot product: \\[u(t) = K e(t) = K_x (x(t) - x_{\\text{target}}(t)) + K_\\theta \\theta(t) + K_{\\dot{x}} \\dot{x}(t) + K_{\\dot{\\theta}} \\dot{\\theta}(t)\\]\nu = mj_data.ctrl = - np.dot(gain, error)\nwe just adapt the gain K to see which one is good, but this is a manual work, so how to find the optimal gain automatically? LQR"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#lqr",
    "href": "posts/exam-ml4r/index.html#lqr",
    "title": "Machine Learning 4 Robotics",
    "section": "",
    "text": "\\(J = \\int_{0}^{\\infty} (\\mathbf{s}(t)^T \\mathbf{Q} \\mathbf{s}(t) + u(t)^T R u(t)) \\, dt\\)\n\nQ (State Weighting Matrix): Penalize state deviation \\[\\mathbf{Q} = \\text{diag}([1, 10, 0, 0]) = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 10 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\]\nThis means:\n\npenalize the square of the cart‚Äôs position (\\(x^2\\)) with a weight of 1.\nheavily penalize the square of the pendulum‚Äôs angle (\\(\\theta^2\\)) with a weight of 10, indicating it‚Äôs crucial to keep the pendulum upright.\ndo not directly penalize the squared cart velocity (\\(\\dot{x}^2\\)) or pendulum angular velocity (\\(\\dot{\\theta}^2\\)) in the cost function.\n\nR (Control Weighting Matrix/Scalar): penalizes the magnitude of the control input. A larger \\(R\\) encourage smoother control, but slower. smaller \\(R\\) canverges faster, but may not always good for human and gearbox \\[R = 0.1\\] This means the square of the control input (\\(u^2\\)) is penalized with a weight of 0.1.\n\n\n\n\nwe use Lagrange dynamics by hand to define matrices A and B manually then put in\n\\(\\dot{\\mathbf{s}}(t) = \\mathbf{A}\\mathbf{s}(t) + \\mathbf{B}u(t)\\)\n\nA (State Matrix): \\[\\mathbf{A} = \\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & -\\frac{g m}{M} & 0 & 0 \\\\ 0 & \\frac{g}{l}\\left(1 + \\frac{m}{M}\\right) & 0 & 0 \\end{bmatrix}\\]\nB (Input Matrix): \\[\\mathbf{B} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\frac{k}{M} \\\\ -\\frac{k}{M l} \\end{bmatrix}\\]\n\nWhere:\n* $g$: acceleration due to gravity\n* $m$: mass of the pendulum (pole)\n* $M$: mass of the cart\n* $l$: length from the pivot to the center of mass of the pendulum\n* $k$: gear ratio, converting the control signal $u$ into a physical force $F = k \\cdot u$ applied to the cart.\n\n\n\nK (State Feedback Gain Matrix):\n\nFor a LTI-System with infinite horizon, we can set derivation of cost function = 0 and find out that \\(u(t) = - \\mathbf{K}\\mathbf{s}(t)\\) at every time point the same \\(K\\).\nFor an LTI system with a finite horizon, K is time-varying because we set extra another cost at the end of horizont(calculated via DRE).\nThis is achieved by rephrasing the cost function above, then we end up with a function including two terms, both are quadratic terms (first one totally dependent on x, the second term where x and u are involved).\nFrom the first one we found out that x and u are in linear relationship \\[\\mathbf{u}^*(t) = -\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}(t)\\]\nBut since we do not know how much is this K gain, so we also set the first term to 0, which is Algebraic Ricatti Equation, to find \\(P\\), and eventually find \\(K = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\)\nThe control.lqr(A,B,Q,R) computes this: \\[\\mathbf{K} = \\begin{bmatrix} K_x & K_\\theta & K_{\\dot{x}} & K_{\\dot{\\theta}} \\end{bmatrix}\\] \\[u(t) = - (K_x x(t) + K_\\theta \\theta(t) + K_{\\dot{x}} \\dot{x}(t) + K_{\\dot{\\theta}} \\dot{\\theta}(t))\\]\n\n\n\n\n\n\n\n\n\nFeature\nFinite-Horizon LQR\nModel Predictive Control (MPC)\n\n\n\n\nSystems\nLimited to local linearization around an equilibrium.\nCan handle non-linear system, because it is mostly just sampling different trajectories at each timestemp and choose the most optimal one\n\n\nCalculation\nCalculate once, offline\nCalculate repeatedly, online at each time step\n\n\nGain K\nPre-computed time-varying schedule \\(\\mathbf{K}(t)\\) for the full horizon. Even for finite horizont, at K(0), can calculate K(1), K(2), K(3),‚Ä¶ arranging recursive relationship in a linear equation system, then substitue back into cost function, we can even solve it using either DRE or transforming in Quadratic Format\na sequence of open-loop controls is generated, and only the first is applied. If the system is linear, can apply LQR easily then no need trajectory sampling, but if not we need sampling\n\n\n\nComputational Load | Low online load (just lookup) | High (solves an optimization problem at each step)"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#finite-horizont-lqr-can-also-be-an-mpc",
    "href": "posts/exam-ml4r/index.html#finite-horizont-lqr-can-also-be-an-mpc",
    "title": "Machine Learning 4 Robotics",
    "section": "",
    "text": "step 1: taking dynamics A, B as input, formulate finite horizont stateinput sequences in a dynamic programmed (matrix) way \\(X_k = \\mathbf{\\Phi} x(k) + \\mathbf{\\Gamma} U_k\\)\nstep 2: substituting into J cost function (just the same as LQR), now we have \\(J(U_k,x(k))\\) into \\(\\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}\\).\nusing QP solver, because this is the format that the solution method is known\n(only difference in MPC) - only apply first one, then re-calculate at the next step"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#value-based-2-steps",
    "href": "posts/exam-ml4r/index.html#value-based-2-steps",
    "title": "Machine Learning 4 Robotics",
    "section": "Value-based (2 steps)",
    "text": "Value-based (2 steps)\n\nValue Iteration approach (we have \\(p(s', r \\mid s, a)\\))\nEach state value \\(V\\), or state-action value \\(Q\\) can be represented RECURSIVELY by immediate reward R + neighboring state-action values weighted by transition actions propability distribution.\nKeypoint Utilizing this Bellman optimality recursion, immediate reward R, they can help us converging to optimal \\(V^*\\) and \\(Q^*\\) by Value Iteration: that starts with random \\(V\\), \\(Q\\), but repeatedly sweeping Bellman optimality equation to all states/state-action pairs.\npolicy = lambda s: [0.25,0.25,0.25,0.25]\nV = np.zeros((4,4))\nQ_pi = np.zeros((4,4,4))\n\nfor steps in range(200):\n    for state_idx in range(16):\n        action_probs = policy(state)\n        subseq_states = np.clip(state+directions,0,3)\n        V_subseq_states = np.array([V_pi[tuple(idx)] for idx in subseq_states])\n        V_pi[state] = np.sum(action_probs*(rewards+gamma*V_subseq_states))\n        Q_pi[state] = rewards + gamma * V_subseq_states\n\\[V^*(s) = \\max_{a \\in \\mathcal{A}} \\left[ R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s, a)(s') V^*(s') \\right]\\]\n\\[Q^*(s, a) = R(s, a) + \\gamma \\mathbb{E}_{s' \\sim P(s,a)}[V^*(s')]\\]\nIn the end when \\(Q^*\\) are converged, we can find a deterministic \\(\\pi^*(s)\\) via:\n\\[\\pi^*(s) = \\arg\\max_{a \\in \\mathcal{A}} Q^*(s, a)\\]\n\n\nSampling Approach (no access to \\(p(s', r \\mid s, a)\\))\n2 steps: (state, action) -&gt; (value) -&gt; argmax deterministic (action)\nevery model-free algorithms has to work based on Monte Carlo for trajectory sampling.\n\n1. Monte Carlo\n\nsample full trajectories\nupdate at the end of each trajectory in reverse\nepsilon-greedy =&gt; Q* != Q_pi. epsilon greedy to sample actions, which are mostly only suboptimal and actually leads to longer trajectory\n\nStart with a random value function: \\(V(s) = \\frac{1}{|S|}\\)\nCollect sampling trajectories \\(M\\) trajectory samples: \\[\ns_0^m \\quad r_1^m \\quad s_1^m \\quad \\cdots \\quad s_{T_m}^m \\qquad m = 1, \\ldots, M\n\\]\n\nFor each trajectory\n\nFor each state in that trajectory\nslowly update along that trajectory using learning rate: \\[\n  V(s_t^m) \\leftarrow V(s_t^m) + \\alpha \\left( g_t^m - V(s_t^m) \\right)\n  \\] return is calculated backward and trace back + adding gamma:\n\n\\[\n  g_\\tau^m = R_{t+1}^m + \\gamma R_{t+2}^m + \\gamma^2 R_{t+3}^m + \\dots + \\gamma^{T - t - 1} R_T^m\n  \\]\n\nDownside:\n\nnearer to the beginning of a trajectory gets higher return but it should be the opposite, where nearer states (more correlative) to terminal state. Because looking closely at return \\(G\\) is actually just like \\(v_\\pi(s)\\), but \\(G\\) is exclusively binded to the trajectory\nUpdate rules not generalized enough because the reward could only update within the trajectory. For e.g.¬†there is a state that is very near the goal, but somehow random sampling only sample it leading to negative terminal =&gt; not correctly negatively updated\n\nfor episode in range(total_episodes):\n  while step&lt;max_steps_pr_episode:\n    states.append(s)\n    a = epsilon_greedy_Q_policy(s, Q_pi_hat)  # Pick action.\n    actions.append(a)\n    next_s, r, terminal, _ = env.step(a)  # Take a step in the environment.\n\n  while (step goes backward):\n    G = gamma * G + rewards[-1]\n    rewards = rewards[:-1] #pop out the last step\n    s = states[-1]\n    states = states[:-1]\n    a = actions[-1]\n    actions = actions[:-1]\n    Q_pi_hat[tuple(s)][a] = (1-alpha)*Q_pi_hat[tuple(s)][a] + alpha * G #kinda an average between old and new\n\n\n\n2. Temporal Difference (Q-Learning, SARSA)\n\nsimilar to Monte Carlo by sampling a lot of trajectories, but update:\n\nin between directly without waiting for trajectory end\nbootstrapping the action-state value of the successor state\n\n\n\nOff-policy (Q-Learning) (use Bellman Optimality)\nUpdate rule: immediate reward + difference between the best next successor state-action value and current \\[Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right)\\]\nThe main keypoint here is that: the best next successor state-action value might not be the one chosen from that sampled trajectory =&gt; update policy != sampling policy\n\n\nOn-policy (SARSA) (not use Bellman Optimality)\nUpdate rule: immediate reward + difference between the actual next successor state-action from that sampled trajectory =&gt; update binded to sampling policy \\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n\\]\nEnd effect of Temporal Difference:\n\nUpdate not trajectory dependent like MC =&gt; more balanced update\nCan update realtime (no need reverse order)\nthe one near the terminal will directly rewarded the most with the most weight, because the term \\(\\gamma \\max_{a} Q(s_{t+1}, a)\\) is then the terminal reward. Sometimes people actually expand this TD(n) so that the last n-states could get even directly to terminal state.\n\nfor i_episode in range(total_episodes):\n    while step&lt;max_steps_per_episode:\n      a = epsilon_greedy_Q_policy(s_disc, Q=Q, eps=eps)  # Pick action.\n      next_s, r, terminal, truncated, _ = env.step(a)  # Take a step in the environment.\n      score += r\n      delta = r + gamma * max(Q[tuple(next_s_disc)]) - Q[tuple(s_disc)][a]\n      Q[tuple(s_disc)][a] += alpha * delta  # Q-learning update.\n\n\n\n\n3. Function Approximation DQN\nGoal: Interpolating Q-Values using a DNN\n\nMSE Loss function between the network calculation and (linear regression)\nThe ground-truth from bootstrapping the max of the next step from the table (Bellman-Optimality)\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\n\nTechnical tricks in implementation:\n\nEach transition separately using replay buffer stores each transition (s, a, s+1, r+1, done?), instead of training in a consecutive sequence like from sampling\nRewards come from the environment, Gymnasium give the reward directly =&gt; use this in Bellman Optimality to calculate Ground Truth\n\nclass QNetwork(nn.Module):\n  # input state space representation, output action space representation\n\nfor i_episode in range(1, n_episodes+1):\n    for t in range(max_t):\n        action = agent.act(state, eps) # Epsilon-greedy\n        next_state, reward, terminated, truncated, _ = env.step(action)\n        agent.step(state, action, reward, next_state, terminated)\n\ndef step(self, state, action, reward, next_state, done):\n    self.memory.add(state, action, reward, next_state, done)\n    # learn update_every timestep\n    self.t_step = (self.t_step + 1)  \n        if (enough samples for a batch):\n            experiences = self.memory.sample()\n            self.learn(experiences, gamma)\n\ndef learn(self, experiences, gamma):\n    states, actions, rewards, next_states, dones = experiences\n\n    ### Calculate target value from bellman equation bootstrapping the next state value in this trajectory\n    q_targets_next = self.qnetwork_target(next_states)\n    q_targets = rewards + gamma * q_targets_next * (1 - dones)\n\n    ### Calculate expected value from local network\n    q_expected = self.qnetwork_local(states).gather(1, actions)\n\n    loss = F.mse_loss(q_expected, q_targets)\n    # update"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#policy-search-1-step",
    "href": "posts/exam-ml4r/index.html#policy-search-1-step",
    "title": "Machine Learning 4 Robotics",
    "section": "Policy Search (1 step)",
    "text": "Policy Search (1 step)\n1 Step: (state)-&gt;(probabilistic action policy)\n\nPolicy Gradients (First Order)\nThis one is Monte Carlo style, it waits until the end then update \\(\\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\) backwards\nObjective function is still in general for every (action|state).\nFor each sampled trajectory:\nFor each (state,action) in the trajectory:\nCalculate return backward: \\[g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\]\nAdvantage function to avoid variance problem:\n\\[\\delta \\leftarrow g_t^m - \\hat{v}(s_t^m, \\mathbf{w})\\]\nBut update only Update this specific policy for that (state,action) \\(\\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\) using gradient descent. Plug in (state,action) to quantize and update theta: \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^{\\theta} \\gamma^t \\delta \\nabla \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\]\nEnd effect:\n\ndirectly learn the behavior instead of state-action value =&gt; more data efficient? because only 1 step directly to calculate action\nusing gradient descent is classic (can use all relevant techniques)\n\nDisadvantage:\n\nlocal minimum\non-policy\nPPO adding clipping into loss function to avoid learning too much\n\nimplementation similar to Q-learning\nclass Policy_Network(nn.Module):\n  #input state space, output action space\n\nfor episode in range(total_num_episodes):\n    while not done:\n        action = agent.act(obs)\n        obs, reward, terminated, truncated, info = env.step(action)\n        agent.rewards.append(reward)\n\n    agent.learn() #finish the trajectory then start to learn\n\ndef learn(self):\n    #calculate G for each timestep in the trajectory\n    #calculate G backwards in Monte Carlo style\n    for R in self.rewards[::-1]:\n        running_g = R + self.gamma * running_g\n        gs.insert(0, running_g)\n    deltas = torch.tensor(gs)\n\n    loss = 0\n    # this time we define loss by ourselves\n    # now loss for each timestep in the trajectory\n    for log_prob, delta in zip(self.probs, deltas):\n        loss += log_prob.mean() * delta * (-1)\n\n    # Update the policy network\n\n\nBayesian Optimization & Gaussian Processes (Zero-order)\n\nObjective function as above, Goal: find theta that funciton‚Äôs min/max but:\n\nMany optimum =&gt; cannot use gradient descent\nEach evaluation step is expensive (could not sample so much)\nGoal of GP: find posterior mean, variance\n\nSolution: GP is a probability distribution over all possible function, all seen sampled data build together a join distribution\n\nGoal: posterior mean & variance of a new datapoint given prior some limited samples!!!\nKey Knowledge to lean on: Covariance Matrix of jointly multivariate Gaussian distribution A,B where we already know how to calculate posterior mean and covariance of (A|B)\nFormulate (yt,f) in that same covariance matrix structure =&gt; we can calculate these posterior mean/covariance of (f|yt) in exactly the same way\nCovariance matrix based on the kernel function (RBF for Gaussian, or also be Euclidian distance) K_ij = k(xi, xj), calculate these for all pair of points and put it in the cov matrix\nBUT‚Ä¶ where to take a new datapoint? apply acquisition function on random 1000 new datapoints (balance exploitation & exploration)! (EI: where variance & mean are the highest)\nMultiple Objective BO actually easy, (e.g.¬†best material: each fixed theta give different function with different max on the graph, test many theta to find many max points for each function =&gt; use Pareto)\n\n\nPro:\n\nMany optimum =&gt; cannot use gradient descent\nEach evaluation step is expensive (could not sample so much)\n\nCons:\n\nDifficult to scale to high-dimensional input space\nComputationally expensive because (distance function, acquisition function sampling, covariance matrix, needs to calculate a lot)\nQuality of the model dependent from use of appropriate kernel\nNo guarrantee a REAL max\n\nfor i in range(1):\n    y_next = true_function(x_next) #evaluate/sample\n    bayes_opt.update_model(x_next, y_next) #update gaussian process\n    x_next = bayes_opt.get_candidate(aq_func) #choose next data to sample\n\ndef update_model(self, x, y):\n    self.x_samples = np.append(self.x_samples, x)\n    self.y_samples = np.append(self.y_samples, y)\n    self.surrogate_model.fit(self.x_samples, self.y_samples) #using true function to update the GP covariance matrix\n\nself.surrogate_model = GaussianProcess(kernel=lambda x1, x2: np.exp(-.5 * np.subtract.outer(x1, x2)**2))\n\ndef get_candidate(self, acquisition_function, num_candidates=1000):\n    x_canditates = np.linspace(self.bounds[0], self.bounds[1], num_candidates)\n    mu,sigma = self.surrogate_model.predict(x_canditates) #calculate variance and mean from all other data\n    aqf_values = acquisition_function(mu, sigma, np.max(self.y_samples))\n    best_index = np.argmax(aqf_values) #choose the one with highest EI\nCartPole Example: find the set of gain K, each time evaluate in environment, and keep finding"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#mpc-known-forward-dynamics",
    "href": "posts/exam-ml4r/index.html#mpc-known-forward-dynamics",
    "title": "Machine Learning 4 Robotics",
    "section": "MPC (known Forward Dynamics)",
    "text": "MPC (known Forward Dynamics)\n\nLQR-based MPC for Linear model and quadratic cost function\nGoal: Bring back to LQR problem to solve as QR * We have to know desired states * stack all sequences of action states and outputs into vectors * why can we do that? because we have a model given * Substitue back into the cost function, and represent it in a quadratic function format that we know how to solve (find U) numerically * Solve Quadratic Function\n\n\nGeneral MPC for Non-linear model vs Non-quadratic cost function\n\nWe have to know desired states\nSampling (try out different input with that dynamic model in the future) using the given dynamics\nChoose the one argmin the quadratic cost using gradient descent or Gaussian Processes (easy: desired states = zeros - actually computed)^2\nCEM: In each iteration, it samples action sequences from the distribution, evaluates them using the cost function, and then fits the distribution to the best performing sequences. This process is repeated until convergence.\n\n# Sampling-Based Planner\ndef plan(state, desired_state, dynamics_model, objective_function, cost_weights, horizon = 50, num_candidates=20000):\n    action_sequences = torch.distributions.uniform.Uniform(-1,1).sample((num_candidates,horizon)).to(state.device)\n    costs = evaluate_action_sequences(state, action_sequences, desired_state, dynamics_model, objective_function, cost_weights)\n    best_cand = costs.argmin()\n    return action_sequences[best_cand,0]\n\n# calculate a trajectory and evaluate it using the given objective function (quadratic cost)\ndef evaluate_action_sequences(state, action_sequences, desired_state=torch.zeros(4), dynamics_model=predict_next_states, objective_function=quadratic_cost, cost_weights = torch.tensor([1.,2.,0.,0.])):\n    trajectories= calculate_trajectories(torch.tile(state,(len(action_sequences),1)), action_sequences, dynamics_model)\n    return objective_function(trajectories, desired_state, cost_weights).sum(dim=1)\n\ndef predict_next_states(states, actions):\n\n    # Non-linear dynamics for the cart-pole system\n    def equations_of_motion(xs, thetas, xs_dot, thetas_dot, actions):\n        # define all the dynamics in here\n        return xs_dot, thetas_dot, xs_dot_dot, thetas_dot_dot\n\n    # find the next state using simple Euler's method\n    xs_dot, thetas_dot, xs_dot_dot, thetas_dot_dot = equations_of_motion(xs, thetas, xs_dot, thetas_dot, actions)\n    xs_dot += xs_dot_dot * dt\n    thetas_dot += thetas_dot_dot * dt\n    xs += xs_dot * dt\n    thetas += thetas_dot * dt\n\n    return torch.stack([xs, thetas, xs_dot, thetas_dot]).T"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#model-based-learning-in-general",
    "href": "posts/exam-ml4r/index.html#model-based-learning-in-general",
    "title": "Machine Learning 4 Robotics",
    "section": "Model-based Learning in general",
    "text": "Model-based Learning in general\nCompare the performance of Model-Based RL and Model-Free RL agents (DQN, REINFORCE, AC,‚Ä¶ are Model-Free)\nModel-free:\n\nconverge guarrantee\nsimple implementation\nlight computation (lighter iteration)\nBUT needs A LOT of interaction to learn (not data efficient)=&gt; converge slower (many iteration)\n\nModel-based:\n\nmore data efficient, it learns from every data it rolled out\nbut can converge quickly (not many iteration) (data efficient)\nBUT: A LOT of computation at each timestep (bc of rollout sampling planner (longer iteration))\nBUT: MB needs a dynamic model in advance, which is very difficult to have!\nBUT converge guarrantee, because sampled planning"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#learn-dynamics-unknown-dynamics",
    "href": "posts/exam-ml4r/index.html#learn-dynamics-unknown-dynamics",
    "title": "Machine Learning 4 Robotics",
    "section": "Learn Dynamics (unknown Dynamics)",
    "text": "Learn Dynamics (unknown Dynamics)\n\nProblem: there are many states that can be presented differently, but in the sense of the task it is the same, so we do not want to overfit or learn irrelevant things =&gt; wrong behavior\nSolution: Autoencoder, pack information ### Simple Dynamics Network\nsimilar to DQN, but input (state, action) -&gt; output (next state)\nalso similar, each transition of the trajectory is saved into replay buffer, then learn each transition separately\nSo yeah: Plan =&gt; separate transition to replay buffer =&gt; learn each\n\nfor i in range(100):\n    while not done:\n        action = agent.act(state)\n        next_state, reward, terminal, truncated, _ = env.step(action)\n        agent.step(state.astype(np.float32), action.astype(np.float32), reward, next_state.astype(np.float32), done)\n        state = next_state\n\n    def step(self, state, action, reward, next_state, done): #next_state is ground truth, input is current state, action\n        # Learn every update_every time steps.\n        if self.t_step % self.update_every == 0:\n            if self.t_step &gt; self.init_period:\n                for _ in range(self.update_steps):\n                    self.learn()\n        self.t_step += 1\n\n    def learn(self): # mean squared error to learn the (next state)\n        self.model.train()\n        td = self.replay_buffer.sample(self.batch_size)\n        next_state_predcited = self.model(td['state'], td['action'])\n        loss = F.mse_loss(next_state_predcited, td['next_state'])\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n# Neural Network as forward dynamics and reward model\nclass ForwardDynamicsModel(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim = 256):\n        super(ForwardDynamicsModel, self).__init__()\n        self.fc1 = nn.Linear(state_dim+action_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, state_dim)\n\nVAE-based (Dreamer)\nGoal: \\[\\mathcal{L}(\\theta, \\phi, \\mathbf{x}) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z}))\\]\nThe VAE loss combines two terms:\n\nReconstruction Loss: Measures how well the decoder reconstructs the input from the latent code (actually MSE derived from Gaussian distribution loss).\nKL Divergence: Bringing learned latent distribution to be close to each other (and also standard normal distribution) =&gt; smoothness and no holes\n\n\nActual Dreamer (2 networks)\nGoal: plan =&gt; separate transition =&gt; bring to a new encoded space, train on 2 networks:\n\nQ-Network (encoded latent space)-&gt;(action): Purpose to generalized the STATE actually the same as Q-Network, but the input is not state, it is encoded image (env observation), Ground Truth taken from table as below (Loss can be MSE). \\[\n  Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n  \\]\nReconstruction-Network (VAE): MSE loss to make sure the latent space we learned was correct, reconstruction loss also MSE for each pixel\n\nfor step in pbar:\n    # Choose an action based on the current state\n    action = vae_dqn_agent.act(state, explore=True)\n    # Take the chosen action in the environment\n    next_state, reward, terminal, truncated, _ = env.step(action)\n    # Store the experience in the agent's replay buffer\n    vae_dqn_agent.store_experience(tensordict of action, reward, next_pixel, dont)\n    # Update the agent's Q-network\n    vae_dqn_agent.update()\n\ndef act(self, state, explore=False): \n    #either random or just take the most rewarded action from Q-network\n\ndef update(self): #update both 2 networks at the same time\n    self.update_vae()\n    self.update_dqn()\n\ndef update_vae(self):\n    # Encode states using VAE\n    mu, log_var = self.vae.encode(states_norm)\n    z = self.vae.reparameterize(mu, log_var)\n    \n    reconstructed_states = self.vae.decode(z)\n\n    loss = self.vae_loss_fn(states_norm, reconstructed_states, mu, log_var)\n    # then update using optimize\n    ...\n\ndef update_dqn(self):\n    ...\n    q_values = self.q_network(latent_states).gather(1, actions.unsqueeze(1))\n    next_q_values = self.target_network(latent_next_states).max(1)[0].detach()\n    target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n\n    loss = self.q_loss_fn(q_values, target_q_values.unsqueeze(1))\n    # then update using optimizer\n    ..."
  },
  {
    "objectID": "posts/exam-ml4r/index.html#imitation-learning-behavior-cloning",
    "href": "posts/exam-ml4r/index.html#imitation-learning-behavior-cloning",
    "title": "Machine Learning 4 Robotics",
    "section": "Imitation Learning / Behavior CLoning",
    "text": "Imitation Learning / Behavior CLoning\n\nActually a Supervised DNN (input:state) -&gt; (output:action), kinda easy\nWe can also extend this network to VAE+Actor+Critic Networks, which I think that is BCQ does\n\nfor epoch in range(num_epochs):\n        for batch in self.replay_buffer:\n            observations, target_actions = batch['observations'], batch['actions'] #sample input & output from dataset\n            actions = self.model(observations)\n            loss = self.loss(actions, target_actions) #MSELoss\n            self.optimizer.zero_grad()\n            loss.backward()\n\nbut, can we use this data to do a reinforcement learning? - No, because we should let RL explore to regularize the environment, just learning from this data makes it does not know what to do in unseen state distributional shift, it infact overestimate because it has never seen a bad example before\n=&gt; Dagger"
  },
  {
    "objectID": "posts/exam-ml4r/index.html#inversed-rl",
    "href": "posts/exam-ml4r/index.html#inversed-rl",
    "title": "Machine Learning 4 Robotics",
    "section": "Inversed RL",
    "text": "Inversed RL\n\ngiven demonstration\nlearn rewards function\nthen RL back to explore and find optimal policy"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html",
    "href": "posts/bayesian-optimization/index.html",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "\\(P(f | \\mathcal{D}) \\propto P(\\mathcal{D} | f) P(f)\\) is a real multiplication of probability densities. However, when dealing with functions and multivariate Gaussians, this multiplication is implicitly handled by the properties of joint and conditional Gaussian distributions.\n\n\nThe key to the GP‚Äôs tractability is that while it‚Äôs a distribution over infinite-dimensional functions, any finite collection of function values drawn from a GP jointly follow a multivariate Gaussian distribution.\nConsider our observed input points \\(\\mathbf{X}_t = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_t\\}\\) and a new test input point \\(\\mathbf{x}^*\\). Under the GP prior, the vector of true function values at these points, \\(\\mathbf{f}_t = [f(\\mathbf{x}_1), \\dots, f(\\mathbf{x}_t)]^T\\) and unobserved \\(f^* = f(\\mathbf{x}^*)\\), has a joint multivariate Gaussian distribution:\n\\[\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}, \\begin{pmatrix} \\mathbf{K}_t & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix} \\right)\\]\nHere:\n\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) is the prior mean at observed points.\n\\(\\mathbf{K}_t\\) is the \\(t \\times t\\) covariance matrix of the observed points, where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_* = [k(\\mathbf{x}^*, \\mathbf{x}_1), \\dots, k(\\mathbf{x}^*, \\mathbf{x}_t)]^T\\) is the \\(t \\times 1\\) vector of covariances between the test point and observed points.\n\\(k(\\mathbf{x}^*, \\mathbf{x}^*)\\) is the prior variance at the test point itself.\n\nThis joint prior distribution for \\(\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix}\\) represents the \\(P(f)\\) term in Bayes‚Äô Theorem. We do not know what that exactly is, but it does tell us about the plausible relationships between function values at observed and unobserved locations before we see any data.\n\n\n\nWe observe the data \\(\\mathcal{D}_t = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^t\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\). This can be written in vector form as:\n\\[\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\]\nwhere \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 \\mathbf{I})\\).\nThe likelihood \\(P(\\mathcal{D}_t | f)\\) (or more precisely, \\(P(\\mathbf{y}_t | \\mathbf{f}_t)\\)) is a Gaussian centered at the true function values \\(\\mathbf{f}_t\\):\n\\[P(\\mathbf{y}_t | \\mathbf{f}_t) = \\mathcal{N}(\\mathbf{y}_t | \\mathbf{f}_t, \\sigma_n^2 \\mathbf{I})\\]\nThis explicitly defines the \\(P(\\mathcal{D}|f)\\) term (using \\(\\mathbf{f}_t\\) as the ‚Äúspecific function‚Äù part for the observed data).\n\n\n\nNow, the ‚Äúmultiplication‚Äù that yields the posterior GP is achieved by a fundamental property of multivariate Gaussian distributions:\nIf you have two random variables (or vectors of variables) \\(A\\) and \\(B\\) that are jointly Gaussian (note that \\(\\Sigma = Cov\\)):\n\\[\\begin{pmatrix} A \\\\ B \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_A \\\\ \\boldsymbol{\\mu}_B \\end{pmatrix}, \\begin{pmatrix} \\boldsymbol{\\Sigma}_{AA} & \\boldsymbol{\\Sigma}_{AB} \\\\ \\boldsymbol{\\Sigma}_{BA} & \\boldsymbol{\\Sigma}_{BB} \\end{pmatrix} \\right)\\]\nThen, the conditional distribution of \\(B\\) given \\(A\\) (i.e., \\(P(B|A)\\)) is also Gaussian with:\n\nConditional Mean: \\(\\boldsymbol{\\mu}_{B|A} = \\boldsymbol{\\mu}_B + \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} (\\mathbf{A} - \\boldsymbol{\\mu}_A)\\)\nConditional Covariance: \\(\\boldsymbol{\\Sigma}_{B|A} = \\boldsymbol{\\Sigma}_{BB} - \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} \\boldsymbol{\\Sigma}_{AB}\\)\n\nHow this maps to GPs:\n\nForm the Joint Distribution of \\((\\mathbf{y}_t, f^*)\\) under the Prior: We need the joint prior distribution of our observed outputs \\(\\mathbf{y}_t\\) and our unobserved test point function value \\(f^*\\). We know \\(\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\). Since \\(\\mathbf{f}_t\\) and \\(f^*\\) are jointly Gaussian (from the GP prior) and \\(\\boldsymbol{\\epsilon}\\) is Gaussian (noise), their sum is also jointly Gaussian. The joint prior mean of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is \\(\\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}\\). The joint prior covariance of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is:\n\\[\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\right) = \\begin{pmatrix} \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I} & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix}\\]\n(Here, the \\(\\sigma_n^2 \\mathbf{I}\\) term comes from the noise added to \\(\\mathbf{f}_t\\) in the \\(\\mathbf{y}_t\\) block).\nConditioning (Applying Bayes‚Äô Rule): Now, we have observed \\(\\mathbf{y}_t\\). We want to find the posterior distribution of \\(f^*\\) given \\(\\mathbf{y}_t\\), which is \\(P(f^* | \\mathbf{y}_t)\\). Using the conditional Gaussian formulas, let:\n\n\\(A = \\mathbf{y}_t\\) (our observed data)\n\\(B = f^*\\) (the function value we want to predict)\n\\(\\boldsymbol{\\mu}_A = \\mathbf{m}_t\\)\n\\(\\boldsymbol{\\mu}_B = m(\\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AA} = \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\)\n\\(\\boldsymbol{\\Sigma}_{BB} = k(\\mathbf{x}^*, \\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AB} = \\mathbf{k}_*\\)\n\\(\\boldsymbol{\\Sigma}_{BA} = \\mathbf{k}_*^T\\)\n\nPlugging these into the conditional mean and covariance formulas gives exactly the GP posterior predictive mean and variance:\nPosterior Mean \\(\\mu_t(\\mathbf{x}^*)\\):\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\]\nPosterior Variance \\(\\sigma_t^2(\\mathbf{x}^*)\\):\n\\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\n\nIn essence:\nThe ‚Äúmultiplication‚Äù \\(P(\\mathcal{D} | f) P(f)\\) is handled internally by the mathematical properties of Gaussian distributions. The GP framework sets up a joint Gaussian prior over all relevant function values (observed and unobserved). The likelihood then specifies how our observed data \\(\\mathbf{y}_t\\) relates to the true function values \\(\\mathbf{f}_t\\). By conditioning this joint prior on the observed data \\(\\mathbf{y}_t\\), we directly derive the exact posterior distribution for the unobserved function values \\(f^*\\), which turns out to also be Gaussian with the mean and variance formulas."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-gp-prior-pf-joint-gaussian-over-all-relevant-points",
    "href": "posts/bayesian-optimization/index.html#the-gp-prior-pf-joint-gaussian-over-all-relevant-points",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "The key to the GP‚Äôs tractability is that while it‚Äôs a distribution over infinite-dimensional functions, any finite collection of function values drawn from a GP jointly follow a multivariate Gaussian distribution.\nConsider our observed input points \\(\\mathbf{X}_t = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_t\\}\\) and a new test input point \\(\\mathbf{x}^*\\). Under the GP prior, the vector of true function values at these points, \\(\\mathbf{f}_t = [f(\\mathbf{x}_1), \\dots, f(\\mathbf{x}_t)]^T\\) and unobserved \\(f^* = f(\\mathbf{x}^*)\\), has a joint multivariate Gaussian distribution:\n\\[\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}, \\begin{pmatrix} \\mathbf{K}_t & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix} \\right)\\]\nHere:\n\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) is the prior mean at observed points.\n\\(\\mathbf{K}_t\\) is the \\(t \\times t\\) covariance matrix of the observed points, where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_* = [k(\\mathbf{x}^*, \\mathbf{x}_1), \\dots, k(\\mathbf{x}^*, \\mathbf{x}_t)]^T\\) is the \\(t \\times 1\\) vector of covariances between the test point and observed points.\n\\(k(\\mathbf{x}^*, \\mathbf{x}^*)\\) is the prior variance at the test point itself.\n\nThis joint prior distribution for \\(\\begin{pmatrix} \\mathbf{f}_t \\\\ f^* \\end{pmatrix}\\) represents the \\(P(f)\\) term in Bayes‚Äô Theorem. We do not know what that exactly is, but it does tell us about the plausible relationships between function values at observed and unobserved locations before we see any data."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-likelihood-pmathcald-f-adding-gaussian-noise",
    "href": "posts/bayesian-optimization/index.html#the-likelihood-pmathcald-f-adding-gaussian-noise",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "We observe the data \\(\\mathcal{D}_t = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^t\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\). This can be written in vector form as:\n\\[\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\]\nwhere \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 \\mathbf{I})\\).\nThe likelihood \\(P(\\mathcal{D}_t | f)\\) (or more precisely, \\(P(\\mathbf{y}_t | \\mathbf{f}_t)\\)) is a Gaussian centered at the true function values \\(\\mathbf{f}_t\\):\n\\[P(\\mathbf{y}_t | \\mathbf{f}_t) = \\mathcal{N}(\\mathbf{y}_t | \\mathbf{f}_t, \\sigma_n^2 \\mathbf{I})\\]\nThis explicitly defines the \\(P(\\mathcal{D}|f)\\) term (using \\(\\mathbf{f}_t\\) as the ‚Äúspecific function‚Äù part for the observed data)."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-multiplication-and-conditioning-to-get-the-posterior-pf-mathcald",
    "href": "posts/bayesian-optimization/index.html#the-multiplication-and-conditioning-to-get-the-posterior-pf-mathcald",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "",
    "text": "Now, the ‚Äúmultiplication‚Äù that yields the posterior GP is achieved by a fundamental property of multivariate Gaussian distributions:\nIf you have two random variables (or vectors of variables) \\(A\\) and \\(B\\) that are jointly Gaussian (note that \\(\\Sigma = Cov\\)):\n\\[\\begin{pmatrix} A \\\\ B \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_A \\\\ \\boldsymbol{\\mu}_B \\end{pmatrix}, \\begin{pmatrix} \\boldsymbol{\\Sigma}_{AA} & \\boldsymbol{\\Sigma}_{AB} \\\\ \\boldsymbol{\\Sigma}_{BA} & \\boldsymbol{\\Sigma}_{BB} \\end{pmatrix} \\right)\\]\nThen, the conditional distribution of \\(B\\) given \\(A\\) (i.e., \\(P(B|A)\\)) is also Gaussian with:\n\nConditional Mean: \\(\\boldsymbol{\\mu}_{B|A} = \\boldsymbol{\\mu}_B + \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} (\\mathbf{A} - \\boldsymbol{\\mu}_A)\\)\nConditional Covariance: \\(\\boldsymbol{\\Sigma}_{B|A} = \\boldsymbol{\\Sigma}_{BB} - \\boldsymbol{\\Sigma}_{BA} \\boldsymbol{\\Sigma}_{AA}^{-1} \\boldsymbol{\\Sigma}_{AB}\\)\n\nHow this maps to GPs:\n\nForm the Joint Distribution of \\((\\mathbf{y}_t, f^*)\\) under the Prior: We need the joint prior distribution of our observed outputs \\(\\mathbf{y}_t\\) and our unobserved test point function value \\(f^*\\). We know \\(\\mathbf{y}_t = \\mathbf{f}_t + \\boldsymbol{\\epsilon}\\). Since \\(\\mathbf{f}_t\\) and \\(f^*\\) are jointly Gaussian (from the GP prior) and \\(\\boldsymbol{\\epsilon}\\) is Gaussian (noise), their sum is also jointly Gaussian. The joint prior mean of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is \\(\\begin{pmatrix} \\mathbf{m}_t \\\\ m(\\mathbf{x}^*) \\end{pmatrix}\\). The joint prior covariance of \\(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\) is:\n\\[\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y}_t \\\\ f^* \\end{pmatrix}\\right) = \\begin{pmatrix} \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I} & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(\\mathbf{x}^*, \\mathbf{x}^*) \\end{pmatrix}\\]\n(Here, the \\(\\sigma_n^2 \\mathbf{I}\\) term comes from the noise added to \\(\\mathbf{f}_t\\) in the \\(\\mathbf{y}_t\\) block).\nConditioning (Applying Bayes‚Äô Rule): Now, we have observed \\(\\mathbf{y}_t\\). We want to find the posterior distribution of \\(f^*\\) given \\(\\mathbf{y}_t\\), which is \\(P(f^* | \\mathbf{y}_t)\\). Using the conditional Gaussian formulas, let:\n\n\\(A = \\mathbf{y}_t\\) (our observed data)\n\\(B = f^*\\) (the function value we want to predict)\n\\(\\boldsymbol{\\mu}_A = \\mathbf{m}_t\\)\n\\(\\boldsymbol{\\mu}_B = m(\\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AA} = \\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\)\n\\(\\boldsymbol{\\Sigma}_{BB} = k(\\mathbf{x}^*, \\mathbf{x}^*)\\)\n\\(\\boldsymbol{\\Sigma}_{AB} = \\mathbf{k}_*\\)\n\\(\\boldsymbol{\\Sigma}_{BA} = \\mathbf{k}_*^T\\)\n\nPlugging these into the conditional mean and covariance formulas gives exactly the GP posterior predictive mean and variance:\nPosterior Mean \\(\\mu_t(\\mathbf{x}^*)\\):\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\]\nPosterior Variance \\(\\sigma_t^2(\\mathbf{x}^*)\\):\n\\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\n\nIn essence:\nThe ‚Äúmultiplication‚Äù \\(P(\\mathcal{D} | f) P(f)\\) is handled internally by the mathematical properties of Gaussian distributions. The GP framework sets up a joint Gaussian prior over all relevant function values (observed and unobserved). The likelihood then specifies how our observed data \\(\\mathbf{y}_t\\) relates to the true function values \\(\\mathbf{f}_t\\). By conditioning this joint prior on the observed data \\(\\mathbf{y}_t\\), we directly derive the exact posterior distribution for the unobserved function values \\(f^*\\), which turns out to also be Gaussian with the mean and variance formulas."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-gaussian-process-gp-as-the-surrogate-model",
    "href": "posts/bayesian-optimization/index.html#the-gaussian-process-gp-as-the-surrogate-model",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "1. The Gaussian Process (GP) as the Surrogate Model",
    "text": "1. The Gaussian Process (GP) as the Surrogate Model\nAs established, the Gaussian Process models our unknown objective function \\(f(\\mathbf{x})\\) as a probability distribution over functions:\n\\[f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))\\]\n\n\\(m(\\mathbf{x})\\): Mean function (often assumed to be \\(m(\\mathbf{x})=0\\) or the mean of observed data for simplicity).\n\\(k(\\mathbf{x}, \\mathbf{x}')\\): Kernel (covariance) function, defining similarity between function values at different points. A common choice is the Squared Exponential (RBF) kernel:\n\\[k(\\mathbf{x}, \\mathbf{x}') = \\sigma_f^2 \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}'\\|^2}{2l^2}\\right)\\]\nwhere \\(\\sigma_f^2\\) is the signal variance (amplitude) and \\(l\\) is the length-scale.\n\nGiven a set of \\(t\\) observed data points \\(\\mathcal{D}_t = \\{(\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_t, y_t)\\}\\), where \\(y_i = f(\\mathbf{x}_i) + \\epsilon_i\\) (with additive Gaussian noise \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\)), the posterior predictive distribution for a new point \\(\\mathbf{x}^*\\) is Gaussian:\n\\[f(\\mathbf{x}^*) | \\mathcal{D}_t \\sim \\mathcal{N}(\\mu_t(\\mathbf{x}^*), \\sigma_t^2(\\mathbf{x}^*))\\]\nThe predictive mean \\(\\mu_t(\\mathbf{x}^*)\\) and variance \\(\\sigma_t^2(\\mathbf{x}^*)\\) are given by:\n\\[\\mu_t(\\mathbf{x}^*) = m(\\mathbf{x}^*) + \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)\\] \\[\\sigma_t^2(\\mathbf{x}^*) = k(\\mathbf{x}^*, \\mathbf{x}^*) - \\mathbf{k}_*^T (\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\]\nWhere:\n\n\\(\\mathbf{y}_t = [y_1, \\dots, y_t]^T\\) (vector of observed values)\n\\(\\mathbf{m}_t = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_t)]^T\\) (mean function evaluated at observed points)\n\\(\\mathbf{K}_t\\): \\(t \\times t\\) covariance matrix where \\([\\mathbf{K}_t]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\\).\n\\(\\mathbf{k}_*\\): \\(t \\times 1\\) vector where \\([\\mathbf{k}_*]_i = k(\\mathbf{x}^*, \\mathbf{x}_i)\\).\n\\(\\mathbf{I}\\): Identity matrix.\n\\(\\sigma_n^2\\): Noise variance."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#bayesian-optimization-iteration-using-an-acquisition-function",
    "href": "posts/bayesian-optimization/index.html#bayesian-optimization-iteration-using-an-acquisition-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "2. Bayesian Optimization Iteration using an Acquisition Function",
    "text": "2. Bayesian Optimization Iteration using an Acquisition Function\nThe goal of Bayesian Optimization is to find \\(\\mathbf{x}^* = \\arg\\max_{\\mathbf{x} \\in \\mathcal{X}} f(\\mathbf{x})\\), where \\(\\mathcal{X}\\) is the search domain.\nThe iterative process involves:\n\nUpdate GP: Use the current dataset \\(\\mathcal{D}_t\\) to compute the posterior mean \\(\\mu_t(\\mathbf{x})\\) and variance \\(\\sigma_t^2(\\mathbf{x})\\) for the entire search space \\(\\mathcal{X}\\).\nMaximize Acquisition Function: Select the next point \\(\\mathbf{x}_{next}\\) by maximizing an acquisition function \\(a(\\mathbf{x})\\), which intelligently balances exploration (sampling in uncertain regions) and exploitation (sampling in promising regions). We‚Äôll use Expected Improvement (EI) for our example:\n\\[\\text{EI}(\\mathbf{x}) = \\mathbb{E}[\\max(0, f(\\mathbf{x}) - y_{\\text{max}}^*)]\\]\nWhere \\(y_{\\text{max}}^* = \\max_{i=1 \\dots t} y_i\\) is the current best observed value.\nThe analytical form of EI (assuming \\(\\sigma_t(\\mathbf{x}) &gt; 0\\)) is:\n\\[\\text{EI}(\\mathbf{x}) = \\sigma_t(\\mathbf{x}) \\left[\\phi(Z) + Z\\Phi(Z)\\right]\\]\nwhere \\(Z = \\frac{\\mu_t(\\mathbf{x}) - y_{\\text{max}}^*}{\\sigma_t(\\mathbf{x})}\\). If \\(\\sigma_t(\\mathbf{x}) = 0\\), then \\(\\text{EI}(\\mathbf{x}) = 0\\). \\(\\phi(\\cdot)\\) is the PDF and \\(\\Phi(\\cdot)\\) is the CDF of the standard normal distribution.\nEvaluate True Function: Obtain \\(y_{next} = f(\\mathbf{x}_{next})\\).\nAdd to Data: \\(\\mathcal{D}_{t+1} = \\mathcal{D}_t \\cup \\{(\\mathbf{x}_{next}, y_{next})\\}\\)."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#numerical-example-optimizing-a-simple-1d-function",
    "href": "posts/bayesian-optimization/index.html#numerical-example-optimizing-a-simple-1d-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "3. Numerical Example: Optimizing a Simple 1D Function",
    "text": "3. Numerical Example: Optimizing a Simple 1D Function\nLet‚Äôs use a very simple 1D objective function \\(f(x) = -(x-2)^2 + 4\\) over the domain \\(x \\in [0,4]\\). The true maximum is at \\(x=2\\) with \\(f(2)=4\\).\nGP Hyperparameters (Fixed for Simplicity):\n\nMean function \\(m(x)=0\\).\nSquared Exponential Kernel: \\(\\sigma_f^2=1.0\\), \\(l=1.0\\). So, \\(k(x,x') = 1.0 \\cdot \\exp\\left(-\\frac{(x-x')^2}{2 \\cdot 1.0^2}\\right) = \\exp\\left(-\\frac{(x-x')^2}{2}\\right)\\).\nNoise variance \\(\\sigma_n^2=0.01\\).\n\nInitial Data Points (\\(\\mathcal{D}_2\\)): Let‚Äôs say we randomly selected two points and evaluated the true function (with no noise for simplicity in the example, so \\(\\epsilon_i=0\\)):\n\n\\(x_1=1.0 \\Rightarrow y_1 = -(1.0-2)^2 + 4 = -(-1)^2 + 4 = 3.0\\)\n\\(x_2=3.0 \\Rightarrow y_2 = -(3.0-2)^2 + 4 = -(1)^2 + 4 = 3.0\\)\n\nSo, our initial dataset is \\(\\mathcal{D}_2 = \\{(1.0, 3.0), (3.0, 3.0)\\}\\). Current best observed value: \\(y_{\\text{max}}^* = 3.0\\).\n\nIteration 1: Find the Next Point to Evaluate\nStep 1: Compute \\(\\mathbf{K}_t + \\sigma_n^2 \\mathbf{I}\\) and its inverse\nFirst, calculate the kernel matrix \\(\\mathbf{K}_2\\) for \\(x_1=1.0\\) and \\(x_2=3.0\\):\n\n\\(k(x_1,x_1) = \\exp\\left(-\\frac{(1-1)^2}{2}\\right) = \\exp(0) = 1.0\\)\n\\(k(x_1,x_2) = \\exp\\left(-\\frac{(1-3)^2}{2}\\right) = \\exp\\left(-\\frac{(-2)^2}{2}\\right) = \\exp\\left(-\\frac{4}{2}\\right) = \\exp(-2) \\approx 0.1353\\)\n\\(k(x_2,x_1) = k(x_1,x_2) \\approx 0.1353\\)\n\\(k(x_2,x_2) = \\exp\\left(-\\frac{(3-3)^2}{2}\\right) = \\exp(0) = 1.0\\)\n\nSo, \\(\\mathbf{K}_2 = \\begin{bmatrix} 1.0 & 0.1353 \\\\ 0.1353 & 1.0 \\end{bmatrix}\\)\nNow add the noise variance \\(\\sigma_n^2 \\mathbf{I} = 0.01 \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix}\\): \\(\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I} = \\begin{bmatrix} 1.01 & 0.1353 \\\\ 0.1353 & 1.01 \\end{bmatrix}\\)\nCalculate the inverse \\((\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1}\\): Determinant \\(= (1.01 \\times 1.01) - (0.1353 \\times 0.1353) = 1.0201 - 0.0183 \\approx 1.0018\\) Inverse \\(\\approx \\frac{1}{1.0018} \\begin{bmatrix} 1.01 & -0.1353 \\\\ -0.1353 & 1.01 \\end{bmatrix} \\approx \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix}\\)\nStep 2: Calculate \\(\\mu_t(x^*)\\) and \\(\\sigma_t^2(x^*)\\) for candidate points\nLet‚Äôs pick a few candidate points \\(x^*\\) to evaluate our GP at:\n\n\\(x_A^* = 0.5\\)\n\\(x_B^* = 2.0\\) (Near the true optimum, but previously unobserved)\n\\(x_C^* = 3.5\\)\n\nFor each \\(x^*\\), we need \\(\\mathbf{k}_* = [k(x^*,x_1), k(x^*,x_2)]^T\\):\nFor \\(x_A^* = 0.5\\):\n\n\\(k(0.5,1.0) = \\exp\\left(-\\frac{(0.5-1.0)^2}{2}\\right) = \\exp\\left(-\\frac{(-0.5)^2}{2}\\right) = \\exp(-0.125) \\approx 0.8825\\)\n\\(k(0.5,3.0) = \\exp\\left(-\\frac{(0.5-3.0)^2}{2}\\right) = \\exp\\left(-\\frac{(-2.5)^2}{2}\\right) = \\exp(-3.125) \\approx 0.0440\\) So, \\(\\mathbf{k}_* = \\begin{bmatrix} 0.8825 \\\\ 0.0440 \\end{bmatrix}\\)\n\nNow compute \\(\\mu_t(0.5)\\) and \\(\\sigma_t^2(0.5)\\): (\\(\\mathbf{y}_t - \\mathbf{m}_t = \\mathbf{y}_t = [3.0, 3.0]^T\\) since \\(m(x)=0\\))\n\\(\\mu_t(0.5) = \\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_t\\) (since \\(\\mathbf{m}_t=0\\)) \\(\\mu_t(0.5) \\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix} \\begin{bmatrix} 3.0 \\\\ 3.0 \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} (1.0082 \\times 3) + (-0.1351 \\times 3) \\\\ (-0.1351 \\times 3) + (1.0082 \\times 3) \\end{bmatrix} = \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 2.6193 \\\\ 2.6193 \\end{bmatrix}\\) \\(\\approx (0.8825 \\times 2.6193) + (0.0440 \\times 2.6193) \\approx 2.3106 + 0.1151 \\approx \\mathbf{2.4257}\\)\n\\(\\sigma_t^2(0.5) = k(0.5,0.5) - \\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\\) \\(k(0.5,0.5) = 1.0\\) \\(\\mathbf{k}_*^T (\\mathbf{K}_2 + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_* \\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 1.0082 & -0.1351 \\\\ -0.1351 & 1.0082 \\end{bmatrix} \\begin{bmatrix} 0.8825 \\\\ 0.0440 \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} (1.0082 \\times 0.8825) + (-0.1351 \\times 0.0440) \\\\ (-0.1351 \\times 0.8825) + (1.0082 \\times 0.0440) \\end{bmatrix}\\) \\(\\approx \\begin{bmatrix} 0.8825 & 0.0440 \\end{bmatrix} \\begin{bmatrix} 0.8837 \\\\ -0.0762 \\end{bmatrix} \\approx (0.8825 \\times 0.8837) + (0.0440 \\times -0.0762) \\approx 0.7797 - 0.0033 \\approx 0.7764\\) \\(\\sigma_t^2(0.5) \\approx 1.0 - 0.7764 = \\mathbf{0.2236}\\)\nLet‚Äôs summarize for our candidates (using a more precise calculator for speed for \\(x_B^*\\) and \\(x_C^*\\)):\n\nAt \\(x_A^* = 0.5\\): \\(\\mu_t(0.5) \\approx 2.425\\) \\(\\sigma_t(0.5) = \\sqrt{0.2236} \\approx 0.473\\)\nAt \\(x_B^* = 2.0\\): (This point is exactly in the middle of our two observed points, so we expect high uncertainty as it‚Äôs unobserved but also perhaps a good mean due to interpolation) \\(\\mu_t(2.0) \\approx 3.0\\) \\(\\sigma_t(2.0) \\approx 0.995\\) (High uncertainty because it‚Äôs far from observed data in terms of kernel distance, but interpolated mean is high)\nAt \\(x_C^* = 3.5\\): \\(\\mu_t(3.5) \\approx 2.425\\) \\(\\sigma_t(3.5) \\approx 0.473\\)\n\nStep 3: Calculate Expected Improvement (EI) for candidate points\nCurrent best \\(y_{\\text{max}}^* = 3.0\\). We will use \\(\\xi=0\\) (the default for simple EI, meaning no exploration-exploitation trade-off parameter).\nFor \\(x_A^* = 0.5\\):\n\\(Z = \\frac{\\mu_t(0.5) - y_{\\text{max}}^*}{\\sigma_t(0.5)} = \\frac{2.425 - 3.0}{0.473} = \\frac{-0.575}{0.473} \\approx -1.215\\) \\(\\phi(-1.215) \\approx 0.1804\\) (PDF value)\n\\(\\Phi(-1.215) \\approx 0.1122\\) (CDF value)\n\\(\\text{EI}(0.5) = 0.473 [0.1804 + (-1.215) \\cdot 0.1122] = 0.473 [0.1804 - 0.1364] = 0.473 [0.044] \\approx \\mathbf{0.0208}\\)\nFor \\(x_B^* = 2.0\\):\n\\(Z = \\frac{\\mu_t(2.0) - y_{\\text{max}}^*}{\\sigma_t(2.0)} = \\frac{3.0 - 3.0}{0.995} = 0\\) \\(\\phi(0) \\approx 0.3989\\) \\(\\Phi(0) = 0.5\\) \\(\\text{EI}(2.0) = 0.995 [0.3989 + 0 \\cdot 0.5] = 0.995 [0.3989] \\approx \\mathbf{0.3964}\\)\nFor \\(x_C^* = 3.5\\):\n\\(Z = \\frac{\\mu_t(3.5) - y_{\\text{max}}^*}{\\sigma_t(3.5)} = \\frac{2.425 - 3.0}{0.473} \\approx -1.215\\) \\(\\text{EI}(3.5) \\approx \\mathbf{0.0208}\\) (same as \\(x_A^*\\) due to symmetry in this specific example setup)\nStep 4: Identify \\(\\mathbf{x}_{next}\\)\nComparing the EI values:\n\n\\(\\text{EI}(0.5) \\approx 0.0208\\)\n\\(\\text{EI}(2.0) \\approx 0.3964\\)\n\\(\\text{EI}(3.5) \\approx 0.0208\\)\n\nThe point \\(\\mathbf{x}_{next}=\\mathbf{2.0}\\) has the highest Expected Improvement. This makes sense: it‚Äôs centrally located relative to the observed points, and while its predicted mean is only 3.0 (same as observed), its uncertainty is very high, suggesting a high potential for improvement.\nStep 5: Evaluate the true function at \\(\\mathbf{x}_{next}\\) and update data\nWe evaluate \\(f(2.0) = -(2.0-2)^2 + 4 = 4.0\\). Our updated dataset becomes \\(\\mathcal{D}_3 = \\{(1.0, 3.0), (3.0, 3.0), (2.0, 4.0)\\}\\). The new best observed value \\(y_{\\text{max}}^* = 4.0\\).\n\n\nIteration 2 (Conceptual)\nWith the new point \\((2.0, 4.0)\\), the GP model would be updated. The uncertainty around \\(x=2.0\\) would drastically decrease, as we now know its value precisely (or with very low noise). The acquisition function would then be maximized again. Given that \\(y_{\\text{max}}^*\\) is now 4.0 (the true optimum), the EI will be very low at \\(x=2.0\\). The algorithm would likely explore regions further away from \\(x=2.0\\) to ensure no other maxima exist, or converge as no significant improvement is expected elsewhere."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#formulation",
    "href": "posts/bayesian-optimization/index.html#formulation",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Formulation",
    "text": "Formulation\nBayes‚Äô Theorem:\n\\[P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)}\\]\nWhere:\n\n\\(P(\\theta | D)\\): Posterior (Our updated belief in the parameters \\(\\theta\\) after seeing the Data \\(D\\)). This is what we want to find in Bayesian inference.\n\\(P(D | \\theta)\\): Likelihood (The probability of observing the Data \\(D\\) given the parameters \\(\\theta\\)). This is the core term that MLE maximizes.\n\\(P(\\theta)\\): Prior Probability (Our initial belief in the parameters \\(\\theta\\) before seeing any data).\n\\(P(D)\\): Evidence / Marginal Likelihood (The total probability of the data, across all possible parameters. This is a normalizing constant).\n\n\nMaximum Likelihood Estimation (MLE)\nMLE is purely data-driven (no belief about parameters needed). It asks: ‚ÄúGiven this data, what are the parameters that make this data most probable?‚Äù\n\\[\\theta_{MLE} = \\arg \\max_{\\theta} P(D | \\theta)\\]\n\n\nMaximum A Posteriori (MAP) Estimation\nMAP is the inverse one, working on posterior:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(\\theta | D)\\]\nApplying Bayes‚Äô Theorem:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} \\frac{P(D | \\theta) P(\\theta)}{P(D)}\\]\nSince \\(P(D)\\) is a constant with respect to \\(\\theta\\) ( doesn‚Äôt depend on \\(\\theta\\)):\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(D | \\theta) P(\\theta)\\]\nKey characteristic: MAP is a blend of data and prior beliefs. It asks: ‚ÄúGiven this data and my prior beliefs about the parameters, what are the parameters that are most probable?‚Äù\n\n\nMLE is a Special Case of MAP\n\\(P(\\theta)\\) is constant, because parameter space is uniformly distributed:\n\\[\\theta_{MAP} = \\arg \\max_{\\theta} P(D | \\theta)\\]\nWow, MAP is reduced to MLE when parameters are uniformly distributed\nIn simple terms:\n\nMLE: ‚ÄúWhat parameters best explain the data I observed?‚Äù \\(\\rightarrow P(D | \\theta)\\)\nMAP: ‚ÄúWhat parameters are most plausible overall, considering both the data and what I already believed before seeing the data?‚Äù \\(\\rightarrow P(D | \\theta) \\times P(\\theta)\\)\nBayes‚Äô Theorem (Full Inference): ‚ÄúWhat‚Äôs the complete updated probability distribution for my parameters, given everything I know?‚Äù"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#naive-bayes-classifier---the-naive-assumption-conditional-independence",
    "href": "posts/bayesian-optimization/index.html#naive-bayes-classifier---the-naive-assumption-conditional-independence",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Naive Bayes Classifier - The ‚ÄúNaive‚Äù Assumption: Conditional Independence",
    "text": "Naive Bayes Classifier - The ‚ÄúNaive‚Äù Assumption: Conditional Independence\nRecall Bayes‚Äô Theorem for a hypothesis \\(H\\) (which is our class \\(C\\)) and evidence \\(E\\) (which are our features \\(F_1, F_2, \\dots, F_n\\)):\n\\[P(C | F_1, F_2, \\dots, F_n) = \\frac{P(F_1, F_2, \\dots, F_n | C) \\cdot P(C)}{P(F_1, F_2, \\dots, F_n)}\\]\nComputing the joint probability of correlated evidences likelihood term \\(P(F_1, F_2, \\dots, F_n | C)\\) can be very complex.\nThe ‚ÄúNaive‚Äù assumption: conditional independence, the joint likelihood = the product of individual feature likelihoods:\n\\[P(F_1, F_2, \\dots, F_n | C) \\approx \\prod_{i=1}^n P(F_i | C)\\]\nThus, we reduced the Naive Bayes Classifier to finding \\(C\\) that maximizes the posterior probability:\n\\(C_{predicted} = \\arg \\max_{C} \\left( P(C) \\cdot \\prod_{i=1}^n P(F_i | C) \\right)\\)\nJust the same as above, the denominator \\(P(E) = \\sum_i P(E|H_i)P(H_i) = P(F_1, F_2, \\dots, F_n)\\) is omitted during prediction because it‚Äôs a constant for all classes, acting only as a normalizer so that the sum of all posterior probabilities for all possible hypotheses equals 1."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#covariance-between-two-random-variables-1d",
    "href": "posts/bayesian-optimization/index.html#covariance-between-two-random-variables-1d",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Covariance between Two Random Variables (1D)",
    "text": "Covariance between Two Random Variables (1D)\nLet‚Äôs start with the simplest case: two scalar random variables \\(X\\) and \\(Y\\). The covariance between \\(X\\) and \\(Y\\), denoted \\(Cov(X, Y)\\) or \\(\\sigma_{XY}\\), measures the degree to which they vary together.\nThe formal definition of covariance is:\n\\[Cov(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\]\nWhere:\n\n\n\\(\\mathbb{E}[X]\\) is the expected value (mean) of a vector that contains some samples but only taking the \\(X\\) dimension.\n\n\n\\(\\mathbb{E}[Y]\\) is the expected value (mean) of a vector that contains the samples but only taking the \\(Y\\) dimension.\n\n\nDot product between vectors \\((X - \\mathbb{E}[X]) and (Y - \\mathbb{E}[Y])\\)\n\n\nThe outer operation \\(\\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\) basically just eventually normalized by dividing the number of samples\n\n\nIntuition:\n\nIf \\(X\\) mostly distributed above its mean AND \\(Y\\) is also above its mean, the product \\((X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\) will often be positive.\nIf \\(X\\) mostly distributed above its mean AND \\(Y\\) is below its mean (and vice-versa), the product will often be negative.\nOtherwise the distributed \\(X\\) and \\(y\\) cancel out on average =&gt; zero covariance only implies no linear relationship; variables can still have a non-linear relationship."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#covariance-of-a-single-random-variable-with-itself-variance",
    "href": "posts/bayesian-optimization/index.html#covariance-of-a-single-random-variable-with-itself-variance",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Covariance of a Single Random Variable with Itself (Variance)",
    "text": "Covariance of a Single Random Variable with Itself (Variance)\nVariance measures how much a single random variable deviates from its mean, or its spread, \\(Cov(X, X)\\). This gives us the variance of \\(X\\), denoted \\(Var(X)\\) or \\(\\sigma_X^2\\):\n\\[Cov(X, X) = \\mathbb{E}[(X - \\mathbb{E}[X])(X - \\mathbb{E}[X])] = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = Var(X)\\]\nCalculate the same way as above, basically a dot product of a vector \\((X - \\mathbb{E}[X])\\) with itself, therefore it will be sth like this:\n\\[Var(X) = (Xsample_1 - meanX)¬≤ + (Xsample_2 - meanX)¬≤ + ... + (Xsample_n - meanX)¬≤ = {a positive or negative number}\\]\nIntuition:\n\nmean \\(E[X]\\) could be in the middle, but when most samples are mostly distributed on negative or positive side, then the Medium will be on that side\nremember we are dealing with only one dimensional (scalar) variables"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-covariance-matrix-for-multiple-random-variables-multivariate-case",
    "href": "posts/bayesian-optimization/index.html#the-covariance-matrix-for-multiple-random-variables-multivariate-case",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "The Covariance Matrix for Multiple Random Variables (Multivariate Case)",
    "text": "The Covariance Matrix for Multiple Random Variables (Multivariate Case)\nNow, let‚Äôs extend both concepts above to multiple random variables. Suppose we have a random vector \\(\\mathbf{X}\\) consisting of \\(n\\) random variables:\n\\[\\mathbf{X} = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_n \\end{bmatrix}\\]\nThe covariance matrix \\(\\mathbf{\\Sigma}\\), is an \\(n \\times n\\) matrix where each element \\(\\mathbf{\\Sigma}_{ij}\\) represents the covariance between the \\(i\\)-th random variable \\(X_i\\) and the \\(j\\)-th random variable \\(X_j\\):\n\\[\\mathbf{\\Sigma}_{ij} = Cov(X_i, X_j)\\]\nMore formally, the covariance matrix \\(\\mathbf{\\Sigma}\\) is defined as:\n\\[\\mathbf{\\Sigma} = \\mathbb{E}[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])^T]\\]\nLet‚Äôs explicitly write out the elements of a \\(3 \\times 3\\) covariance matrix for a random vector \\(\\mathbf{X} = [X_1, X_2, X_3]^T\\):\n\\[\n\\mathbf{\\Sigma} = \\begin{bmatrix}\nVar(X_1) & Cov(X_1, X_2) & Cov(X_1, X_3) \\\\\nCov(X_2, X_1) & Var(X_2) & Cov(X_2, X_3) \\\\\nCov(X_3, X_1) & Cov(X_3, X_2) & Var(X_3)\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#intuition-of-the-covariance-matrix-spread-and-relationships",
    "href": "posts/bayesian-optimization/index.html#intuition-of-the-covariance-matrix-spread-and-relationships",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Intuition of the Covariance Matrix: Spread and Relationships",
    "text": "Intuition of the Covariance Matrix: Spread and Relationships\n\nDiagonal Elements: The variances of each individual random variable a.k.a the individual spread: \\[\\mathbf{\\Sigma}_{ii} = Cov(X_i, X_i) = Var(X_i)\\]\nOff-Diagonal Elements: \\[\\mathbf{\\Sigma}_{ij} = Cov(X_i, X_j) \\quad \\text{for } i \\neq j\\] These tell us how pairs of variables move together.\n\nIf \\(\\mathbf{\\Sigma}_{ij} &gt; 0\\), \\(X_i\\) and \\(X_j\\) tend to increase or decrease together.\nIf \\(\\mathbf{\\Sigma}_{ij} &lt; 0\\), \\(X_i\\) tends to increase when \\(X_j\\) decreases, and vice-versa.\nIf \\(\\mathbf{\\Sigma}_{ij} \\approx 0\\), there is little to no linear relationship between \\(X_i\\) and \\(X_j\\).\n\nSymmetry: The covariance matrix is always symmetric, \\(\\mathbf{\\Sigma}_{ij} = \\mathbf{\\Sigma}_{ji}\\) because \\(Cov(X_i, X_j) = Cov(X_j, X_i)\\), you know how.\n\nIn the next chapter about Gaussian Processes, we will slowly build the kernel function \\(k(x, x')\\) directly defines the elements of this covariance matrix for function values at different input points. This matrix say how much a point x is impacted by x‚Äô."
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#the-linear-model-and-gaussian-noise-assumption",
    "href": "posts/bayesian-optimization/index.html#the-linear-model-and-gaussian-noise-assumption",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "The Linear Model and Gaussian Noise Assumption",
    "text": "The Linear Model and Gaussian Noise Assumption\nConsider a simple linear regression model where we try to predict an output \\(y_i\\) based on input features \\(\\mathbf{x}_i\\):\n\\[y_i = \\mathbf{x}_i^T \\boldsymbol{\\beta} + \\epsilon_i\\]\nWhere:\n\n\\(y_i\\) is the \\(i\\)-th observed output.\n\\(\\mathbf{x}_i^T\\) representing the features for the \\(i\\)-th observation.\n\\(\\boldsymbol{\\beta}\\) is the vector of unknown regression coefficients (parameters) we want to estimate.\n\\(\\epsilon_i\\) is the error or noise term for the \\(i\\)-th observation.\n\nCrucial assumption for this derivation is that these error terms \\(\\epsilon_i\\) are independently and identically distributed (i.i.d.) according to a Gaussian (Normal) distribution with a mean of zero and a constant variance \\(\\sigma^2\\):\n\\[\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\\]\nBecause \\(\\mathbf{x}_i^T \\boldsymbol{\\beta}\\) is a fixed (non-random) quantity for a given \\(\\mathbf{x}_i\\), this assumption about the error implies that \\(y_i\\) itself is also normally distributed:\n\\[y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2 \\sim \\mathcal{N}(\\mathbf{x}_i^T \\boldsymbol{\\beta}, \\sigma^2)\\]\nThis means the probability density function (PDF) for a single observation \\(y_i\\) is:\n\\[p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\]"
  },
  {
    "objectID": "posts/bayesian-optimization/index.html#likelihood-function",
    "href": "posts/bayesian-optimization/index.html#likelihood-function",
    "title": "Gaussian Process & Bayesian Optimization",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nImportant Definition: For a dataset of \\(N\\) independent observations \\((\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_N, y_N)\\), the likelihood function \\(L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y})\\) is the product of the individual PDFs (due to the independence assumption):\n\\[L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\prod_{i=1}^N p(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2)\\]\n\\[L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\]\nGoal Maximum Likelihood Estimation (MLE):\n\nGiven datast \\((\\mathbf{x}_1, y_1), \\dots, (\\mathbf{x}_N, y_N)\\)\n=&gt; Find \\(\\boldsymbol{\\beta}\\) (and \\(\\sigma^2\\)) that maximize this likelihood function.\n\n\nDerivation\nConverts products into sums, simplifying differentiation. Since the logarithm is a monotonically increasing function, maximizing \\(\\ln L\\) is equivalent to maximizing \\(L\\):\n\\[\\ln L(\\boldsymbol{\\beta}, \\sigma^2 | \\mathbf{X}, \\mathbf{y}) = \\ln \\left( \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right) \\right)\\]\nUsing logarithm properties (\\(\\ln(ab) = \\ln a + \\ln b\\) and \\(\\ln(a^b) = b \\ln a\\)):\n\\[\\ln L = \\sum_{i=1}^N \\left[ \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\ln\\left(\\exp\\left(-\\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\right)\\right) \\right]\\]\n\\[\\ln L = \\sum_{i=1}^N \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2} \\right]\\]\n\\[\\ln L = -N \\cdot \\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2}\\]\n\n\nMaximizing this function with respect to \\(\\boldsymbol{\\beta}\\)\nTo do that, easy, we take the partial derivative with respect to \\(\\boldsymbol{\\beta}\\) and set it to zero.\nInterestingly, \\(-N \\cdot \\frac{1}{2}\\ln(2\\pi\\sigma^2)\\), does not depend on \\(\\boldsymbol{\\beta}\\). Therefore, when maximizing \\(\\ln L\\) with respect to \\(\\boldsymbol{\\beta}\\), we only need to consider the second term:\n\\[\\text{maximize} \\left( - \\sum_{i=1}^N \\frac{(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2}{2\\sigma^2} \\right)\\]\nand voila!: \\[\\text{minimize} \\left( \\sum_{i=1}^N (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 \\right)\\]"
  },
  {
    "objectID": "posts/autoencoder-dreamer/index.html",
    "href": "posts/autoencoder-dreamer/index.html",
    "title": "From Autoencoder to Dreamer",
    "section": "",
    "text": "Key Idea: Instead of mapping an input to a single point in the latent space, VAE maps it to parameters of a probability distribution (typically a Gaussian distribution) in the latent space:\n\nAn encoder that maps data to a distribution in latent space.\nA decoder that maps samples from this latent distribution back to data space.\n\n\n\nLet‚Äôs denote:\n\n\\(\\mathbf{x}\\): An input data point (e.g., an image)\n\\(\\mathbf{z}\\): A latent variable (vector) in the lower-dimensional latent space\n\\(p(\\mathbf{x})\\): The true, unknown data distribution we want to model\n\\(p(\\mathbf{z})\\): The prior distribution over the latent variables (typically a simple distribution like \\(\\mathcal{N}(0,I)\\))\n\\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\): The decoder (also called generative model or likelihood). This is a neural network parameterized by \\(\\theta\\) that outputs the parameters of the distribution over \\(\\mathbf{x}\\) given \\(\\mathbf{z}\\).\n\\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\): The encoder (also called inference model or approximate posterior). This is a neural network parameterized by \\(\\phi\\) that outputs the parameters of the distribution over \\(\\mathbf{z}\\) given \\(\\mathbf{x}\\).\n\nGoal: is to maximize the marginal likelihood: \\[p(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})}\\] which we do not know the other element to compute, or \\[\\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) d\\mathbf{z}\\] which is intractable\nTherefore, VAEs optimize the Evidence Lower Bound (ELBO), which is a lower bound on the log-likelihood of the data, we will derive it now\n\n\nThe log-likelihood of a data point \\(\\mathbf{x}\\) can be written as:\n\\[\\log p_\\theta(\\mathbf{x}) = \\log p_\\theta(\\mathbf{x})\\] \\[= \\log p_\\theta(\\mathbf{x}) \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) d\\mathbf{z} \\quad \\text{Multiply by 1}\\] \\[= \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) \\log p_\\theta(\\mathbf{x}) d\\mathbf{z} \\quad \\text{Bring inside the integral}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x})] \\quad \\text{Definition of expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{p_\\theta(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Apply the equation } p_\\theta(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{p_\\theta(\\mathbf{z}|\\mathbf{x})}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z}) q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta(\\mathbf{z}|\\mathbf{x}) q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Multiply by 1}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} + \\log \\frac{q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Split the expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] + D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p_\\theta(\\mathbf{z}|\\mathbf{x})) \\quad \\text{KL divergence}\\]\nThe second term is the Kullback-Leibler (KL) divergence, it is non-negative, now we only focus on maximizing the first term ELBO:\n\\[\\log p_\\theta(\\mathbf{x}) \\ge \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Chain rule of probability}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] + \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Split the expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z})) \\quad \\text{KL divergence}\\]\nThis is the objective function for the VAE (Loss Function), also the ELBO to maximize, or minimizing the negative ELBO.\n\\[\\mathcal{L}(\\theta, \\phi, \\mathbf{x}) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z}))\\]\n\n\n\nThis term encourages the decoder to reconstruct the input \\(\\mathbf{x}\\) accurately from a latent sample \\(\\mathbf{z}\\) drawn from the encoder‚Äôs output distribution.\nWe will derive it now\n\n\nWe assume that each data point \\(\\mathbf{x}\\) (e.g., an image, where each pixel is a dimension) is drawn from a Gaussian distribution, given the latent variable \\(\\mathbf{z}\\) that the decoder outputs. For simplicity, let‚Äôs assume the dimensions of \\(\\mathbf{x}\\) are independent given \\(\\mathbf{z}\\), and they share a fixed variance \\(\\sigma^2\\).\nSo, for each dimension \\(j\\) of \\(\\mathbf{x}\\), \\(x_j\\) is distributed as: \\(p_\\theta(x_j|\\mathbf{z}) = \\mathcal{N}(x_j; \\mu_j(\\mathbf{z}), \\sigma^2)\\)\nHere:\n\n\\(\\mu_j(\\mathbf{z})\\) is the mean for the \\(j\\)-th dimension, which is the output of your decoder network for that dimension when given \\(\\mathbf{z}\\).\n\\(\\sigma^2\\) is the variance. For simplicity, we often assume a fixed \\(\\sigma^2\\) (e.g., \\(\\sigma^2=1\\), or a small constant, or even absorbed into the weights). More advanced VAEs can learn this variance as well (the decoder outputs both mean and variance).\n\n\n\n\nThe PDF for a single-dimensional Gaussian variable \\(x_j\\) is:\n\\(f(x_j) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right)\\)\n\n\n\nSince we assume the dimensions of \\(\\mathbf{x}\\) are independent given \\(\\mathbf{z}\\), the joint probability \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\) is the product of the individual probabilities:\n\\(p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\prod_{j=1}^{D_x} p_\\theta(x_j|\\mathbf{z})\\)\nwhere \\(D_x\\) is the dimensionality of \\(\\mathbf{x}\\) (e.g., number of pixels in an image). Now, let‚Äôs take the logarithm of this product:\n\\[\\log p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\log \\left( \\prod_{j=1}^{D_x} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\log\\left(\\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right)\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2} \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) \\right) - \\sum_{j=1}^{D_x} \\left( \\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2} \\right)\\]The first term is a constant multiplied by \\(D_x\\): \\[= -\\frac{D_x}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{D_x} (x_j - \\mu_j(\\mathbf{z}))^2\\] \\[\\text{MSE} = \\frac{1}{D_x} \\sum_{j=1}^{D_x} (x_j - \\mu_j(\\mathbf{z}))^2\\]\n\n\n\n\nThis term acts as a regularizer. It pushes the approximate posterior \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) (the distribution output by the encoder for a given \\(\\mathbf{x}\\)) to be close to the prior distribution \\(p(\\mathbf{z})\\) (e.g., \\(\\mathcal{N}(0,I)\\)).\nIf \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is assumed to be a diagonal Gaussian \\(\\mathcal{N}(\\mu, \\Sigma)\\) (where \\(\\Sigma\\) is diagonal) and \\(p(\\mathbf{z})\\) is \\(\\mathcal{N}(0,I)\\), the KL divergence has a closed-form solution:\n\\[D_{KL}(\\mathcal{N}(\\mu,\\Sigma)||\\mathcal{N}(0,I)) = \\frac{1}{2} \\sum_{j=1}^{D_z} (\\exp(\\sigma_j) + \\mu_j^2 - 1 - \\sigma_j)\\] where \\(D_z\\) is the dimensionality of \\(\\mathbf{z}\\), and \\(\\mu_j\\) and \\(\\sigma_j\\) are the mean and log-variance (diagonal elements of \\(\\Sigma\\)) for the \\(j\\)-th latent dimension, as output by the encoder.\n\n\nLet‚Äôs say our latent space \\(\\mathbf{z}\\) is 2-dimensional (\\(D_z=2\\)). Our input is a specific image \\(\\mathbf{x}_{cat}\\) (an image of a cat).\nThe encoder network takes \\(\\mathbf{x}_{cat}\\) as input. Its output layer (after processing through several hidden layers) has two sets of \\(D_z=2\\) nodes:\n\nMean Output Nodes: For \\(\\mu_\\phi(\\mathbf{x}_{cat})\\)\nLog-Variance Output Nodes: For \\(\\log(\\sigma_\\phi(\\mathbf{x}_{cat})^2)\\) (we use log-variance for numerical stability, as variance must be positive).\n\nSuppose for this specific \\(\\mathbf{x}_{cat}\\), the encoder outputs:\n\\(\\mu_\\phi(\\mathbf{x}_{cat})=\\begin{bmatrix} 0.8 \\\\ -1.2 \\end{bmatrix}\\)\n\\(\\log(\\sigma_\\phi(\\mathbf{x}_{cat})^2)=\\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix}\\)\nFrom the log-variances, we calculate the variances:\n\\(\\sigma_1^2 = \\exp(-0.2) \\approx 0.8187\\) \\(\\sigma_2^2 = \\exp(0.1) \\approx 1.1052\\)\nSo, for this input \\(\\mathbf{x}_{cat}\\), the encoder defines the latent distribution: \\[q_\\phi(\\mathbf{z}|\\mathbf{x}_{cat})=\\mathcal{N}\\left(\\mathbf{z};\\begin{bmatrix} 0.8 \\\\ -1.2 \\end{bmatrix},\\begin{bmatrix} 0.8187 & 0 \\\\ 0 & 1.1052 \\end{bmatrix}\\right)\\]\nThis means:\n\nThe first latent dimension (\\(z_1\\)) is modeled by a Gaussian with mean 0.8 and variance 0.8187.\nThe second latent dimension (\\(z_2\\)) is modeled by a Gaussian with mean \\(-1.2\\) and variance 1.1052.\n\nThese two dimensions are assumed to be independent (because the off-diagonal elements of the covariance matrix are zero).\nWhen we ‚Äúsample \\(\\mathbf{z}\\) from \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_{cat})\\)‚Äù using the reparameterization trick, we would:\n\nSample \\(\\epsilon_1 \\sim \\mathcal{N}(0,1)\\) and \\(\\epsilon_2 \\sim \\mathcal{N}(0,1)\\).\nCalculate \\(z_1 = 0.8 + \\sqrt{0.8187} \\cdot \\epsilon_1\\)\nCalculate \\(z_2 = -1.2 + \\sqrt{1.1052} \\cdot \\epsilon_2\\)\n\nThe resulting \\(\\mathbf{z}=\\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix}\\) is then passed to the decoder.\nThis formulation allows the VAE to learn a distribution for each input, rather than a single point, enabling the desired properties of a continuous and meaningful latent space.\n\n\n\nImagine a very simplified VAE where our latent space \\(\\mathbf{z}\\) is just one-dimensional (\\(D_z=1\\)). Our prior \\(p(\\mathbf{z})\\) is \\(\\mathcal{N}(0,1)\\) (mean 0, variance 1). The VAE‚Äôs job is to learn an encoder (\\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\)) and a decoder (\\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\)) such that:\n\nThe decoder can reconstruct \\(\\mathbf{x}_A\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\).\nThe decoder can reconstruct \\(\\mathbf{x}_B\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\).\nThe KL divergence \\(D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))\\) is minimized for both \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\).\n\nScenario 1: No KL Regularization (like a vanilla Autoencoder)\nIf there were no KL term, the encoder might learn to map \\(\\mathbf{x}_A\\) to a specific point \\(\\mathbf{z}_A\\) and \\(\\mathbf{x}_B\\) to a specific point \\(\\mathbf{z}_B\\).\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(\\mu_A=-5.0, \\sigma_A=0.001\\) (a very tight distribution at -5.0)\nEncoder output for \\(\\mathbf{x}_B\\): \\(\\mu_B=+5.0, \\sigma_B=0.001\\) (a very tight distribution at +5.0)\n\nScenario 2: With KL Regularization (VAE)\nNow, the KL term \\(D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x})||\\mathcal{N}(0,1))\\) is active.\nLet‚Äôs say the encoder tries to map \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) far apart again:\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(\\mu_A=-5.0, \\sigma_A=0.1\\)\nEncoder output for \\(\\mathbf{x}_B\\): \\(\\mu_B=+5.0, \\sigma_B=0.1\\)\n\nLet‚Äôs calculate the KL divergence for \\(\\mathbf{x}_A\\): \\[D_{KL}(\\mathcal{N}(-5.0,0.1^2)||\\mathcal{N}(0,1)) = \\frac{1}{2} (\\exp(2\\log(0.1))+(-5.0)^2-1-2\\log(0.1))\\] \\[= \\frac{1}{2} (0.01+25-1-(-4.6)) = \\frac{1}{2} (24.01+4.6)=14.3\\]\nThis KL value (14.3) is very high! The VAE loss function will strongly penalize this. To minimize this KL term, the encoder is forced to:\n\nPull the means towards 0: \\(\\mu_A\\) and \\(\\mu_B\\) must be closer to 0.\nPush the variances towards 1: \\(\\sigma_A\\) and \\(\\sigma_B\\) must be closer to 1.\n\nSo, for similar inputs \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\), the trained encoder might output:\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)=\\mathcal{N}(\\mathbf{z};\\mu_A=-0.5,\\sigma_A=0.8)\\)\nEncoder output for \\(\\mathbf{x}_B\\): \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)=\\mathcal{N}(\\mathbf{z};\\mu_B=+0.5,\\sigma_B=0.8)\\)\n\nNow, let‚Äôs evaluate the KL divergence again (for \\(\\mathbf{x}_A\\)): \\[D_{KL}(\\mathcal{N}(-0.5,0.8^2)||\\mathcal{N}(0,1)) = \\frac{1}{2} (\\exp(2\\log(0.8))+(-0.5)^2-1-2\\log(0.8))\\] \\[= \\frac{1}{2} (0.64+0.25-1-(-0.446))= \\frac{1}{2} (-0.11+0.446)=0.168\\]\nThis KL value (0.168) is much smaller! The KL regularization successfully compressed the latent distributions closer to the prior.\nWhy this helps:\nThe only way for the encoder to reconstruct accurately AND stay close to the prior when presented with similar inputs is to map them to overlapping distributions.\n\n\n\n\n\nLet‚Äôs consider two distinct but very similar input data points, \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\), both from training distribution\nPressure from KL Divergence: Make them similar\n\nFor \\(\\mathbf{x}_A\\), the encoder will produce \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)=\\mathcal{N}(\\mu_A,\\Sigma_A)\\). The KL term wants \\(\\mu_A \\approx 0\\) and \\(\\Sigma_A \\approx I\\).\nFor \\(\\mathbf{x}_B\\), the encoder will produce \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)=\\mathcal{N}(\\mu_B,\\Sigma_B)\\). The KL term wants \\(\\mu_B \\approx 0\\) and \\(\\Sigma_B \\approx I\\).\n\nThe mathematical consequence of minimizing \\(D_{KL}(Q||P)\\) is that \\(Q\\) is forced to be similar to \\(P\\). Since \\(P\\) is the same prior for all \\(\\mathbf{x}\\), this means all \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) distributions for any \\(\\mathbf{x}\\) are pulled towards the same central region of the latent space (around 0). This inherently forces them to be closer to each other than they might be in a vanilla autoencoder without this regularization.\nPressure from Reconstruction Loss: Make them distinct\nIf the encoder were to map \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) to identical distributions (e.g., \\(\\mu_A=\\mu_B=0\\) and \\(\\Sigma_A=\\Sigma_B=I\\)), the reconstruction loss would be high to penalize that.\nThe Interplay (The ‚ÄúDual Pressure‚Äù):\n\nThe KL term pushes all latent distributions for different \\(\\mathbf{x}\\) towards the same central region of the latent space and encourages them to have a certain ‚Äúspread‚Äù (variance \\(\\approx I\\)). This means they will naturally overlap.\nThe reconstruction term pulls these distributions slightly apart (or adjusts their means and variances) just enough so that the decoder can distinguish between similar inputs and reconstruct them accurately.\n\nThe balance between these two forces is key. The optimal solution is where the encoder maps similar inputs \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) to latent distributions \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\) and \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\) that are:\n\nClose to each other (due to KL regularization towards the common prior).\nSignificantly overlapping (due to the variances being pushed towards 1 by KL, and their means being close).\nSlightly distinct in their means/variances such that the decoder can still reconstruct \\(\\mathbf{x}_A\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\) and \\(\\mathbf{x}_B\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\) with low reconstruction error.\n\n\n\nA challenge arises because sampling \\(\\mathbf{z}\\) from \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is a stochastic operation, which makes backpropagation through the sampling step difficult. The reparameterization trick solves this. Instead of sampling \\(\\mathbf{z} \\sim \\mathcal{N}(\\mu,\\Sigma)\\), we sample an auxiliary random variable \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0,I)\\) and then compute:\n\\[\\mathbf{z} = \\mu_\\phi(\\mathbf{x}) + \\sigma_\\phi(\\mathbf{x}) \\odot \\boldsymbol{\\epsilon}\\] (where \\(\\odot\\) is element-wise multiplication, and \\(\\sigma_\\phi(\\mathbf{x})\\) is the standard deviation, often computed from log-variance to ensure positivity). Now, the stochasticity is moved to \\(\\boldsymbol{\\epsilon}\\), and \\(\\mathbf{z}\\) becomes a deterministic function of \\(\\mu\\), \\(\\sigma\\), and \\(\\boldsymbol{\\epsilon}\\), allowing gradients to flow back through \\(\\mu_\\phi(\\mathbf{x})\\) and \\(\\sigma_\\phi(\\mathbf{x})\\) to the encoder‚Äôs parameters \\(\\phi\\).\n\n\n\n\n\nGenerative Model: By sampling a \\(\\mathbf{z}\\) from the simple prior \\(p(\\mathbf{z})\\) (e.g., standard normal) and passing it through the decoder \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\), we can generate entirely new data that resembles the training data\nVariational Inference & Trade-off between Reconstruction and Regularization: The balance between the reconstruction loss and the KL divergence term is critical. If the KL term is too strong, the model might prioritize learning the prior over accurate reconstruction, leading to blurry reconstructions. If too weak, the latent space might not be well-regularized for generation. This balance is often controlled by a weighting factor (beta-VAE)."
  },
  {
    "objectID": "posts/autoencoder-dreamer/index.html#mathematical-formulation-of-vae",
    "href": "posts/autoencoder-dreamer/index.html#mathematical-formulation-of-vae",
    "title": "From Autoencoder to Dreamer",
    "section": "",
    "text": "Let‚Äôs denote:\n\n\\(\\mathbf{x}\\): An input data point (e.g., an image)\n\\(\\mathbf{z}\\): A latent variable (vector) in the lower-dimensional latent space\n\\(p(\\mathbf{x})\\): The true, unknown data distribution we want to model\n\\(p(\\mathbf{z})\\): The prior distribution over the latent variables (typically a simple distribution like \\(\\mathcal{N}(0,I)\\))\n\\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\): The decoder (also called generative model or likelihood). This is a neural network parameterized by \\(\\theta\\) that outputs the parameters of the distribution over \\(\\mathbf{x}\\) given \\(\\mathbf{z}\\).\n\\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\): The encoder (also called inference model or approximate posterior). This is a neural network parameterized by \\(\\phi\\) that outputs the parameters of the distribution over \\(\\mathbf{z}\\) given \\(\\mathbf{x}\\).\n\nGoal: is to maximize the marginal likelihood: \\[p(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})}\\] which we do not know the other element to compute, or \\[\\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z}) d\\mathbf{z}\\] which is intractable\nTherefore, VAEs optimize the Evidence Lower Bound (ELBO), which is a lower bound on the log-likelihood of the data, we will derive it now\n\n\nThe log-likelihood of a data point \\(\\mathbf{x}\\) can be written as:\n\\[\\log p_\\theta(\\mathbf{x}) = \\log p_\\theta(\\mathbf{x})\\] \\[= \\log p_\\theta(\\mathbf{x}) \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) d\\mathbf{z} \\quad \\text{Multiply by 1}\\] \\[= \\int q_\\phi(\\mathbf{z}|\\mathbf{x}) \\log p_\\theta(\\mathbf{x}) d\\mathbf{z} \\quad \\text{Bring inside the integral}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x})] \\quad \\text{Definition of expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{p_\\theta(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Apply the equation } p_\\theta(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{p_\\theta(\\mathbf{z}|\\mathbf{x})}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z}) q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta(\\mathbf{z}|\\mathbf{x}) q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Multiply by 1}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} + \\log \\frac{q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Split the expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] + D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p_\\theta(\\mathbf{z}|\\mathbf{x})) \\quad \\text{KL divergence}\\]\nThe second term is the Kullback-Leibler (KL) divergence, it is non-negative, now we only focus on maximizing the first term ELBO:\n\\[\\log p_\\theta(\\mathbf{x}) \\ge \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}|\\mathbf{z}) p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Chain rule of probability}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] + \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p(\\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\quad \\text{Split the expectation}\\] \\[= \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z})) \\quad \\text{KL divergence}\\]\nThis is the objective function for the VAE (Loss Function), also the ELBO to maximize, or minimizing the negative ELBO.\n\\[\\mathcal{L}(\\theta, \\phi, \\mathbf{x}) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})} [\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) || p(\\mathbf{z}))\\]\n\n\n\nThis term encourages the decoder to reconstruct the input \\(\\mathbf{x}\\) accurately from a latent sample \\(\\mathbf{z}\\) drawn from the encoder‚Äôs output distribution.\nWe will derive it now\n\n\nWe assume that each data point \\(\\mathbf{x}\\) (e.g., an image, where each pixel is a dimension) is drawn from a Gaussian distribution, given the latent variable \\(\\mathbf{z}\\) that the decoder outputs. For simplicity, let‚Äôs assume the dimensions of \\(\\mathbf{x}\\) are independent given \\(\\mathbf{z}\\), and they share a fixed variance \\(\\sigma^2\\).\nSo, for each dimension \\(j\\) of \\(\\mathbf{x}\\), \\(x_j\\) is distributed as: \\(p_\\theta(x_j|\\mathbf{z}) = \\mathcal{N}(x_j; \\mu_j(\\mathbf{z}), \\sigma^2)\\)\nHere:\n\n\\(\\mu_j(\\mathbf{z})\\) is the mean for the \\(j\\)-th dimension, which is the output of your decoder network for that dimension when given \\(\\mathbf{z}\\).\n\\(\\sigma^2\\) is the variance. For simplicity, we often assume a fixed \\(\\sigma^2\\) (e.g., \\(\\sigma^2=1\\), or a small constant, or even absorbed into the weights). More advanced VAEs can learn this variance as well (the decoder outputs both mean and variance).\n\n\n\n\nThe PDF for a single-dimensional Gaussian variable \\(x_j\\) is:\n\\(f(x_j) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right)\\)\n\n\n\nSince we assume the dimensions of \\(\\mathbf{x}\\) are independent given \\(\\mathbf{z}\\), the joint probability \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\) is the product of the individual probabilities:\n\\(p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\prod_{j=1}^{D_x} p_\\theta(x_j|\\mathbf{z})\\)\nwhere \\(D_x\\) is the dimensionality of \\(\\mathbf{x}\\) (e.g., number of pixels in an image). Now, let‚Äôs take the logarithm of this product:\n\\[\\log p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\log \\left( \\prod_{j=1}^{D_x} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\log\\left(\\exp\\left(-\\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2}\\right)\\right) \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2} \\right)\\] \\[= \\sum_{j=1}^{D_x} \\left( -\\frac{1}{2}\\log(2\\pi\\sigma^2) \\right) - \\sum_{j=1}^{D_x} \\left( \\frac{(x_j - \\mu_j(\\mathbf{z}))^2}{2\\sigma^2} \\right)\\]The first term is a constant multiplied by \\(D_x\\): \\[= -\\frac{D_x}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{D_x} (x_j - \\mu_j(\\mathbf{z}))^2\\] \\[\\text{MSE} = \\frac{1}{D_x} \\sum_{j=1}^{D_x} (x_j - \\mu_j(\\mathbf{z}))^2\\]\n\n\n\n\nThis term acts as a regularizer. It pushes the approximate posterior \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) (the distribution output by the encoder for a given \\(\\mathbf{x}\\)) to be close to the prior distribution \\(p(\\mathbf{z})\\) (e.g., \\(\\mathcal{N}(0,I)\\)).\nIf \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is assumed to be a diagonal Gaussian \\(\\mathcal{N}(\\mu, \\Sigma)\\) (where \\(\\Sigma\\) is diagonal) and \\(p(\\mathbf{z})\\) is \\(\\mathcal{N}(0,I)\\), the KL divergence has a closed-form solution:\n\\[D_{KL}(\\mathcal{N}(\\mu,\\Sigma)||\\mathcal{N}(0,I)) = \\frac{1}{2} \\sum_{j=1}^{D_z} (\\exp(\\sigma_j) + \\mu_j^2 - 1 - \\sigma_j)\\] where \\(D_z\\) is the dimensionality of \\(\\mathbf{z}\\), and \\(\\mu_j\\) and \\(\\sigma_j\\) are the mean and log-variance (diagonal elements of \\(\\Sigma\\)) for the \\(j\\)-th latent dimension, as output by the encoder.\n\n\nLet‚Äôs say our latent space \\(\\mathbf{z}\\) is 2-dimensional (\\(D_z=2\\)). Our input is a specific image \\(\\mathbf{x}_{cat}\\) (an image of a cat).\nThe encoder network takes \\(\\mathbf{x}_{cat}\\) as input. Its output layer (after processing through several hidden layers) has two sets of \\(D_z=2\\) nodes:\n\nMean Output Nodes: For \\(\\mu_\\phi(\\mathbf{x}_{cat})\\)\nLog-Variance Output Nodes: For \\(\\log(\\sigma_\\phi(\\mathbf{x}_{cat})^2)\\) (we use log-variance for numerical stability, as variance must be positive).\n\nSuppose for this specific \\(\\mathbf{x}_{cat}\\), the encoder outputs:\n\\(\\mu_\\phi(\\mathbf{x}_{cat})=\\begin{bmatrix} 0.8 \\\\ -1.2 \\end{bmatrix}\\)\n\\(\\log(\\sigma_\\phi(\\mathbf{x}_{cat})^2)=\\begin{bmatrix} -0.2 \\\\ 0.1 \\end{bmatrix}\\)\nFrom the log-variances, we calculate the variances:\n\\(\\sigma_1^2 = \\exp(-0.2) \\approx 0.8187\\) \\(\\sigma_2^2 = \\exp(0.1) \\approx 1.1052\\)\nSo, for this input \\(\\mathbf{x}_{cat}\\), the encoder defines the latent distribution: \\[q_\\phi(\\mathbf{z}|\\mathbf{x}_{cat})=\\mathcal{N}\\left(\\mathbf{z};\\begin{bmatrix} 0.8 \\\\ -1.2 \\end{bmatrix},\\begin{bmatrix} 0.8187 & 0 \\\\ 0 & 1.1052 \\end{bmatrix}\\right)\\]\nThis means:\n\nThe first latent dimension (\\(z_1\\)) is modeled by a Gaussian with mean 0.8 and variance 0.8187.\nThe second latent dimension (\\(z_2\\)) is modeled by a Gaussian with mean \\(-1.2\\) and variance 1.1052.\n\nThese two dimensions are assumed to be independent (because the off-diagonal elements of the covariance matrix are zero).\nWhen we ‚Äúsample \\(\\mathbf{z}\\) from \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_{cat})\\)‚Äù using the reparameterization trick, we would:\n\nSample \\(\\epsilon_1 \\sim \\mathcal{N}(0,1)\\) and \\(\\epsilon_2 \\sim \\mathcal{N}(0,1)\\).\nCalculate \\(z_1 = 0.8 + \\sqrt{0.8187} \\cdot \\epsilon_1\\)\nCalculate \\(z_2 = -1.2 + \\sqrt{1.1052} \\cdot \\epsilon_2\\)\n\nThe resulting \\(\\mathbf{z}=\\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix}\\) is then passed to the decoder.\nThis formulation allows the VAE to learn a distribution for each input, rather than a single point, enabling the desired properties of a continuous and meaningful latent space.\n\n\n\nImagine a very simplified VAE where our latent space \\(\\mathbf{z}\\) is just one-dimensional (\\(D_z=1\\)). Our prior \\(p(\\mathbf{z})\\) is \\(\\mathcal{N}(0,1)\\) (mean 0, variance 1). The VAE‚Äôs job is to learn an encoder (\\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\)) and a decoder (\\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\)) such that:\n\nThe decoder can reconstruct \\(\\mathbf{x}_A\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\).\nThe decoder can reconstruct \\(\\mathbf{x}_B\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\).\nThe KL divergence \\(D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x})||p(\\mathbf{z}))\\) is minimized for both \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\).\n\nScenario 1: No KL Regularization (like a vanilla Autoencoder)\nIf there were no KL term, the encoder might learn to map \\(\\mathbf{x}_A\\) to a specific point \\(\\mathbf{z}_A\\) and \\(\\mathbf{x}_B\\) to a specific point \\(\\mathbf{z}_B\\).\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(\\mu_A=-5.0, \\sigma_A=0.001\\) (a very tight distribution at -5.0)\nEncoder output for \\(\\mathbf{x}_B\\): \\(\\mu_B=+5.0, \\sigma_B=0.001\\) (a very tight distribution at +5.0)\n\nScenario 2: With KL Regularization (VAE)\nNow, the KL term \\(D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x})||\\mathcal{N}(0,1))\\) is active.\nLet‚Äôs say the encoder tries to map \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) far apart again:\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(\\mu_A=-5.0, \\sigma_A=0.1\\)\nEncoder output for \\(\\mathbf{x}_B\\): \\(\\mu_B=+5.0, \\sigma_B=0.1\\)\n\nLet‚Äôs calculate the KL divergence for \\(\\mathbf{x}_A\\): \\[D_{KL}(\\mathcal{N}(-5.0,0.1^2)||\\mathcal{N}(0,1)) = \\frac{1}{2} (\\exp(2\\log(0.1))+(-5.0)^2-1-2\\log(0.1))\\] \\[= \\frac{1}{2} (0.01+25-1-(-4.6)) = \\frac{1}{2} (24.01+4.6)=14.3\\]\nThis KL value (14.3) is very high! The VAE loss function will strongly penalize this. To minimize this KL term, the encoder is forced to:\n\nPull the means towards 0: \\(\\mu_A\\) and \\(\\mu_B\\) must be closer to 0.\nPush the variances towards 1: \\(\\sigma_A\\) and \\(\\sigma_B\\) must be closer to 1.\n\nSo, for similar inputs \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\), the trained encoder might output:\n\nEncoder output for \\(\\mathbf{x}_A\\): \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)=\\mathcal{N}(\\mathbf{z};\\mu_A=-0.5,\\sigma_A=0.8)\\)\nEncoder output for \\(\\mathbf{x}_B\\): \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)=\\mathcal{N}(\\mathbf{z};\\mu_B=+0.5,\\sigma_B=0.8)\\)\n\nNow, let‚Äôs evaluate the KL divergence again (for \\(\\mathbf{x}_A\\)): \\[D_{KL}(\\mathcal{N}(-0.5,0.8^2)||\\mathcal{N}(0,1)) = \\frac{1}{2} (\\exp(2\\log(0.8))+(-0.5)^2-1-2\\log(0.8))\\] \\[= \\frac{1}{2} (0.64+0.25-1-(-0.446))= \\frac{1}{2} (-0.11+0.446)=0.168\\]\nThis KL value (0.168) is much smaller! The KL regularization successfully compressed the latent distributions closer to the prior.\nWhy this helps:\nThe only way for the encoder to reconstruct accurately AND stay close to the prior when presented with similar inputs is to map them to overlapping distributions."
  },
  {
    "objectID": "posts/autoencoder-dreamer/index.html#how-two-terms-work-with-each-other-to-ensure-a-smooth-generative-model-for-unseen-data",
    "href": "posts/autoencoder-dreamer/index.html#how-two-terms-work-with-each-other-to-ensure-a-smooth-generative-model-for-unseen-data",
    "title": "From Autoencoder to Dreamer",
    "section": "",
    "text": "Let‚Äôs consider two distinct but very similar input data points, \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\), both from training distribution\nPressure from KL Divergence: Make them similar\n\nFor \\(\\mathbf{x}_A\\), the encoder will produce \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)=\\mathcal{N}(\\mu_A,\\Sigma_A)\\). The KL term wants \\(\\mu_A \\approx 0\\) and \\(\\Sigma_A \\approx I\\).\nFor \\(\\mathbf{x}_B\\), the encoder will produce \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)=\\mathcal{N}(\\mu_B,\\Sigma_B)\\). The KL term wants \\(\\mu_B \\approx 0\\) and \\(\\Sigma_B \\approx I\\).\n\nThe mathematical consequence of minimizing \\(D_{KL}(Q||P)\\) is that \\(Q\\) is forced to be similar to \\(P\\). Since \\(P\\) is the same prior for all \\(\\mathbf{x}\\), this means all \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) distributions for any \\(\\mathbf{x}\\) are pulled towards the same central region of the latent space (around 0). This inherently forces them to be closer to each other than they might be in a vanilla autoencoder without this regularization.\nPressure from Reconstruction Loss: Make them distinct\nIf the encoder were to map \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) to identical distributions (e.g., \\(\\mu_A=\\mu_B=0\\) and \\(\\Sigma_A=\\Sigma_B=I\\)), the reconstruction loss would be high to penalize that.\nThe Interplay (The ‚ÄúDual Pressure‚Äù):\n\nThe KL term pushes all latent distributions for different \\(\\mathbf{x}\\) towards the same central region of the latent space and encourages them to have a certain ‚Äúspread‚Äù (variance \\(\\approx I\\)). This means they will naturally overlap.\nThe reconstruction term pulls these distributions slightly apart (or adjusts their means and variances) just enough so that the decoder can distinguish between similar inputs and reconstruct them accurately.\n\nThe balance between these two forces is key. The optimal solution is where the encoder maps similar inputs \\(\\mathbf{x}_A\\) and \\(\\mathbf{x}_B\\) to latent distributions \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\) and \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\) that are:\n\nClose to each other (due to KL regularization towards the common prior).\nSignificantly overlapping (due to the variances being pushed towards 1 by KL, and their means being close).\nSlightly distinct in their means/variances such that the decoder can still reconstruct \\(\\mathbf{x}_A\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_A)\\) and \\(\\mathbf{x}_B\\) from samples of \\(q_\\phi(\\mathbf{z}|\\mathbf{x}_B)\\) with low reconstruction error.\n\n\n\nA challenge arises because sampling \\(\\mathbf{z}\\) from \\(q_\\phi(\\mathbf{z}|\\mathbf{x})\\) is a stochastic operation, which makes backpropagation through the sampling step difficult. The reparameterization trick solves this. Instead of sampling \\(\\mathbf{z} \\sim \\mathcal{N}(\\mu,\\Sigma)\\), we sample an auxiliary random variable \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0,I)\\) and then compute:\n\\[\\mathbf{z} = \\mu_\\phi(\\mathbf{x}) + \\sigma_\\phi(\\mathbf{x}) \\odot \\boldsymbol{\\epsilon}\\] (where \\(\\odot\\) is element-wise multiplication, and \\(\\sigma_\\phi(\\mathbf{x})\\) is the standard deviation, often computed from log-variance to ensure positivity). Now, the stochasticity is moved to \\(\\boldsymbol{\\epsilon}\\), and \\(\\mathbf{z}\\) becomes a deterministic function of \\(\\mu\\), \\(\\sigma\\), and \\(\\boldsymbol{\\epsilon}\\), allowing gradients to flow back through \\(\\mu_\\phi(\\mathbf{x})\\) and \\(\\sigma_\\phi(\\mathbf{x})\\) to the encoder‚Äôs parameters \\(\\phi\\)."
  },
  {
    "objectID": "posts/autoencoder-dreamer/index.html#key-points-of-vae",
    "href": "posts/autoencoder-dreamer/index.html#key-points-of-vae",
    "title": "From Autoencoder to Dreamer",
    "section": "",
    "text": "Generative Model: By sampling a \\(\\mathbf{z}\\) from the simple prior \\(p(\\mathbf{z})\\) (e.g., standard normal) and passing it through the decoder \\(p_\\theta(\\mathbf{x}|\\mathbf{z})\\), we can generate entirely new data that resembles the training data\nVariational Inference & Trade-off between Reconstruction and Regularization: The balance between the reconstruction loss and the KL divergence term is critical. If the KL term is too strong, the model might prioritize learning the prior over accurate reconstruction, leading to blurry reconstructions. If too weak, the latent space might not be well-regularized for generation. This balance is often controlled by a weighting factor (beta-VAE)."
  },
  {
    "objectID": "posts/autoencoder-dreamer/index.html#modules-involved",
    "href": "posts/autoencoder-dreamer/index.html#modules-involved",
    "title": "From Autoencoder to Dreamer",
    "section": "Modules Involved:",
    "text": "Modules Involved:\n\nEncoder (Representation Model): \\(q_\\phi(s_t|h_t,o_t)\\) maps observation \\(o_t\\) and recurrent state \\(h_t\\) to posterior distribution parameters (\\(\\mu_{post},\\sigma_{post}\\)) for \\(s_t\\).\nDecoder (Observation Model): \\(p_\\theta(o_t|s_t)\\) maps stochastic state \\(s_t\\) to observation reconstruction parameters.\nReward Model: \\(p_\\theta(r_t|s_t)\\) maps stochastic state \\(s_t\\) to reward prediction parameters.\nRecurrent Model (Deterministic Dynamics): \\(f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})\\) updates \\(h_t\\).\nPrior Model (Transition Dynamics): \\(p(s_t|h_t)\\) maps recurrent state \\(h_t\\) to prior distribution parameters (\\(\\mu_{prior},\\sigma_{prior}\\)) for \\(s_t\\).\nActor (Policy Network): \\(p_\\psi(a_t|s_t)\\) maps stochastic state \\(s_t\\) to action distribution parameters.\nCritic (Value Network): \\(V_\\psi(s_t)\\) predicts the expected future value from state \\(s_t\\)."
  },
  {
    "objectID": "posts/autoencoder-dreamer/index.html#training-steps-one-iteration",
    "href": "posts/autoencoder-dreamer/index.html#training-steps-one-iteration",
    "title": "From Autoencoder to Dreamer",
    "section": "Training Steps (One Iteration):",
    "text": "Training Steps (One Iteration):\n\nPhase 1: Data Collection (Real-World Interaction)\n\nObserve Current State: The agent receives the current observation \\(o_t\\) from the real environment.\nInfer Latent State (Encoder): Using the Encoder and Recurrent Model, infer the current stochastic latent state \\(s_t\\) and update the deterministic hidden state \\(h_t\\):\n\n\\(h_t=f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})\\) (using the last actual state and action from the environment)\n\\(q(s_t|h_t,o_t)\\) is computed by the Encoder. A sample \\(s_t\\) is drawn (using the reparameterization trick) from this posterior distribution.\n\nChoose Action (Actor): Feed the inferred latent state \\(s_t\\) to the Actor (Policy Network) \\(p_\\psi(a_t|s_t)\\) to get an action distribution. Sample an action \\(a_t\\) from this distribution (e.g., using \\(\\epsilon\\)-greedy for exploration or pure sampling for stochastic policies).\nExecute Action: Execute action \\(a_t\\) in the real environment.\nReceive Feedback: Get the next observation \\(o_{t+1}\\) and reward \\(r_{t+1}\\) from the environment.\nStore Experience: Store the tuple \\((o_t,a_t,r_{t+1},o_{t+1})\\) in a replay buffer.\n\n\n\nPhase 2: Model Training (World Model Update)\nThis phase happens after collecting a batch of new experiences (e.g., a short trajectory or a few steps) or can be done continuously from the replay buffer.\n\nSample Batch: Sample a batch of (e.g., 50-step) sequences from the replay buffer.\nProcess Sequence (RSSM Pass): For each sequence in the batch, process it through the Recurrent State-Space Model (RSSM):\n\nFor each step \\(t\\) in the sequence:\n\nUpdate Recurrent State: \\(h_t=f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})\\)\nCompute Prior: \\(p(s_t|h_t)\\) (using the Prior Model)\nCompute Posterior: \\(q(s_t|h_t,o_t)\\) (using the Encoder and the observed \\(o_t\\))\nSample Latent State: Sample \\(s_t\\) from the posterior \\(q(s_t|h_t,o_t)\\) (using reparameterization trick for gradients).\n\n\nCalculate World Model Loss: The total world model loss is calculated for the entire batch. It typically combines three terms for each step \\(t\\):\n\nReconstruction Loss: \\(Loss_{obs} = -\\log p_\\theta(o_t|s_t)\\) (from Decoder). This encourages \\(s_t\\) to contain enough information to reconstruct the observation.\nReward Loss: \\(Loss_{reward} = -\\log p_\\theta(r_t|s_t)\\) (from Reward Model). This encourages \\(s_t\\) to contain enough information to predict the reward.\nKL Divergence (Consistency Loss): \\(Loss_{KL} = D_{KL}(q(s_t|h_t,o_t)||p(s_t|h_t))\\). This is crucial:\n\nIt regularizes the posterior \\(q\\) towards the prior \\(p\\).\nIt also forces the Prior Model to learn to accurately predict the next stochastic state before seeing the actual observation, by pulling it closer to the posterior that does see the observation. This is key for effective imagination.\n\nTotal World Model Loss: \\(L_{WM} = \\sum_t (Loss_{obs} + Loss_{reward} + \\beta \\cdot Loss_{KL})\\) (where \\(\\beta\\) is a KL weight, often 1 or adjusted).\n\nOptimize World Model: Perform an optimization step (e.g., Adam) to update the parameters of the Encoder, Decoder, Reward Model, Recurrent Model, and Prior Model based on \\(L_{WM}\\).\n\n\n\nPhase 3: Behavior Learning (Policy & Value Update by Imagination)\nThis phase also happens concurrently with World Model training, typically after a few world model updates. It leverages the current learned world model.\n\nSample Initial States: Sample a batch of latent states \\(s_k\\) from recent past experiences in the replay buffer. These serve as starting points for imagined trajectories.\nImagine Trajectories: For each sampled \\(s_k\\):\n\nRollout: Use the learned Prior Model (\\(p(s_{t+1}|h_{t+1})\\)) and Recurrent Model (\\(h_{t+1}=f_{recurrent}(\\dots)\\)) to simulate forward in the latent space for \\(H\\) steps (the imagination horizon).\nAct in Imagination: At each step \\(t'\\) in the imagination, the Actor \\(p_\\psi(a_{t'}|s_{t'})\\) chooses an action \\(a_{t'}\\).\nPredict Reward: The Reward Model \\(p_\\theta(r_{t'}|s_{t'})\\) predicts the reward for that step.\nPredict Value: The Critic \\(V_\\psi(s_{t'})\\) predicts the value of the future state.\nThis generates an imagined trajectory of states (\\(s_k,s'_{k+1},\\dots,s'_{k+H}\\)) and rewards (\\(r'_k,\\dots,r'_{k+H}\\)).\n\nCalculate Policy Loss:\n\nValue Target: Calculate value targets (e.g., GAE-lambda returns or N-step returns) using the predicted imagined rewards \\(r'_{t'}\\) and the Critic‚Äôs predicted values \\(V_\\psi(s'_{t'})\\) along the imagined trajectory.\nActor Loss: Optimize the Actor to maximize the expected value from imagined trajectories. This typically involves maximizing the expected return from actions chosen by the policy. Gradients flow back through the entire imagined sequence and the dynamics of the world model.\nCritic Loss: Optimize the Critic to accurately predict the value of imagined states, typically using an MSE loss between its prediction and the calculated value targets.\nTotal Policy Loss: \\(L_{Policy} = \\text{Actor Loss} + \\text{Critic Loss}\\)\n\nOptimize Policy & Value Networks: Perform an optimization step (e.g., Adam) to update the parameters of the Actor and Critic Networks based on \\(L_{Policy}\\).\n\n\n1. Parameters We Are Trying to Learn (The \\(\\psi\\) Parameters)\nIn the behavior learning phase, we are trying to learn the parameters of two distinct neural networks:\n\nActor Network (Policy Network): \\(p_\\psi(a_t|s_t)\\)\n\nParameters: These are the weights and biases of the neural network that takes a latent state \\(s_t\\) as input and outputs a distribution over actions \\(a_t\\). Let‚Äôs denote these specific parameters as \\(\\psi_{actor}\\).\nPurpose: To learn a policy that chooses actions which maximize expected future rewards within the imagined world.\n\nCritic Network (Value Network): \\(V_\\psi(s_t)\\)\n\nParameters: These are the weights and biases of the neural network that takes a latent state \\(s_t\\) as input and outputs a single scalar value, representing the estimated expected future return (value) from that state. Let‚Äôs denote these specific parameters as \\(\\psi_{critic}\\).\nPurpose: To learn to accurately predict the ‚Äúgoodness‚Äù (value) of a given latent state. This value prediction is then used as a baseline and target for training the Actor.\n\n\nSo, when we talk about optimizing \\(\\psi\\), we are jointly optimizing \\(\\psi_{actor}\\) and \\(\\psi_{critic}\\).\n\n\n2. Loss Functions Summed Up (Mathematical Formulation)\nThe total policy loss, \\(L_{Policy}(\\psi)\\), is composed of two main parts, each designed to update its respective network:\n\\[L_{Policy}(\\psi) = \\underbrace{L_{Critic}(\\psi_{critic})}_{\\text{Value Estimation Loss}} + \\underbrace{L_{Actor}(\\psi_{actor})}_{\\text{Policy Improvement Loss}}\\]\nLet‚Äôs detail each:\n\nCritic Loss: \\(L_{Critic}(\\psi_{critic})\\)\nGoal: Make the Critic‚Äôs value predictions \\(V_{\\psi_{critic}}(s''_{t'})\\) as accurate as possible for the imagined states \\(s''_{t'}\\). ‚ÄúAccurate‚Äù here means matching the calculated value targets (or \\(\\lambda\\)-returns) \\(V_{target}(s''_{t'})\\).\nFormulation: It‚Äôs a standard Mean Squared Error (MSE) loss.\n\\[L_{Critic}(\\psi_{critic}) = \\frac{1}{M} \\sum_{m=1}^{M} (V_{\\psi_{critic}}(s_m'') - V_{target}(s_m''))^2\\]\nWhere:\n\n\\(M\\) is the total number of (state, target) pairs from all imagined trajectories in the current batch.\n\\(s_m''\\) is an imagined latent state at some time step \\(t'\\) from an imagined trajectory.\n\\(V_{\\psi_{critic}}(s_m'')\\): The Critic‚Äôs Prediction (The Thing We Want to Improve)\n\nWhat it is: This is the current output of your Critic neural network. It‚Äôs the network‚Äôs best guess or estimate of the expected sum of future discounted rewards that the agent will receive if it starts from the imagined latent state \\(s_m''\\) and then follows its current policy \\(p_{\\psi_{actor}}\\) thereafter.\nSource: It comes directly from the forward pass of the Critic network (\\(V_{\\psi_{critic}}\\)) with \\(s_m''\\) as input.\nPurpose in Loss: This is the value that we are trying to adjust during training. The Critic loss will compute how ‚Äúwrong‚Äù this prediction is compared to the target, and then use that error to update the Critic‚Äôs parameters \\(\\psi_{critic}\\) via backpropagation. We want to make this prediction get closer to the target value.\n\n\\(V_{target}(s_m'')\\) is the computed target value for \\(s_m''\\) (Ground Truth). This target is crucial and calculated using the imagined rewards \\(r''_{t'+1}\\) and the bootstrapped value predictions from the Critic itself, specifically the \\(\\lambda\\)-return (or GAE-based target): \\[ V_{target}(s_{t'}' ) = \\sum_{j=0}^{H-t'-1} (\\gamma\\lambda)^j r_{t'+j+1}' + (\\gamma\\lambda)^{H-t'} V_{\\psi_{critic}}(s_{k+H}') \\quad \\text{(simplified } \\lambda\\text{-return)}\\] More robustly, using GAE: \\[ V_{target}(s_{t'}' ) = V_{\\psi_{critic}}(s_{t'}' ) + A_{t'}^{\\text{GAE}(\\gamma, \\lambda)}\\] where \\(A_{t'}^{\\text{GAE}(\\gamma,\\lambda)} = \\sum_{j=0}^{H-t'-1} (\\gamma\\lambda)^j \\delta''_{t'+j}\\) and \\(\\delta''_{t'} = r''_{t'+1} + \\gamma V_{\\psi_{critic}}(s''_{t'+1}) - V_{\\psi_{critic}}(s''_{t'})\\).\n\n\n\nActor Loss: \\(L_{Actor}(\\psi_{actor})\\)\nGoal: Adjust the Actor‚Äôs policy parameters \\(\\psi_{actor}\\) so that actions that lead to higher rewards (higher advantages) become more probable. Also, encourage exploration (entropy).\nFormulation: This is a policy gradient loss, typically based on the REINFORCE algorithm but with the addition of a value baseline (the Critic‚Äôs prediction) and entropy regularization.\n\\[L_{Actor}(\\psi_{actor}) = -\\frac{1}{M} \\sum_{m=1}^{M} (A_m^{\\text{GAE}(\\gamma,\\lambda)} \\cdot \\log p_{\\psi_{actor}}(a_m''|s_m'') + \\beta_H \\cdot H(p_{\\psi_{actor}}(a_m''|s_m'')))\\]\nWhere:\n\n\\(M\\) is the total number of (state, action, advantage) triples from all imagined trajectories.\n\\(A_m^{\\text{GAE}(\\gamma,\\lambda)}\\) is the calculated Generalized Advantage Estimate for the imagined state \\(s_m''\\) and action \\(a_m''\\). This term serves as the ‚Äúcredit assignment‚Äù for the action.\n\\(\\log p_{\\psi_{actor}}(a_m''|s_m'')\\) is the log-probability of the imagined action \\(a_m''\\) under the current policy \\(p_{\\psi_{actor}}\\). This is the standard policy gradient term.\n\\(H(p_{\\psi_{actor}}(a_m''|s_m''))\\) is the entropy of the action distribution at state \\(s_m''\\).\n\\(\\beta_H\\) is a hyperparameter for entropy regularization. A positive \\(\\beta_H\\) means we add a negative entropy term to the loss, which encourages maximizing entropy (more exploration).\n\nI will explain more down here\n\n1. The Thing We Want to Improve: \\(\\log p_{\\psi_{actor}}(a_m''|s_m'')\\) (The Policy Itself)\n\nWhat it is: This term represents the log-probability of the action \\(a_m''\\) that was actually chosen by the Actor (policy) network for the imagined latent state \\(s_m''\\). The Actor network \\(p_{\\psi_{actor}}(a|s)\\) outputs an action distribution, and this is the log-likelihood of a specific action under that distribution.\nSource: It comes directly from a forward pass of the Actor network \\(p_{\\psi_{actor}}\\) given input \\(s_m''\\), and then evaluating the log-probability of \\(a_m''\\).\nPurpose in Loss: This is the core part of the policy that we are trying to adjust. By optimizing the Actor loss, we are trying to change the parameters \\(\\psi_{actor}\\) such that the probability of ‚Äúgood‚Äù actions increases, and the probability of ‚Äúbad‚Äù actions decreases.\n\n\n\n2. The ‚ÄúGround Truth‚Äù (The Guidance Signal): \\(A_m^{\\text{GAE}(\\gamma,\\lambda)}\\) (The Advantage)\n\nWhat it is: This is the Generalized Advantage Estimate (GAE) for the imagined state-action pair (\\(s_m'',a_m''\\)). The advantage function tells us how much better (or worse) a specific action \\(a_m''\\) taken in state \\(s_m''\\) was compared to the average expected value of that state \\(V_{\\psi_{critic}}(s_m'')\\).\n\nIf \\(A_m^{\\text{GAE}} &gt; 0\\): The action \\(a_m''\\) led to better-than-expected rewards.\nIf \\(A_m^{\\text{GAE}} &lt; 0\\): The action \\(a_m''\\) led to worse-than-expected rewards.\n\nSource: This is computed based on the imagined rewards \\(r''_{t'}\\) (from the Reward Model) and the Critic‚Äôs value predictions \\(V_{\\psi_{critic}}(s''_{t'})\\) (from the Critic Network).\nPurpose in Loss: This is the signal that tells the Actor whether the action it just took (in imagination) was good or bad. It serves as the ‚Äúground truth‚Äù in the sense that it‚Äôs the target direction and magnitude for policy improvement. We want to increase the likelihood of actions associated with positive advantages and decrease the likelihood of actions associated with negative advantages.\n\n\n\n3. The Entropy Term: \\(\\beta_H \\cdot H(p_{\\psi_{actor}}(a_m''|s_m''))\\)\n\nWhat it is: This is the entropy of the action distribution output by the Actor for state \\(s_m''\\). It measures the ‚Äúrandomness‚Äù or ‚Äúpredictability‚Äù of the policy. High entropy means the policy is more exploratory (less confident in a single action), low entropy means it‚Äôs more deterministic.\nSource: It‚Äôs calculated directly from the action distribution output by the Actor network.\nPurpose in Loss: This is a regularization term. It‚Äôs not about ‚Äúimproving a prediction‚Äù in the same way as the other terms. Instead, we typically want to maximize entropy (hence the negative sign in the loss formulation) to encourage exploration and prevent the policy from collapsing to a single action too quickly. This helps the agent continue to discover better strategies.\n\n\n\n\n\nHow the Loss Helps Learn \\(\\psi_{actor}\\)\nThe Actor loss, when minimized, works as follows:\n\nPolicy Gradient: The core term \\(A_m^{\\text{GAE}(\\gamma,\\lambda)} \\cdot \\log p_{\\psi_{actor}}(a_m''|s_m'')\\) is the standard policy gradient component. When we minimize the negative of this term:\n\nIf \\(A_m^{\\text{GAE}} &gt; 0\\) (good action): We want to increase \\(\\log p_{\\psi_{actor}}(a_m''|s_m'')\\), meaning we increase the probability of taking action \\(a_m''\\) in state \\(s_m''\\).\nIf \\(A_m^{\\text{GAE}} &lt; 0\\) (bad action): We want to decrease \\(\\log p_{\\psi_{actor}}(a_m''|s_m'')\\), meaning we decrease the probability of taking action \\(a_m''\\) in state \\(s_m''\\).\nThe magnitude of the advantage dictates the strength of this probability adjustment.\n\nEntropy Regularization: The \\(-\\beta_H \\cdot H(\\dots)\\) term ensures that even while the policy is being pushed towards high-advantage actions, it doesn‚Äôt become overly deterministic. It retains some level of randomness, which is beneficial for continued exploration."
  },
  {
    "objectID": "posts/learning-from-demonstration/index.html",
    "href": "posts/learning-from-demonstration/index.html",
    "title": "Learning from Demonstration",
    "section": "",
    "text": "What it is: Imitation Learning, also known as Learning from Demonstration (LfD) or Apprenticeship Learning, is a machine learning paradigm where an agent learns to perform a task by observing and mimicking the behavior of an expert (typically a human). Instead of relying on a reward function and trial-and-error (like in Reinforcement Learning), the agent is provided with demonstrations (e.g., recorded state-action pairs) and learns a policy that maps observed states to appropriate actions, attempting to replicate the expert‚Äôs decisions.\nWhy use it? * No Reward Function Needed: Many robotic tasks are difficult to define with precise, hand-engineered reward functions (e.g., ‚Äútie a shoelace‚Äù or ‚Äúdo surgery‚Äù). IL bypasses this problem. * Safety: It can be safer than pure trial-and-error RL, especially in real-world robotics where random exploration could cause damage. * Human Intuition: Leverages human intuition and dexterity for complex tasks that are easy for humans but hard to program explicitly.\nBasic Approach: Behavioral Cloning (BC) The simplest form of imitation learning is Behavioral Cloning. It treats the problem as a supervised learning task:\n\nData: Collect a dataset of \\((s,a)\\) pairs where \\(s\\) is an observed state and \\(a\\) is the action taken by the expert in that state.\nTraining: Train a policy (e.g., a neural network) to predict the expert‚Äôs action \\(a\\) given a state \\(s\\). This is typically done by minimizing the difference between the policy‚Äôs predicted action and the expert‚Äôs action (e.g., MSE for continuous actions, cross-entropy for discrete actions).\n\nKey Challenge of BC: Distribution Shift Behavioral cloning suffers from a critical problem called ‚Äúcovariate shift‚Äù or ‚Äúdistribution shift‚Äù. If the learned policy makes even a tiny mistake and deviates slightly from the expert‚Äôs trajectory, it might find itself in a state that was never seen in the expert demonstrations. Since it hasn‚Äôt been trained on what to do in such unseen states, its performance can quickly degrade, leading to compounding errors and failure. Imagine a self-driving car trained only on perfect lane-keeping; if it drifts slightly, it might not know how to correct because it‚Äôs never seen ‚Äúslightly off-center‚Äù states in the training data.\n\n\n\nWhat it is: DAgger (Dataset Aggregation) is an iterative imitation learning algorithm designed to mitigate the distribution shift problem of Behavioral Cloning. It was proposed by Ross, Gordon, and Bagnell in 2011.\nHow it works (Iterative Process): DAgger operates in a loop:\n\nInitial Policy Training: Train an initial policy \\(\\pi_i\\) using standard Behavioral Cloning on the current aggregated dataset \\(D=\\{(s,a_{\\text{expert}})_j\\}\\). Initially, \\(D\\) contains only the original expert demonstrations.\nPolicy Rollout: Deploy the current learned policy \\(\\pi_i\\) in the environment. The agent executes actions dictated by \\(\\pi_i\\) and collects a new trajectory of observations \\(s_t\\).\nExpert Intervention (Labeling): For every state \\(s_t\\) encountered during the policy‚Äôs rollout (even if the policy performed badly and got into an unseen state), query the human expert for the optimal action \\(a_{\\text{expert},t}\\) that the expert would have taken in that specific state \\(s_t\\).\nDataset Aggregation: Add these new \\((s_t,a_{\\text{expert},t})\\) pairs to the aggregated dataset \\(D\\). The crucial part is that \\(s_t\\) includes states the learner visited, not just states the expert initially demonstrated.\nRe-train Policy: Go back to step 1 and train a new policy \\(\\pi_{i+1}\\) on the expanded dataset \\(D\\).\n\nThis process repeats for several iterations.\nKey Difference from Pure BC: DAgger continuously adds data from states that the learner itself visits, with expert labels. This ensures that the policy gets explicit training on how to recover from its own mistakes and navigate states that are slightly off the expert trajectory, thus addressing the distribution shift problem. It makes the training distribution match the policy‚Äôs execution distribution over time.\nDrawback of DAgger: It requires online human supervision during the policy rollout phase. The expert needs to be available to label actions for potentially many states encountered by the struggling policy, which can be time-consuming and labor-intensive, especially for long-horizon tasks or real-time robotics.\n\n\n\nWhat it is: Action Chunking with Transformers (ACT) is a more recent imitation learning approach (2022) specifically designed for robotic manipulation, that leverages the power of Transformers and a technique called action chunking.\nKey Ideas:\n\nAction Chunking: Instead of predicting a single action at a time, ACT predicts a sequence or ‚Äúchunk‚Äù of future actions (\\(k\\) actions) given the current observation. This is an open-loop prediction for \\(k\\) steps.\n\nBenefit 1: Reduced Horizon: By predicting multiple steps, it effectively reduces the ‚Äúeffective horizon‚Äù of the control problem, making it easier for the model to capture temporal dependencies and long-term consequences.\nBenefit 2: Handles Non-Markovian Behavior: Human demonstrations can often be non-Markovian (meaning the optimal action depends on past context, not just the current state). By predicting a chunk, ACT implicitly incorporates more context.\nBenefit 3: Lower Inference Frequency: The policy can run at a lower frequency (e.g., predict 50 actions every 1 second instead of 1 action every 20ms), which is more practical for complex visuomotor policies.\n\nTransformer Architecture: It uses a Transformer (often a Conditional Variational Autoencoder - CVAE - with a Transformer backbone) to predict these action chunks. Transformers are excellent at modeling sequences and capturing long-range dependencies, which is well-suited for predicting future action sequences.\nTemporal Ensembling: To mitigate jerkiness from open-loop chunk execution, ACT uses temporal ensembling: if there are overlapping predictions from multiple chunks, it averages them to produce smoother actions.\n\nHow it‚Äôs different from DAgger:\n\nData Collection Paradigm: ACT is primarily a Behavioral Cloning-like approach. It trains on a fixed dataset of expert demonstrations (usually collected offline). It doesn‚Äôt inherently have DAgger‚Äôs interactive, iterative data aggregation loop with online expert labeling.\nAddressing Distribution Shift: DAgger explicitly solves distribution shift by querying the expert for out-of-distribution states. ACT addresses temporal aspects and complex behaviors through action chunking and Transformer‚Äôs sequential modeling capabilities, which helps with robustness, but it still fundamentally relies on the initial expert data. If ACT encounters truly novel, far-off-distribution states, it can still struggle, similar to pure BC.\nOutput: DAgger outputs a single action per state, iteratively refining it. ACT outputs a sequence of actions per state (chunking).\nComplexity: ACT uses a more complex neural network architecture (Transformers/CVAE) compared to the potentially simpler policies used in DAgger‚Äôs BC steps.\n\n\n\n\nWhat it is: Diffusion Policy (2023) is a cutting-edge approach to visuomotor policy learning that frames the problem of action generation as a conditional denoising diffusion process. Inspired by generative AI models (like DALL-E or Stable Diffusion for images), it learns to gradually refine a noisy action proposal into a coherent, expert-like action.\nKey Ideas:\n\nGenerative Model: Diffusion models are generative models that learn a data distribution by training to reverse a diffusion process (gradually adding noise). In Diffusion Policy, this means they learn the distribution of expert actions given a visual observation.\nIterative Denoising: During inference, the policy starts with a random noise vector (representing an initial ‚Äúnoisy‚Äù action) and iteratively refines it over several steps, guided by the learned diffusion model, to produce the final action.\nMultimodal Action Distributions: A significant advantage is its ability to elegantly handle multimodal action distributions. For example, if an expert can perform a task in multiple valid ways (e.g., pick up an object from left or right), a diffusion policy can learn to represent all these modes, rather than just averaging them out (which can happen in BC) or collapsing to a single mode.\nHigh-Dimensional Actions: They are well-suited for high-dimensional action spaces (e.g., controlling a complex robot arm with many joints).\nTraining Stability: Diffusion models are known for their stable training properties.\n\nHow it‚Äôs different from DAgger and ACT:\n\nUnderlying Mechanism: This is the biggest difference. DAgger and ACT are based on direct regression (BC-like, predicting actions directly). Diffusion Policy is a generative model that iteratively denoises actions.\nData Collection Paradigm: Diffusion Policy, like ACT, is primarily an offline imitation learning method. It learns from a fixed dataset of expert demonstrations. It does not inherently involve the online expert querying loop of DAgger.\nAddressing Multimodality: Diffusion Policy‚Äôs core strength is its ability to handle multimodal actions. DAgger and ACT (especially vanilla BC) can struggle if the expert has multiple ways of solving a task, as they might try to average actions or pick an arbitrary mode.\nTemporal Aspects: While ACT explicitly predicts chunks of actions for temporal consistency, Diffusion Policy can also be extended to predict sequences of actions (receding horizon control), leveraging the generative nature of diffusion models to ensure temporal coherence.\nOnline vs.¬†Offline: DAgger is fundamentally an online algorithm that requires iterative interaction. ACT and Diffusion Policy are primarily offline methods that learn from pre-recorded datasets, although variants like Diff-DAgger try to combine them.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nDAgger\nAction Chunk Transformation (ACT)\nDiffusion Policy\n\n\n\n\nCore Idea\nIterative data aggregation with online expert labeling to combat distribution shift.\nPredicts chunks of future actions using Transformers for temporal consistency.\nGenerative model that denoises action proposals, learning multimodal action distributions.\n\n\nLearning Type\nOnline, Interactive IL (iterative BC)\nOffline IL (advanced BC)\nOffline IL (generative model for actions)\n\n\nPrimary Output\nSingle action per state\nChunk (sequence) of actions per state\nSingle action (after denoising iterations) or a sequence of actions\n\n\nHandles OOD\nExplicitly, by querying expert in encountered OOD states.\nIndirectly, through chunking and Transformer‚Äôs capacity for sequential patterns; still susceptible to large OOD shifts.\nBetter generalization due to learning the distribution of actions, and robust to noise, but not inherently designed for OOD correction like DAgger. (However, Diff-DAgger explicitly combines them).\n\n\nHuman Effort\nHigh (online supervision)\nLow (offline data collection)\nLow (offline data collection)\n\n\nNetwork Type\nAny policy network (often MLPs/CNNs)\nTransformer (often CVAE-based)\nDiffusion model (often U-Net or Transformer backbone)\n\n\nMultimodality\nCan handle, but relies on expert to explicitly demonstrate all modes.\nCan struggle, might average modes.\nExcellent at handling and generating from multimodal action distributions.\n\n\n\nIn short, DAgger addresses the core online interaction and distribution shift problem. ACT focuses on improving long-term temporal coherence and learning from limited demonstrations through action chunking. Diffusion Policy tackles learning complex, often multimodal, action distributions by leveraging powerful generative models. Recent research (like Diff-DAgger) is exploring ways to combine the strengths of these different approaches."
  },
  {
    "objectID": "posts/learning-from-demonstration/index.html#imitation-learning-il",
    "href": "posts/learning-from-demonstration/index.html#imitation-learning-il",
    "title": "Learning from Demonstration",
    "section": "",
    "text": "What it is: Imitation Learning, also known as Learning from Demonstration (LfD) or Apprenticeship Learning, is a machine learning paradigm where an agent learns to perform a task by observing and mimicking the behavior of an expert (typically a human). Instead of relying on a reward function and trial-and-error (like in Reinforcement Learning), the agent is provided with demonstrations (e.g., recorded state-action pairs) and learns a policy that maps observed states to appropriate actions, attempting to replicate the expert‚Äôs decisions.\nWhy use it? * No Reward Function Needed: Many robotic tasks are difficult to define with precise, hand-engineered reward functions (e.g., ‚Äútie a shoelace‚Äù or ‚Äúdo surgery‚Äù). IL bypasses this problem. * Safety: It can be safer than pure trial-and-error RL, especially in real-world robotics where random exploration could cause damage. * Human Intuition: Leverages human intuition and dexterity for complex tasks that are easy for humans but hard to program explicitly.\nBasic Approach: Behavioral Cloning (BC) The simplest form of imitation learning is Behavioral Cloning. It treats the problem as a supervised learning task:\n\nData: Collect a dataset of \\((s,a)\\) pairs where \\(s\\) is an observed state and \\(a\\) is the action taken by the expert in that state.\nTraining: Train a policy (e.g., a neural network) to predict the expert‚Äôs action \\(a\\) given a state \\(s\\). This is typically done by minimizing the difference between the policy‚Äôs predicted action and the expert‚Äôs action (e.g., MSE for continuous actions, cross-entropy for discrete actions).\n\nKey Challenge of BC: Distribution Shift Behavioral cloning suffers from a critical problem called ‚Äúcovariate shift‚Äù or ‚Äúdistribution shift‚Äù. If the learned policy makes even a tiny mistake and deviates slightly from the expert‚Äôs trajectory, it might find itself in a state that was never seen in the expert demonstrations. Since it hasn‚Äôt been trained on what to do in such unseen states, its performance can quickly degrade, leading to compounding errors and failure. Imagine a self-driving car trained only on perfect lane-keeping; if it drifts slightly, it might not know how to correct because it‚Äôs never seen ‚Äúslightly off-center‚Äù states in the training data."
  },
  {
    "objectID": "posts/learning-from-demonstration/index.html#dagger-dataset-aggregation",
    "href": "posts/learning-from-demonstration/index.html#dagger-dataset-aggregation",
    "title": "Learning from Demonstration",
    "section": "",
    "text": "What it is: DAgger (Dataset Aggregation) is an iterative imitation learning algorithm designed to mitigate the distribution shift problem of Behavioral Cloning. It was proposed by Ross, Gordon, and Bagnell in 2011.\nHow it works (Iterative Process): DAgger operates in a loop:\n\nInitial Policy Training: Train an initial policy \\(\\pi_i\\) using standard Behavioral Cloning on the current aggregated dataset \\(D=\\{(s,a_{\\text{expert}})_j\\}\\). Initially, \\(D\\) contains only the original expert demonstrations.\nPolicy Rollout: Deploy the current learned policy \\(\\pi_i\\) in the environment. The agent executes actions dictated by \\(\\pi_i\\) and collects a new trajectory of observations \\(s_t\\).\nExpert Intervention (Labeling): For every state \\(s_t\\) encountered during the policy‚Äôs rollout (even if the policy performed badly and got into an unseen state), query the human expert for the optimal action \\(a_{\\text{expert},t}\\) that the expert would have taken in that specific state \\(s_t\\).\nDataset Aggregation: Add these new \\((s_t,a_{\\text{expert},t})\\) pairs to the aggregated dataset \\(D\\). The crucial part is that \\(s_t\\) includes states the learner visited, not just states the expert initially demonstrated.\nRe-train Policy: Go back to step 1 and train a new policy \\(\\pi_{i+1}\\) on the expanded dataset \\(D\\).\n\nThis process repeats for several iterations.\nKey Difference from Pure BC: DAgger continuously adds data from states that the learner itself visits, with expert labels. This ensures that the policy gets explicit training on how to recover from its own mistakes and navigate states that are slightly off the expert trajectory, thus addressing the distribution shift problem. It makes the training distribution match the policy‚Äôs execution distribution over time.\nDrawback of DAgger: It requires online human supervision during the policy rollout phase. The expert needs to be available to label actions for potentially many states encountered by the struggling policy, which can be time-consuming and labor-intensive, especially for long-horizon tasks or real-time robotics."
  },
  {
    "objectID": "posts/learning-from-demonstration/index.html#action-chunk-transformation-act",
    "href": "posts/learning-from-demonstration/index.html#action-chunk-transformation-act",
    "title": "Learning from Demonstration",
    "section": "",
    "text": "What it is: Action Chunking with Transformers (ACT) is a more recent imitation learning approach (2022) specifically designed for robotic manipulation, that leverages the power of Transformers and a technique called action chunking.\nKey Ideas:\n\nAction Chunking: Instead of predicting a single action at a time, ACT predicts a sequence or ‚Äúchunk‚Äù of future actions (\\(k\\) actions) given the current observation. This is an open-loop prediction for \\(k\\) steps.\n\nBenefit 1: Reduced Horizon: By predicting multiple steps, it effectively reduces the ‚Äúeffective horizon‚Äù of the control problem, making it easier for the model to capture temporal dependencies and long-term consequences.\nBenefit 2: Handles Non-Markovian Behavior: Human demonstrations can often be non-Markovian (meaning the optimal action depends on past context, not just the current state). By predicting a chunk, ACT implicitly incorporates more context.\nBenefit 3: Lower Inference Frequency: The policy can run at a lower frequency (e.g., predict 50 actions every 1 second instead of 1 action every 20ms), which is more practical for complex visuomotor policies.\n\nTransformer Architecture: It uses a Transformer (often a Conditional Variational Autoencoder - CVAE - with a Transformer backbone) to predict these action chunks. Transformers are excellent at modeling sequences and capturing long-range dependencies, which is well-suited for predicting future action sequences.\nTemporal Ensembling: To mitigate jerkiness from open-loop chunk execution, ACT uses temporal ensembling: if there are overlapping predictions from multiple chunks, it averages them to produce smoother actions.\n\nHow it‚Äôs different from DAgger:\n\nData Collection Paradigm: ACT is primarily a Behavioral Cloning-like approach. It trains on a fixed dataset of expert demonstrations (usually collected offline). It doesn‚Äôt inherently have DAgger‚Äôs interactive, iterative data aggregation loop with online expert labeling.\nAddressing Distribution Shift: DAgger explicitly solves distribution shift by querying the expert for out-of-distribution states. ACT addresses temporal aspects and complex behaviors through action chunking and Transformer‚Äôs sequential modeling capabilities, which helps with robustness, but it still fundamentally relies on the initial expert data. If ACT encounters truly novel, far-off-distribution states, it can still struggle, similar to pure BC.\nOutput: DAgger outputs a single action per state, iteratively refining it. ACT outputs a sequence of actions per state (chunking).\nComplexity: ACT uses a more complex neural network architecture (Transformers/CVAE) compared to the potentially simpler policies used in DAgger‚Äôs BC steps."
  },
  {
    "objectID": "posts/learning-from-demonstration/index.html#diffusion-policy",
    "href": "posts/learning-from-demonstration/index.html#diffusion-policy",
    "title": "Learning from Demonstration",
    "section": "",
    "text": "What it is: Diffusion Policy (2023) is a cutting-edge approach to visuomotor policy learning that frames the problem of action generation as a conditional denoising diffusion process. Inspired by generative AI models (like DALL-E or Stable Diffusion for images), it learns to gradually refine a noisy action proposal into a coherent, expert-like action.\nKey Ideas:\n\nGenerative Model: Diffusion models are generative models that learn a data distribution by training to reverse a diffusion process (gradually adding noise). In Diffusion Policy, this means they learn the distribution of expert actions given a visual observation.\nIterative Denoising: During inference, the policy starts with a random noise vector (representing an initial ‚Äúnoisy‚Äù action) and iteratively refines it over several steps, guided by the learned diffusion model, to produce the final action.\nMultimodal Action Distributions: A significant advantage is its ability to elegantly handle multimodal action distributions. For example, if an expert can perform a task in multiple valid ways (e.g., pick up an object from left or right), a diffusion policy can learn to represent all these modes, rather than just averaging them out (which can happen in BC) or collapsing to a single mode.\nHigh-Dimensional Actions: They are well-suited for high-dimensional action spaces (e.g., controlling a complex robot arm with many joints).\nTraining Stability: Diffusion models are known for their stable training properties.\n\nHow it‚Äôs different from DAgger and ACT:\n\nUnderlying Mechanism: This is the biggest difference. DAgger and ACT are based on direct regression (BC-like, predicting actions directly). Diffusion Policy is a generative model that iteratively denoises actions.\nData Collection Paradigm: Diffusion Policy, like ACT, is primarily an offline imitation learning method. It learns from a fixed dataset of expert demonstrations. It does not inherently involve the online expert querying loop of DAgger.\nAddressing Multimodality: Diffusion Policy‚Äôs core strength is its ability to handle multimodal actions. DAgger and ACT (especially vanilla BC) can struggle if the expert has multiple ways of solving a task, as they might try to average actions or pick an arbitrary mode.\nTemporal Aspects: While ACT explicitly predicts chunks of actions for temporal consistency, Diffusion Policy can also be extended to predict sequences of actions (receding horizon control), leveraging the generative nature of diffusion models to ensure temporal coherence.\nOnline vs.¬†Offline: DAgger is fundamentally an online algorithm that requires iterative interaction. ACT and Diffusion Policy are primarily offline methods that learn from pre-recorded datasets, although variants like Diff-DAgger try to combine them."
  },
  {
    "objectID": "posts/learning-from-demonstration/index.html#summary-of-differences",
    "href": "posts/learning-from-demonstration/index.html#summary-of-differences",
    "title": "Learning from Demonstration",
    "section": "",
    "text": "Feature\nDAgger\nAction Chunk Transformation (ACT)\nDiffusion Policy\n\n\n\n\nCore Idea\nIterative data aggregation with online expert labeling to combat distribution shift.\nPredicts chunks of future actions using Transformers for temporal consistency.\nGenerative model that denoises action proposals, learning multimodal action distributions.\n\n\nLearning Type\nOnline, Interactive IL (iterative BC)\nOffline IL (advanced BC)\nOffline IL (generative model for actions)\n\n\nPrimary Output\nSingle action per state\nChunk (sequence) of actions per state\nSingle action (after denoising iterations) or a sequence of actions\n\n\nHandles OOD\nExplicitly, by querying expert in encountered OOD states.\nIndirectly, through chunking and Transformer‚Äôs capacity for sequential patterns; still susceptible to large OOD shifts.\nBetter generalization due to learning the distribution of actions, and robust to noise, but not inherently designed for OOD correction like DAgger. (However, Diff-DAgger explicitly combines them).\n\n\nHuman Effort\nHigh (online supervision)\nLow (offline data collection)\nLow (offline data collection)\n\n\nNetwork Type\nAny policy network (often MLPs/CNNs)\nTransformer (often CVAE-based)\nDiffusion model (often U-Net or Transformer backbone)\n\n\nMultimodality\nCan handle, but relies on expert to explicitly demonstrate all modes.\nCan struggle, might average modes.\nExcellent at handling and generating from multimodal action distributions.\n\n\n\nIn short, DAgger addresses the core online interaction and distribution shift problem. ACT focuses on improving long-term temporal coherence and learning from limited demonstrations through action chunking. Diffusion Policy tackles learning complex, often multimodal, action distributions by leveraging powerful generative models. Recent research (like Diff-DAgger) is exploring ways to combine the strengths of these different approaches."
  },
  {
    "objectID": "posts/exam-touchsensing/index.html",
    "href": "posts/exam-touchsensing/index.html",
    "title": "Touch Sensing",
    "section": "",
    "text": "Human to feel the emotion & grasp objects\nRoboticscomplement sensor at local scale e.g:\n\nprecise direct local scale force+position+direction: grasping needs a sense of a correct force and direction applying on an object, too soft, will slip, too strong and not correct direction will also apply forces wrongly =&gt; also slip. Soley relying on visual might not be able to detect that because just 1-2 pixels.\navoid vision occlusion and noise: in dark environment, human rely on touching surrounding things in room\n\n=&gt; actually not widely yet because we havent had much proofs and experiment to demonstrate its proficiency but some works have been done. e.g:\n\nThe Feeling of Success 2017: complement 2 RGB-D cameras by 2 GelSight sensors -&gt; get object position from RGB-D, labels based on success lift or not-&gt; 83% success, only-vision only 56%\nGeneral In-hand Rotation with Vision and Touch 2023: rotation reward X,Y,Z"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#why-is-it-important",
    "href": "posts/exam-touchsensing/index.html#why-is-it-important",
    "title": "Touch Sensing",
    "section": "",
    "text": "Human to feel the emotion & grasp objects\nRoboticscomplement sensor at local scale e.g:\n\nprecise direct local scale force+position+direction: grasping needs a sense of a correct force and direction applying on an object, too soft, will slip, too strong and not correct direction will also apply forces wrongly =&gt; also slip. Soley relying on visual might not be able to detect that because just 1-2 pixels.\navoid vision occlusion and noise: in dark environment, human rely on touching surrounding things in room\n\n=&gt; actually not widely yet because we havent had much proofs and experiment to demonstrate its proficiency but some works have been done. e.g:\n\nThe Feeling of Success 2017: complement 2 RGB-D cameras by 2 GelSight sensors -&gt; get object position from RGB-D, labels based on success lift or not-&gt; 83% success, only-vision only 56%\nGeneral In-hand Rotation with Vision and Touch 2023: rotation reward X,Y,Z"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#mechanoreceptors",
    "href": "posts/exam-touchsensing/index.html#mechanoreceptors",
    "title": "Touch Sensing",
    "section": "Mechanoreceptors",
    "text": "Mechanoreceptors\nforce, position, pressure, vibration, stretch. Everything is in Dermis, but Epidermis is outer layer to protect, re-generating and healing:\n\nMerkel‚Äôs disk: skin upper layer, slowly adapting (at presence of static stimulus), balance structure =&gt; good for shape+texture identification & light touch\nMeissner‚Äôs corpuscle: also upper layer, but rapid adapting (at stimulus change, gradient of touch), vertical structure =&gt; good for pressure sensing + low frequency\nPacinian corpuscle: deep in skin layer, rapid adapting, BUT the round shape =&gt; high frequency sensing\nRuffini Endings: deep in skin layer, slow adapting, vertical structure =&gt; good for stretch detection"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#thermoreceptors-nocireceptors-pain-proprioreceptors-imu-chemoreceptors",
    "href": "posts/exam-touchsensing/index.html#thermoreceptors-nocireceptors-pain-proprioreceptors-imu-chemoreceptors",
    "title": "Touch Sensing",
    "section": "Thermoreceptors, Nocireceptors (pain), Proprioreceptors (IMU), Chemoreceptors",
    "text": "Thermoreceptors, Nocireceptors (pain), Proprioreceptors (IMU), Chemoreceptors"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#difficult-to-reconstruct",
    "href": "posts/exam-touchsensing/index.html#difficult-to-reconstruct",
    "title": "Touch Sensing",
    "section": "Difficult to reconstruct",
    "text": "Difficult to reconstruct\nSpatial Resolution + Normal Force Resolution currently human finger finer"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#requirements",
    "href": "posts/exam-touchsensing/index.html#requirements",
    "title": "Touch Sensing",
    "section": "Requirements",
    "text": "Requirements\n\nsmall =&gt; more challenging grasping tasks like surgery\nreliability =&gt; precise & robust\nmanufacture & affordable =&gt; more approachable for research and use"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#technology",
    "href": "posts/exam-touchsensing/index.html#technology",
    "title": "Touch Sensing",
    "section": "Technology",
    "text": "Technology\n\nCapacitive: Conductive touch represent a parallel capacitor to ground (multiple touch possible), but in many cases this would be wrongly intepreted (e.g: any conductive material noise but without any force like water)\nRessistive: material bend create change in resistance R = ro * l / S =&gt; widely used, cheap, insensitive to liquid, but no multi-touch\nPiezoelectric ion Breaking out generates electrical field at mechanical change because of breaking out balance state of + & - ions =&gt; electric movements =&gt; good for vibration\nMagnetic Magnetic Field change interparticle position changes =&gt; changes in magnetic field"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#depth-estimation-3d-reconstruction",
    "href": "posts/exam-touchsensing/index.html#depth-estimation-3d-reconstruction",
    "title": "Touch Sensing",
    "section": "Depth Estimation & 3D Reconstruction",
    "text": "Depth Estimation & 3D Reconstruction\nColor gradient (change in color by time) + Bend in markers"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#image-to-video",
    "href": "posts/exam-touchsensing/index.html#image-to-video",
    "title": "Touch Sensing",
    "section": "Image to Video",
    "text": "Image to Video\none image cannot do anything, mostly what we care is a change detection (force, slip) =&gt;\n\nRNN\nTransformer\nOptical Flow"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#force-detection",
    "href": "posts/exam-touchsensing/index.html#force-detection",
    "title": "Touch Sensing",
    "section": "Force detection",
    "text": "Force detection\n\nTouch detection was in exercise 3. input 2 digit images: =&gt; use simple ResNet or Vision Transformer:\n\nmany way to implement: either concatenate 2 images in the same input then feed into one network, but I was afraid the network has to learn differentiating 2 images separately, so I input each of them in separate CNN network, after that concatenate them into linear layers."
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#objectmaterial-classification",
    "href": "posts/exam-touchsensing/index.html#objectmaterial-classification",
    "title": "Touch Sensing",
    "section": "Object/Material Classification",
    "text": "Object/Material Classification\n\nCoin: coin classification in the exercise 2, I used vision transformer for image\nHardness: LSTM, each timeframe a CNN, if the frame features involve with bolder change by time =&gt; it is harder"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#object-property-regression",
    "href": "posts/exam-touchsensing/index.html#object-property-regression",
    "title": "Touch Sensing",
    "section": "Object property regression",
    "text": "Object property regression"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#slip-detection",
    "href": "posts/exam-touchsensing/index.html#slip-detection",
    "title": "Touch Sensing",
    "section": "Slip detection",
    "text": "Slip detection\n\nLSTM using 2 inputs at each frame (GelSight + external image), each frame and each input goes through a separate CNN"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#pose-estimation-prediction",
    "href": "posts/exam-touchsensing/index.html#pose-estimation-prediction",
    "title": "Touch Sensing",
    "section": "Pose Estimation & Prediction",
    "text": "Pose Estimation & Prediction\n\nTac2Pose: mkeypoint is feature matching of real tactile image with the closest simulated data collected, this kinda needs to know the objece model in advance\nSlip Prediction: seems to not yet well-studied"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#multi-modal-transfer",
    "href": "posts/exam-touchsensing/index.html#multi-modal-transfer",
    "title": "Touch Sensing",
    "section": "Multi-modal Transfer",
    "text": "Multi-modal Transfer\n\nGoal: use of multiple sensor:\nMethod: learning based, inputs are e.g.¬†GelSight, external iamge and go through CNN layers separately, then concatenate them in the same linear laers, output is just a classification of are they the same 1 or 0 (sigmoid cross entropy loss)\nthis is important so that in future we can utilize them in more complex system utilizing multiple sensor:\n\nacross sensors: DIGIT, GelSight,‚Ä¶\nacross tasks"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#grasping",
    "href": "posts/exam-touchsensing/index.html#grasping",
    "title": "Touch Sensing",
    "section": "Grasping",
    "text": "Grasping\n\nThe Feeling of Success 2017: input GelSight + external camera\nMore than a Feeling 2018 add temporal aspect + action input (at each timestep predicts the next timestep success rate), it is also where you figured out that applying more force does not always means better"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#manipulation",
    "href": "posts/exam-touchsensing/index.html#manipulation",
    "title": "Touch Sensing",
    "section": "Manipulation",
    "text": "Manipulation\n\nDIGIT 2020: MBRL similar to Dreamer, from my unterstanding only goal at one of 2 sensors\nGeneral In-Hand Rotation 2023:\nLearning to play Piano with touch 2022: MIDI converter+tactilesensing+proprioperceptive sensor RL"
  },
  {
    "objectID": "posts/exam-touchsensing/index.html#locomotion",
    "href": "posts/exam-touchsensing/index.html#locomotion",
    "title": "Touch Sensing",
    "section": "Locomotion",
    "text": "Locomotion\n\nprovide information about force & contact, but not so many works"
  },
  {
    "objectID": "posts/control/index.html",
    "href": "posts/control/index.html",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "A PID controller is a feedback control loop that continuously calculates an ‚Äúerror‚Äù value \\(e(t)\\) as the difference between a desired setpoint \\(r(t)\\) and a measured process variable \\(y(t)\\):\n\\[e(t) = r(t) - y(t)\\]\nBased on this error, the PID controller generates a control output \\(u(t)\\) by combining three distinct terms:\n\nProportional Term (\\(P\\)-term): Accounts for the current error.\nIntegral Term (\\(I\\)-term): Accounts for the accumulation of past errors.\nDerivative Term (\\(D\\)-term): Accounts for the rate of change of the error.\n\nCombining these, the continuous-time PID control law is given by:\n\\[u(t) = K_p e(t) + K_i \\int e(t) dt + K_d \\frac{de(t)}{dt}\\]\nWhere:\n\n\\(u(t)\\) is the controller‚Äôs output.\n\\(e(t)\\) is the error at time \\(t\\).\n\\(K_p\\) is the proportional gain.\n\\(K_i\\) is the integral gain.\n\\(K_d\\) is the derivative gain.\n\n\n\n\n\n\n\\[u_P(t) = K_p e(t)\\]\n\nPresent: If the error is large, the controller acts strongly; if the error is small, it acts weakly. =&gt; quickly drive the system towards the setpoint.\nGradual Loss Problem: There are some system where the output just naturally decays over time (like heat loss from a room, or friction in a motor). A simplified model could be, notice that for the output \\(y\\) to be maintained at a constant setpoint \\(r\\) (i.e., \\(\\frac{dy}{dt}=0\\)), the control input \\(u\\) must provide a continuous effort to compensate: \\(u_{required} = \\frac{ay}{b}\\). This system is the core problem:\n\\[\\frac{dy}{dt} = -ay + bu\\]\nSo what we require is that for this type of model to be at steady state, \\(\\frac{dy}{dt}=0 \\implies ay_{ss} = bu_{ss}\\).\nBut the problem is here!!! With P-control, we can only have \\(u_{ss} = K_p (r - y_{ss})\\) that kinda only acts based on the last timestep error. \\[ay_{ss} = bK_p (r - y_{ss})\\] \\[ay_{ss} = bK_pr - bK_py_{ss}\\] \\[y_{ss} = \\frac{K_p r}{(K_p + a/b)}\\]\nSince \\(a,b,K_p\\) are positive, \\(y_{ss}\\) will always be less than \\(r\\), meaning there will always be a non-zero steady-state error: \\(e_{ss} = r - y_{ss} \\ne 0\\).\nTherefore In this time of time-decaying systems, a sole P-Term simply cannot provide a sustained, non-zero output, simply because it only acts with the current error, and never act for the upcoming decay\n\n\n\n\n\\[u_I(t) = K_i \\int e(t) dt\\]\n\nCompensate the gradual Loss: Probably now you know what to do‚Ä¶ we push a little more, in such time-decaying system, we need continuous effort to to maintain the setpoint. It does this by continuously accumulating errors over time.\nAnalogy: You‚Äôre driving at 99 km/h when the limit is 100 km/h (small error). The P-term might give only a tiny gas pedal press. But you know, car on the street is exactly this type of time-decaying system (\\(\\frac{dy}{dt} = -ay + bu\\)) =&gt; To maintain 99 km/h for a long time, the I-term ‚Äúnotices‚Äù this persistent deficit and gradually pushes the gas pedal a little harder and holds it there until you finally reach 100 km/h.\nDrawback: The integral term can make the system slower to respond and potentially cause overshoot or oscillations if its gain \\(K_i\\) is set too high, because it‚Äôs reacting to past errors, not current or future ones.\nWhen does this accumulated stop? I would say almost never, because we have a time-decaying system, so we always need it.\nBUT, of course sometimes we want do stop overshooting it, therefore we have another term down here‚Ä¶\n\n\n\n\n\\[u_D(t) = K_d \\frac{de(t)}{dt}\\]\n\nAnticipation and Damping :This is really nice.\n\nIf the error is rapidly increasing (either negative or positive quantitatively), the D-term will counteract it quickly.\nIf the error is rapidly decreasing (meaning the system is approaching the setpoint quickly), the D-term will reduce the control action to prevent overshoot.\n\nAnalogy: You see a sharp turn (error changing rapidly) approaching in your car. You start braking before the turn to slow down smoothly and avoid overshooting the curve. Or, you‚Äôre speeding towards the 100 km/h limit; as you get closer, the D-term will gradually ease off the gas, preventing you from overshooting.\nBenefits: Reduces overshoot, reduces oscillations, and improves the transient response (how quickly and smoothly the system reaches the setpoint).\nDrawback: The D-term is very sensitive to noise in the measurement signal. Rapid changes in noisy signals can lead to large, jerky control actions."
  },
  {
    "objectID": "posts/control/index.html#mathematical-formulation",
    "href": "posts/control/index.html#mathematical-formulation",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "A PID controller is a feedback control loop that continuously calculates an ‚Äúerror‚Äù value \\(e(t)\\) as the difference between a desired setpoint \\(r(t)\\) and a measured process variable \\(y(t)\\):\n\\[e(t) = r(t) - y(t)\\]\nBased on this error, the PID controller generates a control output \\(u(t)\\) by combining three distinct terms:\n\nProportional Term (\\(P\\)-term): Accounts for the current error.\nIntegral Term (\\(I\\)-term): Accounts for the accumulation of past errors.\nDerivative Term (\\(D\\)-term): Accounts for the rate of change of the error.\n\nCombining these, the continuous-time PID control law is given by:\n\\[u(t) = K_p e(t) + K_i \\int e(t) dt + K_d \\frac{de(t)}{dt}\\]\nWhere:\n\n\\(u(t)\\) is the controller‚Äôs output.\n\\(e(t)\\) is the error at time \\(t\\).\n\\(K_p\\) is the proportional gain.\n\\(K_i\\) is the integral gain.\n\\(K_d\\) is the derivative gain."
  },
  {
    "objectID": "posts/control/index.html#but-why-p-i-and-d-why-not-just-use-the-current-error",
    "href": "posts/control/index.html#but-why-p-i-and-d-why-not-just-use-the-current-error",
    "title": "From Control to Model-based Learning",
    "section": "",
    "text": "\\[u_P(t) = K_p e(t)\\]\n\nPresent: If the error is large, the controller acts strongly; if the error is small, it acts weakly. =&gt; quickly drive the system towards the setpoint.\nGradual Loss Problem: There are some system where the output just naturally decays over time (like heat loss from a room, or friction in a motor). A simplified model could be, notice that for the output \\(y\\) to be maintained at a constant setpoint \\(r\\) (i.e., \\(\\frac{dy}{dt}=0\\)), the control input \\(u\\) must provide a continuous effort to compensate: \\(u_{required} = \\frac{ay}{b}\\). This system is the core problem:\n\\[\\frac{dy}{dt} = -ay + bu\\]\nSo what we require is that for this type of model to be at steady state, \\(\\frac{dy}{dt}=0 \\implies ay_{ss} = bu_{ss}\\).\nBut the problem is here!!! With P-control, we can only have \\(u_{ss} = K_p (r - y_{ss})\\) that kinda only acts based on the last timestep error. \\[ay_{ss} = bK_p (r - y_{ss})\\] \\[ay_{ss} = bK_pr - bK_py_{ss}\\] \\[y_{ss} = \\frac{K_p r}{(K_p + a/b)}\\]\nSince \\(a,b,K_p\\) are positive, \\(y_{ss}\\) will always be less than \\(r\\), meaning there will always be a non-zero steady-state error: \\(e_{ss} = r - y_{ss} \\ne 0\\).\nTherefore In this time of time-decaying systems, a sole P-Term simply cannot provide a sustained, non-zero output, simply because it only acts with the current error, and never act for the upcoming decay\n\n\n\n\n\\[u_I(t) = K_i \\int e(t) dt\\]\n\nCompensate the gradual Loss: Probably now you know what to do‚Ä¶ we push a little more, in such time-decaying system, we need continuous effort to to maintain the setpoint. It does this by continuously accumulating errors over time.\nAnalogy: You‚Äôre driving at 99 km/h when the limit is 100 km/h (small error). The P-term might give only a tiny gas pedal press. But you know, car on the street is exactly this type of time-decaying system (\\(\\frac{dy}{dt} = -ay + bu\\)) =&gt; To maintain 99 km/h for a long time, the I-term ‚Äúnotices‚Äù this persistent deficit and gradually pushes the gas pedal a little harder and holds it there until you finally reach 100 km/h.\nDrawback: The integral term can make the system slower to respond and potentially cause overshoot or oscillations if its gain \\(K_i\\) is set too high, because it‚Äôs reacting to past errors, not current or future ones.\nWhen does this accumulated stop? I would say almost never, because we have a time-decaying system, so we always need it.\nBUT, of course sometimes we want do stop overshooting it, therefore we have another term down here‚Ä¶\n\n\n\n\n\\[u_D(t) = K_d \\frac{de(t)}{dt}\\]\n\nAnticipation and Damping :This is really nice.\n\nIf the error is rapidly increasing (either negative or positive quantitatively), the D-term will counteract it quickly.\nIf the error is rapidly decreasing (meaning the system is approaching the setpoint quickly), the D-term will reduce the control action to prevent overshoot.\n\nAnalogy: You see a sharp turn (error changing rapidly) approaching in your car. You start braking before the turn to slow down smoothly and avoid overshooting the curve. Or, you‚Äôre speeding towards the 100 km/h limit; as you get closer, the D-term will gradually ease off the gas, preventing you from overshooting.\nBenefits: Reduces overshoot, reduces oscillations, and improves the transient response (how quickly and smoothly the system reaches the setpoint).\nDrawback: The D-term is very sensitive to noise in the measurement signal. Rapid changes in noisy signals can lead to large, jerky control actions."
  },
  {
    "objectID": "posts/control/index.html#example-lti-system-the-mass-spring-damper",
    "href": "posts/control/index.html#example-lti-system-the-mass-spring-damper",
    "title": "From Control to Model-based Learning",
    "section": "Example LTI System: The Mass-Spring-Damper",
    "text": "Example LTI System: The Mass-Spring-Damper\nSystem Description: This is a common second-order system. Consider a mass \\(m\\) (kg) connected to a spring with stiffness \\(k\\) (N/m) and a damper with damping coefficient \\(b\\) (Ns/m). An external force \\(F(t)\\) (N) is applied to the mass, causing a displacement \\(y(t)\\) (m) from its equilibrium position.\n1. Governing Differential Equation: Applying Newton‚Äôs Second Law (\\(\\sum F = m \\ddot{y}\\)) to the mass:\n\nApplied force: \\(+F(t)\\)\nSpring force (restoring): \\(-k y(t)\\)\nDamping force (opposing velocity): \\(-b \\dot{y}(t)\\)\n\nSo, we have now a second-order linear ordinary differential equation: \\[m\\ddot{y}(t) + b\\dot{y}(t) + k y(t) = F(t)\\] \\[m\\ddot{y}(t) = F(t) - b\\dot{y}(t) - k y(t)\\]\n2. Converting to State-Space Form: To convert this second-order equation into the first-order state-space form, we define state variables. A common choice is to pick the position and velocity as states:\n\n\\(x_1(t) = y(t)\\) (the position of the mass)\n\\(x_2(t) = \\dot{y}(t)\\) (the velocity of the mass)\n\nNow, we need to express the derivatives of these state variables in terms of the states themselves and the input \\(F(t)\\). \\[\\dot{x_1}(t) = \\dot{y}(t) = x_2(t)\\] \\[\\ddot{y}(t) = \\frac{1}{m} F(t) - \\frac{b}{m}\\dot{y}(t) - \\frac{k}{m}y(t)\\] Substituting our state variables (\\(y(t)=x_1(t)\\) and \\(\\dot{y}(t)=x_2(t)\\)) and our input \\(u(t) = F(t)\\): \\[\\dot{x_2}(t) = -\\frac{k}{m}x_1(t) - \\frac{b}{m}x_2(t) + \\frac{1}{m}u(t)\\]\n3. Writing in Matrix Form: Now, we can assemble these first-order equations into the state-space matrix form \\(\\dot{\\mathbf{x}}(t)=\\mathbf{A}\\mathbf{x}(t)+\\mathbf{B}\\mathbf{u}(t)\\):\n\\[\\begin{pmatrix} \\dot{x_1}(t) \\\\ \\dot{x_2}(t) \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -\\frac{k}{m} & -\\frac{b}{m} \\end{pmatrix} \\begin{pmatrix} x_1(t) \\\\ x_2(t) \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{1}{m} \\end{pmatrix} u(t)\\]\nFrom this, we can identify the statematrix \\(\\mathbf{A}\\) and input matrix \\(\\mathbf{B}\\): \\[\\mathbf{A} = \\begin{pmatrix} 0 & 1 \\\\ -\\frac{k}{m} & -\\frac{b}{m} \\end{pmatrix}, \\mathbf{B} = \\begin{pmatrix} 0 \\\\ \\frac{1}{m} \\end{pmatrix}\\]"
  },
  {
    "objectID": "posts/control/index.html#formulate-into-integral-of-terms-using-x-and-u",
    "href": "posts/control/index.html#formulate-into-integral-of-terms-using-x-and-u",
    "title": "From Control to Model-based Learning",
    "section": "Formulate into integral of terms using x and u",
    "text": "Formulate into integral of terms using x and u\nThe formal LQR problem is to find the optimal control input \\(\\mathbf{u}^*(t)\\) that minimizes the cost function \\(J\\), subject to the system dynamics:\n\\[\\text{Minimize } J = \\int_0^\\infty (\\mathbf{x}^T(t)\\mathbf{Q}\\mathbf{x}(t) + \\mathbf{u}^T(t)\\mathbf{R}\\mathbf{u}(t)) dt\\] \\[\\text{Subject to: } \\dot{\\mathbf{x}}(t) = \\mathbf{A}\\mathbf{x}(t) + \\mathbf{B}\\mathbf{u}(t)\\]\nOur goal is now, choosing the weighting matrices \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\), we can tune the controller to prioritize different aspects of performance:\n\nLarger \\(\\mathbf{Q}\\): driving states to zero quickly\nLarger \\(\\mathbf{R}\\): minimizing control effort\n\nYou know what? LQR is a linear state-feedback control system, so it people from long time ago has found out it also satisfies this form:\n\\[\\mathbf{u}(t) = -\\mathbf{K}\\mathbf{x}(t)\\]\nwell, we do not know what \\(K\\) is, we need to find \\(P\\) to calculate \\(K\\):\n\\[\\mathbf{K} = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\]\nBut \\(\\mathbf{P} \\in \\mathbb{R}^{n \\times n}\\) is also sth that we need to find.\nPeople long time ago just started by adding and subtracting \\(\\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0\\) from \\(J\\) to see what they can explore from here:\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 - \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty (\\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u}) dt\\]\nSubstituting the integral form of \\(-\\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 = \\int_0^\\infty ( \\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}))dt\\):\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) + \\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u} \\right) dt\\]\nNow we derive \\(\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x})\\). Since \\(\\mathbf{P}\\) is a constant, symmetric matrix (\\(\\mathbf{P} = \\mathbf{P}^T\\)):\n\\[\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) = \\dot{\\mathbf{x}}^T\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\dot{\\mathbf{x}}\\]\nNow, substitute \\(\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u}\\) into this expression:\n\\[\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) = (\\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u})^T\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}(\\mathbf{A}\\mathbf{x} + \\mathbf{B}\\mathbf{u})\\] \\[\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) = (\\mathbf{x}^T\\mathbf{A}^T + \\mathbf{u}^T\\mathbf{B}^T)\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{A}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u}\\] \\[\\frac{d}{dt}(\\mathbf{x}^T\\mathbf{P}\\mathbf{x}) = \\mathbf{x}^T\\mathbf{A}^T\\mathbf{P}\\mathbf{x} + \\mathbf{u}^T\\mathbf{B}^T\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{A}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u}\\]\nSubstitute this back into the expression for \\(J\\):\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\mathbf{x}^T\\mathbf{A}^T\\mathbf{P}\\mathbf{x} + \\mathbf{u}^T\\mathbf{B}^T\\mathbf{P}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{A}\\mathbf{x} + \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u} + \\mathbf{x}^T\\mathbf{Q}\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u} \\right) dt\\]\nNow, let‚Äôs group the terms. We can gather terms involving \\(\\mathbf{x}^T (\\cdot) \\mathbf{x}\\) and terms involving \\(\\mathbf{u}\\). Note that \\(\\mathbf{u}^T\\mathbf{B}^T\\mathbf{P}\\mathbf{x}\\) and \\(\\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u}\\) are scalars and transposes of each other (and since \\(\\mathbf{P}=\\mathbf{P}^T\\)), they are equal. So their sum is \\(2\\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u}\\).\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\mathbf{x}^T(\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q})\\mathbf{x} + \\mathbf{u}^T\\mathbf{R}\\mathbf{u} + 2\\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u} \\right) dt\\]\nNow, we want to rewrite the terms dependent on \\(\\mathbf{u}\\) into a perfect square like:\n\\[(a + b)¬≤ = a¬≤ + 2ab + b¬≤\\] \\[a¬≤ + 2ab = (a + b)¬≤ - b¬≤\\]\nwe somehow figuredout it looks like this, I also cannot derive it how, but thats the result:\n\\[\\mathbf{u}^T\\mathbf{R}\\mathbf{u} + 2\\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{u} = (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})^T \\mathbf{R} (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}) - \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}\\]\nSubstitute this back into the expression for \\(J\\):\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\mathbf{x}^T(\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q})\\mathbf{x} + (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})^T \\mathbf{R} (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}) - \\mathbf{x}^T\\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x} \\right) dt\\]\n\nImportant part\nFinally, group the \\(\\mathbf{x}^T(\\cdot)\\mathbf{x}\\) terms AGAIN, we did this twice aigoo:\n\\[J = \\mathbf{x}_0^T\\mathbf{P}\\mathbf{x}_0 + \\int_0^\\infty \\left( \\mathbf{x}^T(\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P})\\mathbf{x} + (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})^T \\mathbf{R} (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}) \\right) dt\\]\nTo minimize \\(J\\), we need to make the integral as small as possible. Let‚Äôs analyze the terms within the integral:\n\nThe term \\(\\mathbf{x}^T(\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P})\\mathbf{x}\\) depends only on the state \\(\\mathbf{x}\\), which is a consequence of the control.\nThe term \\((\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})^T \\mathbf{R} (\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x})\\) is a quadratic form involving \\(\\mathbf{u}\\). Since \\(\\mathbf{R}\\) is a positive definite matrix, this term is always greater than or equal to zero.\n\nTo minimize \\(J\\), we must choose \\(\\mathbf{u}\\) such that the second term in the integral is zero (its minimum possible value). This occurs when:\n\\[\\mathbf{u} + \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x} = \\mathbf{0}\\]\nImagine we already have a optimal control: \\[\\mathbf{u}^*(t) = -\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\mathbf{x}(t)\\]\nImagine a specific case at a convergence, this \\(\\mathbf{u}^*(t)\\) term is only 0, only when \\(\\mathbf{x}\\) must also be zero. Therefore, interestingly:\n\\[\\mathbf{A}^T\\mathbf{P} + \\mathbf{P}\\mathbf{A} + \\mathbf{Q} - \\mathbf{P}\\mathbf{B}\\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P} = \\mathbf{0}\\]\nThis is the famous Algebraic Riccati Equation (ARE) which we can solve numerically.\nFInally Back to the Top: Now Calculate \\(\\mathbf{K}\\) using the obtained \\(\\mathbf{P}\\) to get optimal control \\(\\mathbf{u}(t)\\):\n\\[\\mathbf{K} = \\mathbf{R}^{-1}\\mathbf{B}^T\\mathbf{P}\\] \\[\\mathbf{u}(t) = -\\mathbf{K}\\mathbf{x}(t)\\]\nThis means the controller takes the current state \\(\\mathbf{x}(t)\\), multiplies it by the pre-computed gain matrix \\(\\mathbf{K}\\), and applies this as the control input. The negative sign indicates feedback (driving the state towards zero)."
  },
  {
    "objectID": "posts/control/index.html#derivation-of-linear-optimal-control-u--kx-and-quadratic-value-function-vxxt-px",
    "href": "posts/control/index.html#derivation-of-linear-optimal-control-u--kx-and-quadratic-value-function-vxxt-px",
    "title": "From Control to Model-based Learning",
    "section": "Derivation of Linear Optimal Control (\\(u^* = -Kx\\)) and Quadratic Value Function (\\(V(x)=x^T Px\\))",
    "text": "Derivation of Linear Optimal Control (\\(u^* = -Kx\\)) and Quadratic Value Function (\\(V(x)=x^T Px\\))\n\nBellman Optimality -&gt; The Value Function (Cost-to-Go):\nif a path from point A to point C is optimal, then any segment of that path (e.g., from point B to point C, where B is on the path) must also be optimal from point B. This motivated the transformation of cost function J to the value function below.\nThe Cost Function (J): Our problem statement is to find \\(u(t)\\) that minimizes: \\[J = \\int_{0}^{\\infty} (x^T(\\tau)Qx(\\tau) + u^T(\\tau)Ru(\\tau))d\\tau\\]\nThis is the objective.\nThe Value Function (V) is Defined in Terms of J: The value function V(x(t),t)$, as the minimum possible future cost from the current state \\(x(t)\\) at time \\(t\\) to the end of the control horizon (which is \\(\\infty\\) for infinite-horizon LQR). \\[V(x(t),t) = \\min_{u(\\tau),\\tau \\ge t} \\int_{t}^{\\infty} (x^T(\\tau)Qx(\\tau) + u^T(\\tau)Ru(\\tau))d\\tau\\]\nSo, \\(V(x(t),t)\\) is literally the minimum value of a section of the integral \\(J\\).\nTime-Invariance of \\(V(x)\\): For an LTI system with an infinite horizon and constant cost weights, the optimal cost-to-go function \\(V\\) will eventually reach a steady-state. This means it will no longer explicitly depend on time \\(t\\). Therefore, \\(\\frac{\\partial V}{\\partial t} = 0\\). \\[-\\frac{\\partial V}{\\partial t} = \\min_{u} \\left[ x^T Q x + u^T R u + \\left(\\frac{\\partial V}{\\partial x}\\right)^T (Ax + Bu) \\right]\\]\n\\[0 = \\min_{u} \\left[ x^T Q x + u^T R u + \\frac{\\partial V}{\\partial t} + \\left(\\frac{\\partial V}{\\partial x}\\right)^T (Ax + Bu) \\right]\\]\n\n\nDeriving \\(u^* = -Kx\\)\nLet‚Äôs focus on the term inside the \\(\\min_{u}\\) operator. This is a function of \\(u\\). To find the \\(u\\) that minimizes it, we take the partial derivative with respect to \\(u\\) and set it to zero.\nLet \\(g(u) = x^T Q x + u^T R u + \\left(\\frac{\\partial V}{\\partial x}\\right)^T (Ax + Bu)\\). We‚Äôre minimizing \\(g(u)\\) with respect to \\(u\\). Only terms involving \\(u\\) are relevant:\n\\[g(u) = u^T R u + \\left(\\frac{\\partial V}{\\partial x}\\right)^T Bu\\]\nTaking the derivative with respect to \\(u\\):\n\\[\\frac{\\partial g}{\\partial u} = 2Ru + B^T \\frac{\\partial V}{\\partial x}\\]\nSet to zero to find the optimal \\(u^*\\):\n\\[2Ru^* + B^T \\frac{\\partial V}{\\partial x} = 0\\] \\[u^* = -\\frac{1}{2} R^{-1} B^T \\frac{\\partial V}{\\partial x}\\]\nThis is the crucial step: The optimal control is found to be a linear function of the gradient of the value function!\n\n\nDeriving \\(V(x)=x^T Px\\)\nAt this point, we have \\(u^*\\) expressed in terms of \\(\\frac{\\partial V}{\\partial x}\\). Now we need to solve for \\(V(x)\\). This is where the specific structure of the LQR problem (linear dynamics, quadratic cost) becomes paramount.\nSince the problem is Linear-Quadratic, it is a known property from optimal control theory that the optimal value function \\(V(x)\\) will be a quadratic form of the state. This is not just a guess, but a deduction based on the inherent structure of LQ problems.\n\nWhy Quadratic? If \\(V(x)\\) were linear, its second derivative would be zero, which wouldn‚Äôt match the quadratic terms in the HJB. If it were higher order, the derivatives would lead to more complex non-linear equations, which would contradict the simplicity and linearity that arise from the problem. The quadratic form \\(x^T Px\\) is the lowest-order non-trivial form that is consistent with the problem‚Äôs structure.\nSymmetry: \\(P\\) is typically chosen to be symmetric (\\(P=P^T\\)) because \\(x^T Px = x^T P^T x\\). Any asymmetric part of \\(P\\) cancels out in the quadratic form, so we enforce symmetry for uniqueness and consistency.\n\nSo, we propose (or infer) the form:\n\\[V(x) = x^T Px\\]\nwhere \\(P\\) is a symmetric positive definite matrix.\nNow, calculate the gradient of \\(V(x)\\) with respect to \\(x\\):\n\\[\\frac{\\partial V}{\\partial x} = 2Px\\]\n\n\nDeriving ARE\nSubstitute \\(\\frac{\\partial V}{\\partial x} = 2Px\\) back into the expression for \\(u^*\\):\n\\[u^* = -\\frac{1}{2} R^{-1} B^T (2Px)\\] \\[u^* = -R^{-1} B^T Px\\]\nThis is our desired linear state feedback law! Here, \\(K = R^{-1} B^T P\\).\nNow, substitute \\(u^*\\) and \\(\\frac{\\partial V}{\\partial x}\\) back into the simplified HJB equation:\n\\[0 = x^T Q x + (-R^{-1} B^T Px)^T R (-R^{-1} B^T Px) + (2Px)^T (Ax + B(-R^{-1} B^T Px))\\] \\[0 = x^T Q x + x^T P^T (B^T)^T (R^{-1})^T R R^{-1} B^T Px + 2x^T P^T (Ax - BR^{-1} B^T Px)\\] Since \\(P = P^T\\) and \\(R\\) is symmetric (\\(R=R^T\\)), \\(R^{-1}\\) is also symmetric (\\((R^{-1})^T = R^{-1}\\)). Also, \\((B^T)^T = B\\).\n\\[0 = x^T Q x + x^T P B R^{-1} B^T Px + 2x^T PAx - 2x^T P B R^{-1} B^T Px\\] \\[0 = x^T Q x + 2x^T PAx - x^T P B R^{-1} B^T Px\\]\nRecognizing that \\(2x^T PAx = x^T PAx + x^T A^T P^T x = x^T (A^T P + PA)x\\) (since \\(P\\) is symmetric):\n\\[0 = x^T (Q + A^T P + PA - P B R^{-1} B^T P)x\\]\nFor this equation to hold for any state \\(x\\), the matrix in the parenthesis must be identically zero.\n\\[A^T P + PA - P B R^{-1} B^T P + Q = 0\\]\nThis is the Algebraic Riccati Equation (ARE)."
  },
  {
    "objectID": "posts/control/index.html#transformation-to-a-standard-quadratic-program-qp",
    "href": "posts/control/index.html#transformation-to-a-standard-quadratic-program-qp",
    "title": "From Control to Model-based Learning",
    "section": "Transformation to a Standard Quadratic Program (QP)",
    "text": "Transformation to a Standard Quadratic Program (QP)\nThe goal is to eliminate the state variables \\(x(k+i|k)\\) from the optimization problem, leaving only the control input sequence \\(U_k\\) as the decision variables. This is possible because the state evolution is precisely defined by the linear system dynamics, which act as equality constraints.\n\n1. Prediction of Future States:\nWe start by recursively expanding the system dynamics equation: \\(x(k+i+1|k) = Ax(k+i|k) + Bu(k+i|k)\\).\n\nInitial State: \\(x(k|k) = x(k)\\) (the current measured state).\n1-step ahead prediction: \\(x(k+1|k) = Ax(k|k) + Bu(k|k)\\)\n2-step ahead prediction: \\(x(k+2|k) = Ax(k+1|k) + Bu(k+1|k)\\) Substitute \\(x(k+1|k)\\): \\(x(k+2|k) = A(Ax(k|k) + Bu(k|k)) + Bu(k+1|k)\\) \\(x(k+2|k) = A^2 x(k|k) + ABu(k|k) + Bu(k+1|k)\\)\n\nAnd so on, up to \\(H_p\\) steps:\nGeneralizing, the predicted state at any future time \\(k+i\\) can be expressed as a sum of terms related to the initial state \\(x(k)\\) and the future control inputs \\(u(k|k), \\dots, u(k+i-1|k)\\):\n\\[x(k+i|k) = A^i x(k) + \\sum_{j=0}^{i-1} A^{i-1-j} Bu(k+j|k)\\]\nNow, let‚Äôs stack all the predicted states and controls into large vectors.\nLet \\(X_k = \\begin{bmatrix} x(k+1|k) \\\\ x(k+2|k) \\\\ \\vdots \\\\ x(k+H_p|k) \\end{bmatrix}\\) and \\(U_k = \\begin{bmatrix} u(k|k) \\\\ u(k+1|k) \\\\ \\vdots \\\\ u(k+H_c-1|k) \\end{bmatrix}\\) (remembering the assumption that \\(u\\) is constant after \\(H_c-1\\)).\nWe can write the entire sequence of future states as:\n\\[X_k = \\mathbf{\\Phi} x(k) + \\mathbf{\\Gamma} U_k\\]\nwhere:\n\n\\(\\mathbf{\\Phi}\\) is a large block matrix derived from powers of \\(A\\).\n\\(\\mathbf{\\Gamma}\\) is a large block lower triangular matrix (often called the Toeplitz matrix or controllability matrix) containing terms like \\(B, AB, A^2 B, \\dots\\). Its structure reflects how current and past controls affect future states.\n\n\n\n2. Substituting into the Objective Function:\nRecall the objective function:\n\\[J(U_k, x(k)) = \\sum_{i=0}^{H_p-1} \\left( \\|x(k+i|k) - x_{ref}(k+i)\\|_Q^2 + \\|u(k+i|k) - u_{ref}(k+i)\\|_R^2 \\right) + \\|x(k+H_p|k) - x_{ref}(k+H_p)\\|_P^2\\]\nWe can rewrite this in a compact quadratic form. Let‚Äôs simplify by assuming \\(x_{ref}=0\\) and \\(u_{ref}=0\\) for now to highlight the structure. The general case simply introduces linear terms.\n\\[J(U_k, x(k)) = \\sum_{i=0}^{H_p-1} x(k+i|k)^T Q x(k+i|k) + \\sum_{i=0}^{H_c-1} u(k+i|k)^T R u(k+i|k) + x(k+H_p|k)^T P x(k+H_p|k)\\]\nBy substituting \\(x(k+i|k) = A^i x(k) + \\sum_{j=0}^{i-1} A^{i-1-j} Bu(k+j|k)\\) into the expression for \\(J\\), the objective function becomes a quadratic function of \\(U_k\\) and \\(x(k)\\):\n\\[J(U_k, x(k)) = \\frac{1}{2} U_k^T H U_k + G^T U_k + J_{const}\\]\nwhere:\n\n\\(H\\) is a symmetric positive definite matrix (or positive semi-definite, depending on \\(R\\)). It encapsulates the weights \\(Q, R, P\\) and system matrices \\(A, B\\).\n\\(G\\) is a vector that depends on the current state \\(x(k)\\) and the reference trajectories.\n\\(J_{const}\\) is a term that depends only on \\(x(k)\\) and the reference trajectories, which doesn‚Äôt affect the minimization with respect to \\(U_k\\).\n\n\n\n3. Substituting into Constraints:\nSimilarly, all constraints (input, state, output) are originally expressed in terms of \\(x(k+i|k)\\) and \\(u(k+i|k)\\). By substituting the state prediction equation \\(x(k+i|k) = \\mathbf{\\Phi}_i x(k) + \\mathbf{\\Gamma}_i U_k\\) (where \\(\\mathbf{\\Phi}_i\\) and \\(\\mathbf{\\Gamma}_i\\) are parts of \\(\\mathbf{\\Phi}\\) and \\(\\mathbf{\\Gamma}\\) corresponding to time \\(k+i\\)), all constraints can be rewritten purely in terms of \\(x(k)\\) (which is known) and \\(U_k\\) (the decision variables).\nFor example, a state constraint \\(x_{min} \\le x(k+i|k) \\le x_{max}\\) becomes:\n\\[x_{min} \\le \\mathbf{\\Phi}_{i} x(k) + \\mathbf{\\Gamma}_{i} \\mathbf{U}_k \\le x_{max}\\]\nThis can be rearranged into the standard inequality form:\n\\[-\\mathbf{\\Gamma}_{i} \\mathbf{U}_k \\le \\mathbf{\\Phi}_{i} x(k) - x_{min}\\] \\[\\mathbf{\\Gamma}_{i} \\mathbf{U}_k \\le x_{max} - \\mathbf{\\Phi}_{i} x(k)\\]\nThese are stacked for all \\(i\\) and all types of constraints (input, state, output) into the compact form:\n\\[\\mathbf{L} \\mathbf{U}_k \\le \\mathbf{W}\\]\nwhere \\(\\mathbf{L}\\) and \\(\\mathbf{W}\\) are matrices and vectors that encapsulate all the constraint bounds, system matrices, and the current state \\(x(k)\\). Equality constraints (like the dynamics, if kept explicit rather than condensed) would form \\(\\mathbf{M}U_k = \\mathbf{V}\\)."
  },
  {
    "objectID": "posts/control/index.html#the-result-a-standard-qp-form",
    "href": "posts/control/index.html#the-result-a-standard-qp-form",
    "title": "From Control to Model-based Learning",
    "section": "The Result: A Standard QP Form",
    "text": "The Result: A Standard QP Form\nAfter this transformation, the MPC problem at each time step \\(k\\) is reduced to:\nMinimize: \\[\\frac{1}{2} U_k^T H U_k + G^T U_k\\] Subject to: \\[\\mathbf{L}U_k \\le \\mathbf{W} \\quad \\text{(and possibly } \\mathbf{M}U_k = \\mathbf{V} \\text{ for equality constraints)}\\]\nThis is precisely the standard form of a Quadratic Program (QP), where \\(U_k\\) is the optimization variable (what‚Äôs called ‚Äòz‚Äô in a generic QP solver). The matrices \\(H, G, L, W, M, V\\) are updated at each time step based on the current measured state \\(x(k)\\)."
  },
  {
    "objectID": "posts/control/index.html#solutions-for-mpc-quadratic-programming-qp-solvers",
    "href": "posts/control/index.html#solutions-for-mpc-quadratic-programming-qp-solvers",
    "title": "From Control to Model-based Learning",
    "section": "Solutions for MPC (Quadratic Programming (QP) Solvers)",
    "text": "Solutions for MPC (Quadratic Programming (QP) Solvers)\n\nActive Set Methods: These methods work by iteratively selecting a subset of the inequality constraints to be ‚Äúactive‚Äù (i.e., treated as equality constraints). They solve an equality-constrained QP at each iteration, update the active set, and move towards the optimal solution.\n\nPros: Highly reliable, provide exact solutions (within machine precision), often used for small to medium-sized problems.\nCons: Can be slow for large problems, especially if many active set changes are required. Number of iterations can be high.\n\nInterior-Point Methods: These methods transform the constrained QP into a sequence of unconstrained problems by adding ‚Äúbarrier functions‚Äù to the objective that penalize approaching the constraint boundaries. They then use Newton‚Äôs method to solve these unconstrained problems.\n\nPros: Generally scale much better with problem size (fewer iterations, though each iteration is more complex) and are often faster for large-scale QPs. They work well with warm-starting (using the previous solution as an initial guess).\nCons: Can be more complex to implement than active set methods.\n\nPrimal-Dual Methods: A broader class that includes many interior-point methods. They simultaneously solve the primal (original) QP problem and its dual problem."
  },
  {
    "objectID": "posts/control/index.html#derivation",
    "href": "posts/control/index.html#derivation",
    "title": "From Control to Model-based Learning",
    "section": "Derivation",
    "text": "Derivation\n\nState Prediction in Compact Matrix Form\nTHE POINT: ONLY REPRESENT IN U, YOU CAN SEE IN THE MATRIX BELOW\nRecap: We want to express all future predicted states \\(x(k+i|k)\\) as a function of the current state \\(x(k)\\) and the sequence of future control inputs \\(U_k\\).\nSystem: \\(x(k+1)=Ax(k)+Bu(k)\\) Prediction Horizon: \\(H_p\\) Control Horizon: \\(H_c\\) (For simplicity, let‚Äôs assume \\(H_c=H_p\\) for now. The case \\(H_c&lt;H_p\\) just means some \\(u\\)s are repeated, which is a minor modification.)\nStep-by-step prediction expansion:\n\\(x(k+1|k)=Ax(k)+Bu(k|k)\\) \\(x(k+2|k)=Ax(k+1|k)+Bu(k+1|k)=A(Ax(k)+Bu(k|k))+Bu(k+1|k)=A^2x(k)+ABu(k|k)+Bu(k+1|k)\\) \\(x(k+3|k)=Ax(k+2|k)+Bu(k+2|k)=A(A^2x(k)+ABu(k|k)+Bu(k+1|k))+Bu(k+2|k)=A^3x(k)+A^2Bu(k|k)+ABu(k+1|k)+Bu(k+2|k)\\) ‚Ä¶ and so on, up to \\(x(k+H_p|k)\\).\nStacking the Predictions:\nNow, let‚Äôs define the stacked vectors:\n\nFuture States Vector: \\(X_k = \\begin{bmatrix} x(k+1|k) \\\\ x(k+2|k) \\\\ \\vdots \\\\ x(k+H_p|k) \\end{bmatrix}\\) (size \\(n \\cdot H_p \\times 1\\))\nControl Input Sequence (Decision Variable): \\(U_k = \\begin{bmatrix} u(k|k) \\\\ u(k+1|k) \\\\ \\vdots \\\\ u(k+H_c-1|k) \\end{bmatrix}\\) (size \\(m \\cdot H_c \\times 1\\))\n\nSubstitute the expanded predictions into \\(X_k\\):\n\\[X_k = \\begin{bmatrix} Ax(k)+Bu(k|k) \\\\ A^2x(k)+ABu(k|k)+Bu(k+1|k) \\\\ A^3x(k)+A^2Bu(k|k)+ABu(k+1|k)+Bu(k+2|k) \\\\ \\vdots \\\\ A^{H_p}x(k)+A^{H_p-1}Bu(k|k)+\\dots+Bu(k+H_p-1|k) \\end{bmatrix}\\]\nNow, we separate the terms involving \\(x(k)\\) from the terms involving \\(U_k\\):\n\\[X_k = \\begin{bmatrix} A \\\\ A^2 \\\\ A^3 \\\\ \\vdots \\\\ A^{H_p} \\end{bmatrix} x(k) + \\begin{bmatrix} B & 0 & 0 & \\dots & 0 \\\\ AB & B & 0 & \\dots & 0 \\\\ A^2B & AB & B & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ A^{H_p-1}B & A^{H_p-2}B & A^{H_p-3}B & \\dots & B \\end{bmatrix} \\begin{bmatrix} u(k|k) \\\\ u(k+1|k) \\\\ u(k+2|k) \\\\ \\vdots \\\\ u(k+H_p-1|k) \\end{bmatrix}\\]\nThis is exactly the form: \\(X_k = \\mathbf{\\Phi} x(k) + \\mathbf{\\Gamma} U_k\\)\nWhere:\n\n\\(\\mathbf{\\Phi}\\) (State Contribution Matrix): \\[\\mathbf{\\Phi} = \\begin{bmatrix} A \\\\ A^2 \\\\ \\vdots \\\\ A^{H_p} \\end{bmatrix}\\] Size: (\\(n \\cdot H_p\\)) \\(\\times n\\). Each block is \\(n \\times n\\). This matrix shows how the initial state \\(x(k)\\) propagates to all future states if no control were applied.\n\\(\\mathbf{\\Gamma}\\) (Control Contribution Matrix / Block Controllability Matrix): \\[\\mathbf{\\Gamma} = \\begin{bmatrix} B & 0 & 0 & \\dots & 0 \\\\ AB & B & 0 & \\dots & 0 \\\\ A^2B & AB & B & \\dots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ A^{H_p-1}B & A^{H_p-2}B & A^{H_p-3}B & \\dots & B \\end{bmatrix}\\] Size: (\\(n \\cdot H_p\\)) \\(\\times\\) (\\(m \\cdot H_p\\)). Each block is \\(n \\times m\\). This lower triangular block matrix (sometimes called a block Toeplitz matrix) shows how the sequence of control inputs \\(U_k\\) influences the future states. The ‚Äúcontrollability‚Äù aspect comes from the powers of \\(A\\) multiplying \\(B\\), indicating how inputs at different times propagate through the system.\n\n\nNumerical Example\nLet‚Äôs use a very simple system: \\(x(k+1)=Ax(k)+Bu(k)\\) \\(A=\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\\), \\(B=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) (This is a discrete-time integrator/position system) Let \\(H_p=2\\) and \\(H_c=2\\).\nPredictions:\n\\(x(k+1|k)=Ax(k)+Bu(k|k)\\) \\(x(k+2|k)=Ax(k+1|k)+Bu(k+1|k)=A^2x(k)+ABu(k|k)+Bu(k+1|k)\\)\nCalculate powers of A and products with B: \\(A^2 = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\) \\(AB = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\)\nStacked Vectors: \\(X_k = \\begin{bmatrix} x(k+1|k) \\\\ x(k+2|k) \\end{bmatrix}\\) \\(U_k = \\begin{bmatrix} u(k|k) \\\\ u(k+1|k) \\end{bmatrix}\\)\nForming \\(\\mathbf{\\Phi}\\) and \\(\\mathbf{\\Gamma}\\):\n\\[X_k = \\begin{bmatrix} A \\\\ A^2 \\end{bmatrix} x(k) + \\begin{bmatrix} B & 0 \\\\ AB & B \\end{bmatrix} U_k\\]\nSubstitute the numerical matrices:\n\\[\\mathbf{\\Phi} = \\begin{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\\\ \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\quad \\text{(size } (2 \\cdot 2) \\times 2 = 4 \\times 2 \\text{)}\\]\n\\[\\mathbf{\\Gamma} = \\begin{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} & \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\\\ \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} & \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 0 \\\\ 1 & 1 \\end{bmatrix} \\quad \\text{(size } (2 \\cdot 2) \\times (1 \\cdot 2) = 4 \\times 2 \\text{)}\\]\nSo, \\(X_k = \\mathbf{\\Phi} x(k) + \\mathbf{\\Gamma} U_k\\) where all components are explicitly defined. This is how the state prediction is condensed.\n\n\n\n\nSubstituting into the Objective Function (Deriving H, G, J_const)\nRecap: We want to transform \\(J(U_k,x(k))\\) into \\(\\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}\\).\nObjective Function:\n\\[J(U_k,x(k)) = \\sum_{i=0}^{H_p-1} (\\|x(k+i|k)-x_{ref}(k+i)\\|_Q^2 + \\|u(k+i|k)-u_{ref}(k+i)\\|_R^2) + \\|x(k+H_p|k)-x_{ref}(k+H_p)\\|_P^2\\]\nLet \\(e_x(k+i|k)=x(k+i|k)-x_{ref}(k+i)\\) and \\(e_u(k+i|k)=u(k+i|k)-u_{ref}(k+i)\\). The term \\(\\|v\\|_W^2 = v^T W v\\).\nLet‚Äôs adjust the sum for x states to start from \\(i=1\\) to \\(H_p\\) to align with the stacked \\(X_k\\) derived in Part 1 (which starts from \\(x(k+1|k)\\)). The terminal cost (with P) is explicitly included at \\(H_p\\).\n\\[J(U_k,x(k)) = \\sum_{i=1}^{H_p} \\|x(k+i|k)-x_{ref}(k+i)\\|_Q^2 + \\sum_{i=0}^{H_c-1} \\|u(k+i|k)-u_{ref}(k+i)\\|_R^2\\] (Note: For clarity, the terminal cost is typically a separate term at \\(x(k+H_p|k)\\) with weight \\(P\\). If \\(P\\) is used as a Q for the last state in the sum, then \\(Q_{H_p}\\) is \\(P\\). Let‚Äôs explicitly keep it in the sum by making \\(Q_{H_p} = P\\) for \\(i=H_p\\).)\nWe can write the sums in stacked form:\n\\[J = E_X^T \\bar{Q} E_X + E_U^T \\bar{R} E_U\\]\nWhere: * \\(E_X = \\begin{bmatrix} e_x(k+1|k) \\\\ e_x(k+2|k) \\\\ \\vdots \\\\ e_x(k+H_p|k) \\end{bmatrix}\\) (size \\(n \\cdot H_p \\times 1\\)) * \\(E_U = \\begin{bmatrix} e_u(k|k) \\\\ e_u(k+1|k) \\\\ \\vdots \\\\ e_u(k+H_c-1|k) \\end{bmatrix}\\) (size \\(m \\cdot H_c \\times 1\\))\n\n\\(\\bar{Q} = \\text{diag}(Q, Q, \\dots, Q, P)\\) (a block diagonal matrix of size \\((n \\cdot H_p) \\times (n \\cdot H_p)\\), where the last block is \\(P\\) for \\(x(k+H_p|k)\\) and others are \\(Q\\)).\n\\(\\bar{R} = \\text{diag}(R, R, \\dots, R)\\) (\\(H_c\\) times, block diagonal matrix, size \\((m \\cdot H_c) \\times (m \\cdot H_c)\\)).\n\nNow, substitute \\(X_k=\\mathbf{\\Phi}x(k)+\\mathbf{\\Gamma}U_k\\) into the objective. Let‚Äôs define the full reference vector \\(X_{ref}=[x_{ref}(k+1)^T \\dots x_{ref}(k+H_p)^T]^T\\) and \\(U_{ref}=[u_{ref}(k)^T \\dots u_{ref}(k+H_c-1)^T]^T\\).\nThe stacked state error is \\(E_X = X_k - X_{ref} = (\\mathbf{\\Phi}x(k)+\\mathbf{\\Gamma}U_k) - X_{ref}\\). The stacked input error is \\(E_U = U_k - U_{ref}\\).\nThen:\n\\[J = (\\mathbf{\\Phi}x(k)+\\mathbf{\\Gamma}U_k - X_{ref})^T \\bar{Q} (\\mathbf{\\Phi}x(k)+\\mathbf{\\Gamma}U_k - X_{ref}) + (U_k - U_{ref})^T \\bar{R} (U_k - U_{ref})\\]\nNow, expand and collect terms by powers of \\(U_k\\):\n\nQuadratic term in \\(U_k\\) (for \\(H\\)): From the first term: \\((\\mathbf{\\Gamma}U_k)^T \\bar{Q} (\\mathbf{\\Gamma}U_k) = U_k^T \\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Gamma} U_k\\) From the second term: \\(U_k^T \\bar{R} U_k\\) So, the full quadratic term is \\(U_k^T (\\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Gamma} + \\bar{R}) U_k\\). Therefore, \\(H = 2(\\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Gamma} + \\bar{R})\\). (The factor of 2 comes from the standard \\(\\frac{1}{2}z^T H z\\) form of QP objective).\nLinear term in \\(U_k\\) (for \\(G\\)): Let \\(c_x = \\mathbf{\\Phi}x(k) - X_{ref}\\). From the first term (cross-term \\(2 c_x^T \\bar{Q} \\mathbf{\\Gamma} U_k\\)): \\(2(\\mathbf{\\Phi}x(k) - X_{ref})^T \\bar{Q} \\mathbf{\\Gamma} U_k\\) From the second term (cross-term \\(-2 U_{ref}^T \\bar{R} U_k\\)): \\(-2U_{ref}^T \\bar{R} U_k\\) So, the full linear term is \\(2(\\mathbf{\\Phi}x(k) - X_{ref})^T \\bar{Q} \\mathbf{\\Gamma} U_k - 2U_{ref}^T \\bar{R} U_k\\). Therefore, \\(G^T = 2(\\mathbf{\\Phi}x(k) - X_{ref})^T \\bar{Q} \\mathbf{\\Gamma} - 2U_{ref}^T \\bar{R}\\). Or, \\(G = 2\\mathbf{\\Gamma}^T \\bar{Q} (\\mathbf{\\Phi}x(k) - X_{ref}) - 2\\bar{R}^T U_{ref}\\). Note that \\(\\bar{R}\\) is symmetric, so \\(\\bar{R}^T=\\bar{R}\\).\nConstant term (for \\(J_{const}\\)): This term does not depend on \\(U_k\\) and is effectively ignored by the optimizer. It includes: \\((\\mathbf{\\Phi}x(k) - X_{ref})^T \\bar{Q} (\\mathbf{\\Phi}x(k) - X_{ref}) + U_{ref}^T \\bar{R} U_{ref}\\). This term is important if you want to know the actual minimum cost, but not for finding the optimal \\(U_k\\).\n\n\nNumerical Example\nContinue with our previous example: \\(A=\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}\\), \\(B=\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) \\(H_p=2\\), \\(H_c=2\\). Let \\(Q=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) (identity matrix for state penalty), \\(R=[0.1]\\) (scalar for input penalty). Let \\(P=Q\\) (for simplicity of \\(\\bar{Q}\\) structure), \\(x_{ref}=0\\), \\(u_{ref}=0\\).\nFrom Part 1: \\(\\mathbf{\\Phi}=\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\) \\(\\mathbf{\\Gamma}=\\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 0 \\\\ 1 & 1 \\end{bmatrix}\\)\nAlso: \\(\\bar{Q}=\\begin{bmatrix} Q & 0 \\\\ 0 & P \\end{bmatrix} = \\begin{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} & \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} \\\\ \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} & \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\) (size \\(4 \\times 4\\)) \\(\\bar{R}=\\begin{bmatrix} R & 0 \\\\ 0 & R \\end{bmatrix}=\\begin{bmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{bmatrix}\\) (size \\(2 \\times 2\\))\nNow, let‚Äôs compute \\(H\\) (assuming \\(H = 2(\\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Gamma} + \\bar{R})\\) as per the QP formulation): First, \\(\\mathbf{\\Gamma}^T \\bar{Q}\\): \\[\\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\] Then, \\((\\mathbf{\\Gamma}^T \\bar{Q})\\mathbf{\\Gamma}\\): \\[\\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 0 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} (0)(0)+(1)(1)+(0)(0)+(1)(1) & (0)(0)+(1)(0)+(0)(0)+(1)(1) \\\\ (0)(0)+(0)(1)+(0)(0)+(1)(1) & (0)(0)+(0)(0)+(0)(0)+(1)(1) \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 1 \\end{bmatrix}\\] Finally, \\(H = 2 \\left( \\begin{bmatrix} 2 & 1 \\\\ 1 & 1 \\end{bmatrix} + \\begin{bmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{bmatrix} \\right) = 2 \\begin{bmatrix} 2.1 & 1 \\\\ 1 & 1.1 \\end{bmatrix} = \\begin{bmatrix} 4.2 & 2 \\\\ 2 & 2.2 \\end{bmatrix}\\)\nNow, let‚Äôs compute \\(G\\) (assuming \\(x_{ref}=0, u_{ref}=0\\), so \\(X_{ref}=0, U_{ref}=0\\)). In this case, \\(G = 2\\mathbf{\\Gamma}^T \\bar{Q} \\mathbf{\\Phi}x(k)\\). Let \\(x(k)=\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\).\nFirst, \\(\\mathbf{\\Phi}x(k)\\): \\[\\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 2 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_1+x_2 \\\\ x_1 \\\\ 2x_1+x_2 \\end{bmatrix}\\] Then, \\(\\bar{Q} \\mathbf{\\Phi}x(k)\\): \\[\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_1+x_2 \\\\ x_1 \\\\ 2x_1+x_2 \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_1+x_2 \\\\ x_1 \\\\ 2x_1+x_2 \\end{bmatrix}\\] Finally, \\(G = 2\\mathbf{\\Gamma}^T (\\bar{Q} \\mathbf{\\Phi}x(k))\\): \\[G = 2 \\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_1+x_2 \\\\ x_1 \\\\ 2x_1+x_2 \\end{bmatrix} = 2 \\begin{bmatrix} (0)x_1 + (1)(x_1+x_2) + (0)x_1 + (1)(2x_1+x_2) \\\\ (0)x_1 + (0)(x_1+x_2) + (0)x_1 + (1)(2x_1+x_2) \\end{bmatrix}\\] \\[G = 2 \\begin{bmatrix} x_1+x_2 + 2x_1+x_2 \\\\ 2x_1+x_2 \\end{bmatrix} = 2 \\begin{bmatrix} 3x_1+2x_2 \\\\ 2x_1+x_2 \\end{bmatrix} = \\begin{bmatrix} 6x_1+4x_2 \\\\ 4x_1+2x_2 \\end{bmatrix}\\]\nThe \\(J_{const}\\) term would be \\(x(k)^T \\mathbf{\\Phi}^T \\bar{Q} \\mathbf{\\Phi} x(k)\\), which would be a scalar depending only on \\(x(k)\\).\nThis detailed breakdown shows how the matrices \\(H\\) and \\(G\\) are explicitly constructed from the system matrices, weights, and the current state. This allows a standard QP solver to take these numerical matrices and solve for the optimal \\(U_k\\) sequence at each time step.\n\n\n\nBut why do we need to transform the cost function into separated linear, quadratic and Jconst term?\nQP solvers are designed to solve problems in a very specific mathematical form. The standard general form of a Quadratic Program is:\nMinimize: \\[f(z) = \\frac{1}{2} z^T H z + g^T z\\]\nSubject to: \\[A_{eq} z = b_{eq}\\] \\[A_{ineq} z \\le b_{ineq}\\]\nWhere: * \\(z\\) is the vector of optimization variables. * \\(H\\) is the Hessian matrix (symmetric). It determines the curvature of the objective function. * \\(g\\) is the linear term vector. * \\(A_{eq}, b_{eq}, A_{ineq}, b_{ineq}\\) define the linear equality and inequality constraints.\nBy transforming the MPC objective into the form \\(\\frac{1}{2} U_k^T H U_k + G^T U_k + J_{const}\\):\n\nOur decision variable \\(U_k\\) maps directly to the generic \\(z\\).\nOur calculated \\(H\\) matrix maps directly to the generic \\(H\\).\nOur calculated \\(G\\) vector maps directly to the generic \\(g\\)."
  },
  {
    "objectID": "posts/control/index.html#simple-example-1d-car-path-following-with-mpc",
    "href": "posts/control/index.html#simple-example-1d-car-path-following-with-mpc",
    "title": "From Control to Model-based Learning",
    "section": "Simple Example: 1D Car Path Following with MPC",
    "text": "Simple Example: 1D Car Path Following with MPC\nScenario: Imagine a car that can only move along a straight line (1D). Its goal is to follow a predefined ‚Äúpath‚Äù (a sequence of desired positions) along this line. We want to control its acceleration to achieve this.\n\n1. Dynamics Model:\nLet‚Äôs use a very simple discrete-time model for longitudinal motion:\n\nStates (\\(x\\)):\n\n\\(p(k)\\): Position of the car at time \\(k\\)\n\\(v(k)\\): Velocity of the car at time \\(k\\) So, \\(x(k)=\\begin{bmatrix} p(k) \\\\ v(k) \\end{bmatrix}\\)\n\nInput (\\(u\\)):\n\n\\(a(k)\\): Acceleration applied by the car at time \\(k\\) So, \\(u(k)=[a(k)]\\)\n\n\nDiscrete-Time Equations: Assume a sampling time \\(\\Delta t=1\\) second for simplicity. \\(p(k+1)=p(k)+v(k)\\Delta t+\\frac{1}{2}a(k)(\\Delta t)^2\\) \\(v(k+1)=v(k)+a(k)\\Delta t\\)\nPlugging in \\(\\Delta t=1\\): \\(p(k+1)=p(k)+v(k)+\\frac{1}{2}a(k)\\) \\(v(k+1)=v(k)+a(k)\\)\nIn state-space form \\(x(k+1)=Ax(k)+Bu(k)\\): \\(A=\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\), \\(B=\\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}\\) (Self-correction: The position update \\(p(k+1)=p(k)+v(k)\\Delta t\\) means the v(k) term is in the first row of A. The velocity update \\(v(k+1)=v(k)+a(k)\\Delta t\\) means v(k) is in the second row of A, and a(k) is in the second row of B. The original matrix A was incorrect for this model. The corrected A matrix is \\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\) and B is \\(\\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}\\).)\n\n\n2. Path Representation & Reference Trajectory:\nLet our desired path be a set of target positions. For this 1D example, let the car‚Äôs goal be to reach position 10 and stop. * Target positions (path): \\(p_{ref}=[1,2,3,4,5,6,7,8,9,10,10,10,\\dots]\\) * Target velocities: \\(v_{ref}=[0,0,0,0,0,0,0,0,0,0,0,0,\\dots]\\) (we want it to stop at 10)\nSo, \\(x_{ref}(k+i)=\\begin{bmatrix} p_{ref}(k+i) \\\\ v_{ref}(k+i) \\end{bmatrix}\\)\n\n\n3. MPC Parameters:\n\nPrediction Horizon (\\(H_p\\)): Let‚Äôs choose \\(H_p=3\\) steps.\nControl Horizon (\\(H_c\\)): Let‚Äôs choose \\(H_c=2\\) steps. This means we‚Äôll optimize \\(a(k)\\) and \\(a(k+1)\\), and assume \\(a(k+2)\\) (and beyond) is zero.\nWeights:\n\n\\(Q=\\begin{bmatrix} 1 & 0 \\\\ 0 & 0.1 \\end{bmatrix}\\) (Penalize position error strongly, velocity error lightly)\n\\(R=[0.01]\\) (Penalize acceleration lightly to allow motion)\n\\(P=\\begin{bmatrix} 5 & 0 \\\\ 0 & 1 \\end{bmatrix}\\) (Strong terminal penalty on position at the end of horizon)\n\nConstraints:\n\nAcceleration limits: \\(-1 \\le a(k) \\le 1\\) (\\(m/s^2\\))\nVelocity limits: \\(0 \\le v(k) \\le 5\\) (\\(m/s\\)) (Car cannot go backwards, max speed 5 m/s)\nPosition limits: \\(0 \\le p(k) \\le 11\\) (Stay within a narrow road segment)\n\n\n\n\n4. MPC at Time \\(k=0\\) (First Iteration):\nAssume current measured state: \\(x(0)=\\begin{bmatrix} p(0) \\\\ v(0) \\end{bmatrix}=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) (car starts at origin, at rest).\na) Define Decision Variables: The control sequence we need to find is \\(U_0=\\begin{bmatrix} u(0|0) \\\\ u(1|0) \\end{bmatrix}=\\begin{bmatrix} a(0) \\\\ a(1) \\end{bmatrix}\\). (Length \\(m \\cdot H_c = 1 \\cdot 2 = 2\\))\nb) Generate Reference Trajectory for the Horizon: From our path, for \\(H_p=3\\) steps:\n\\(x_{ref}(1)=\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\) \\(x_{ref}(2)=\\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}\\) \\(x_{ref}(3)=\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\)\nc) Formulate Objective Function (QP style): The goal is to find \\(U_0\\) that minimizes:\n\\[J = \\sum_{i=1}^{3} \\|x(i|0)-x_{ref}(i)\\|_Q^2 + \\sum_{i=0}^{1} \\|u(i|0)-u_{ref}(i)\\|_R^2\\]\n(Here, \\(u_{ref}(i)=0\\) as we just want to follow the path, not target specific acceleration) Note: The terminal cost is explicit if \\(P\\) is the \\(Q\\) for the final state in the sum. Let‚Äôs make it explicit for clarity:\n\\[J = \\|x(1|0)-x_{ref}(1)\\|_Q^2 + \\|x(2|0)-x_{ref}(2)\\|_Q^2 + \\|x(3|0)-x_{ref}(3)\\|_P^2 + \\|u(0|0)\\|_R^2 + \\|u(1|0)\\|_R^2\\]\nCondensing into \\(\\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}\\):\nFirst, state predictions: \\(x(1|0)=Ax(0)+Bu(0|0)\\) \\(x(2|0)=A^2x(0)+ABu(0|0)+Bu(1|0)\\) \\(x(3|0)=A^3x(0)+A^2Bu(0|0)+ABu(1|0)+Bu(2|0)\\) (Since \\(H_c=2\\), \\(u(2|0)\\) would be zero here or \\(u(1|0)\\)) Let‚Äôs assume \\(u(k+i|k)=0\\) for \\(i \\ge H_c\\). So \\(u(2|0)=0\\).\nCalculate powers of A and products with B: \\(A^2 = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}\\) \\(A^3 = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 3 \\\\ 0 & 1 \\end{bmatrix}\\) \\(B = \\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix}\\) \\(AB = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1.5 \\\\ 1 \\end{bmatrix}\\) \\(A^2B = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2.5 \\\\ 1 \\end{bmatrix}\\)\n\\(\\mathbf{\\Phi} = \\begin{bmatrix} A \\\\ A^2 \\\\ A^3 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 1 & 2 \\\\ 0 & 1 \\\\ 1 & 3 \\\\ 0 & 1 \\end{bmatrix}\\) (size \\(6 \\times 2\\))\n\\(\\mathbf{\\Gamma} = \\begin{bmatrix} B & 0 \\\\ AB & B \\\\ A^2B & AB \\end{bmatrix} = \\begin{bmatrix} 0.5 & 0 \\\\ 1 & 0 \\\\ 1.5 & 0.5 \\\\ 1 & 1 \\\\ 2.5 & 1.5 \\\\ 1 & 1 \\end{bmatrix}\\) (size \\(6 \\times 2\\))\n\\(\\bar{Q}_{stacked} = \\text{diag}(Q, Q, P) = \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0.1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0.1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 5 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 1 \\end{bmatrix}\\) (size \\(6 \\times 6\\)) \\(\\bar{R} = \\text{diag}(R,R) = \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix}\\) (size \\(2 \\times 2\\))\nThen, \\(H = 2(\\mathbf{\\Gamma}^T \\bar{Q}_{stacked} \\mathbf{\\Gamma} + \\bar{R})\\). And \\(G = 2\\mathbf{\\Gamma}^T \\bar{Q}_{stacked}(\\mathbf{\\Phi}x(0) - X_{ref}) - 2\\bar{R}U_{ref}\\). Since \\(x(0) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\) and \\(u_{ref}(i)=0\\), \\(U_{ref}=0\\). \\(X_{ref} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 3 \\\\ 0 \\end{bmatrix}\\). So, \\(G = 2\\mathbf{\\Gamma}^T \\bar{Q}_{stacked}(\\mathbf{\\Phi}x(0) - X_{ref})\\).\nd) Formulate Constraints (QP style):\n\nInput Constraints: \\(-1 \\le a(k) \\le 1 \\implies \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} a(k) \\le \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) Stacked for \\(U_0 = \\begin{bmatrix} a(0) \\\\ a(1) \\end{bmatrix}\\): \\[\\begin{bmatrix} 1 & 0 \\\\ -1 & 0 \\\\ 0 & 1 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} a(0) \\\\ a(1) \\end{bmatrix} \\le \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\]\nState Constraints (Velocity): \\(0 \\le v(k+i|0) \\le 5\\) for \\(i=1,2,3\\). \\(v(k+i|0)\\) is the second component of \\(x(k+i|0)\\). \\(x(k+i|0) = \\mathbf{\\Phi}_i x(0) + \\mathbf{\\Gamma}_i U_0\\). The constraint can be written as: \\(\\begin{bmatrix} 0 & 1 \\end{bmatrix} x(k+i|0) \\le 5\\) \\(-\\begin{bmatrix} 0 & 1 \\end{bmatrix} x(k+i|0) \\le 0\\) Substitute \\(x(k+i|0)\\): \\(\\begin{bmatrix} 0 & 1 \\end{bmatrix} (\\mathbf{\\Phi}_i x(0) + \\mathbf{\\Gamma}_i U_0) \\le 5\\) \\(-\\begin{bmatrix} 0 & 1 \\end{bmatrix} (\\mathbf{\\Phi}_i x(0) + \\mathbf{\\Gamma}_i U_0) \\le 0\\)\nFor \\(x(0)=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\): \\(v(1|0) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (A x(0) + B u(0|0)) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} B u(0|0) = 1 \\cdot a(0) = a(0)\\). So, \\(0 \\le a(0) \\le 5\\). (This is covered by \\(-1 \\le a(0) \\le 1\\) limits).\n\\(v(2|0) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (A^2 x(0) + AB u(0|0) + B u(1|0)) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (AB u(0|0) + B u(1|0)) = 1 \\cdot a(0) + 1 \\cdot a(1) = a(0)+a(1)\\). So, \\(0 \\le a(0)+a(1) \\le 5\\).\n\\(v(3|0) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (A^3 x(0) + A^2B u(0|0) + AB u(1|0)) = \\begin{bmatrix} 0 & 1 \\end{bmatrix} (A^2B u(0|0) + AB u(1|0)) = 1 \\cdot a(0) + 1 \\cdot a(1) = a(0)+a(1)\\). So, \\(0 \\le a(0)+a(1) \\le 5\\).\nState Constraints (Position): \\(0 \\le p(k+i|0) \\le 11\\) for \\(i=1,2,3\\). \\(p(k+i|0)\\) is the first component of \\(x(k+i|0)\\). \\(\\begin{bmatrix} 1 & 0 \\end{bmatrix} x(k+i|0) \\le 11\\) \\(-\\begin{bmatrix} 1 & 0 \\end{bmatrix} x(k+i|0) \\le 0\\)\nFor \\(x(0)=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\): \\(p(1|0) = \\begin{bmatrix} 1 & 0 \\end{bmatrix} B u(0|0) = 0.5 \\cdot a(0)\\). So, \\(0 \\le 0.5 a(0) \\le 11\\).\n\\(p(2|0) = \\begin{bmatrix} 1 & 0 \\end{bmatrix} (AB u(0|0) + B u(1|0)) = 1.5 \\cdot a(0) + 0.5 \\cdot a(1)\\). So, \\(0 \\le 1.5 a(0) + 0.5 a(1) \\le 11\\).\n\\(p(3|0) = \\begin{bmatrix} 1 & 0 \\end{bmatrix} (A^2B u(0|0) + AB u(1|0)) = 2.5 \\cdot a(0) + 1.5 \\cdot a(1)\\). So, \\(0 \\le 2.5 a(0) + 1.5 a(1) \\le 11\\).\n\nYou would formulate all of these into the final \\(\\mathbf{L}U_k \\le \\mathbf{W}\\) matrix.\n\n\n5. Solve the QP:\nAt time \\(k=0\\), a QP solver (like quadprog in MATLAB, or cvxopt in Python) is given \\(H, G, L, W\\) (and \\(x(0)\\) is embedded in \\(G\\) and \\(W\\)). The solver would return the optimal \\(U_0^*=\\begin{bmatrix} a^*(0) \\\\ a^*(1) \\end{bmatrix}\\).\nExample Output (Illustrative - not numerically solved): Let‚Äôs assume the solver returns: \\(a^*(0)=1.0\\) (accelerate to get moving) \\(a^*(1)=0.8\\) (continue accelerating)\n\n\n6. Apply First Control Action:\nThe car‚Äôs controller applies only the first element: \\(a_{applied}(0)=a^*(0)=1.0\\).\n\n\n7. Move to Next Timestep (\\(k=1\\)):\nThe car‚Äôs actual state is measured: \\(x(1)\\). Using \\(a_{applied}(0)=1.0\\): \\(p(1)=p(0)+v(0)+\\frac{1}{2}a(0)=0+0+\\frac{1}{2}(1.0)=0.5\\) \\(v(1)=v(0)+a(0)=0+1.0=1.0\\) So, \\(x(1)=\\begin{bmatrix} 0.5 \\\\ 1.0 \\end{bmatrix}\\)\nThe entire process repeats: * MPC takes \\(x(1)\\) as its new current state. * It generates a new reference trajectory for \\(k=1,2,3\\) (e.g., \\(x_{ref}(2), x_{ref}(3), x_{ref}(4)\\) from the path). * It re-formulates a new QP with updated \\(G\\) and \\(W\\) matrices (as they depend on \\(x(1)\\)). * It solves the QP for a new optimal control sequence \\(U_1^*=\\begin{bmatrix} a^*(1) \\\\ a^*(2) \\end{bmatrix}\\). * Only \\(a^*(1)\\) from this new sequence is applied."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "‡ªí(‚äô·¥ó‚äô)‡•≠‚úé‚ñ§",
    "section": "",
    "text": "Computer Graphics Foundation\n\n\n\n\n\n\n\n\nJul 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning 4 Robotics\n\n\n\n\n\n\n\n\nJul 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTouch Sensing\n\n\n\n\n\n\n\n\nJul 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearning from Demonstration\n\n\n\n\n\n\n\n\nJul 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Autoencoder to Dreamer\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Control to Model-based Learning\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process & Bayesian Optimization\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nModel-free Reinforcement Learning\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\nNo matching items"
  }
]