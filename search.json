[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "haha",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning\n\n\n\nmachine-learning\n\nRL\n\n\n\n\n\n\n\n\n\nJun 25, 2025\n\n\nDinh Tien Thang\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 22, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html",
    "href": "posts/reinforcement-learning/index.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning (RL) is a subfield of machine learning concerned with how agents ought to take actions in an environment in order to maximize cumulative reward."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#value-functions",
    "href": "posts/reinforcement-learning/index.html#value-functions",
    "title": "Reinforcement Learning",
    "section": "Value Functions",
    "text": "Value Functions\nVery similar to return \\(\\max_\\pi \\, \\mathbb{E}_\\pi \\left[ G_t \\right]\\).\nValue functions are used to estimate expected returns:\n\nState-value function: \\(V_\\pi(s_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s]\\)\nAction-value function: \\(Q_\\pi(s_t, a_t) = \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\nRelationship: \\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) and \\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)\nDerivation of this relationship:\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(s, a)\\) (semantically true because)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[G_t \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} \\mid s_t = s, a_t = a]\\) (because \\(\\mathbb{E}[G_{t+1} \\mid s, a] = \\mathbb{E}[V_\\pi(S_{t+1}) \\mid s, a]\\))\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\mathbb{E}_\\pi[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid s_t = s, a_t = a]\\)\n\\(V_\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a)[r + \\gamma V_\\pi(s')]\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "href": "posts/reinforcement-learning/index.html#the-bellman-equations",
    "title": "Reinforcement Learning",
    "section": "The Bellman Equations",
    "text": "The Bellman Equations\nFor any policy \\(\\pi\\), all \\(s \\in \\mathcal{S}\\), and all \\(a \\in \\mathcal{A}(s)\\):\nState-value function:\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\, q_\\pi(s, a) \\tag{1}\n\\]\nAction-value function:\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right] \\tag{2}\n\\]\nAfter substitution as derived above:\n\nBellman Equation for \\(v_\\pi(s)\\)\n\\[\nv_\\pi(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]\n\n\nBellman Equation for \\(q_\\pi(s, a)\\)\n\\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma \\sum_{a' \\in \\mathcal{A}(s')} \\pi(a' \\mid s') q_\\pi(s', a') \\right]\n\\]\n\n\nOptimal Bellman\n\\(V^*(s) = \\arg\\max_a Q^*(s, a)\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "href": "posts/reinforcement-learning/index.html#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement",
    "title": "Reinforcement Learning",
    "section": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement",
    "text": "Generalized Policy Iteration (GPI): Policy Evaluation & Policy Improvement\nUsing the Bellman Equation above and an initial estimate (e.g. all states are randomly initialized or uniformly negative, except the terminal state which is 0), we iteratively update:\nWe apply the Bellman equations above for every single state (for V), or (state, action) pair (for Q, which forms a finer table). Over many runs, it will slowly converge. \\[\n\\pi'(s) = \\arg\\max_a q_\\pi(s, a)\n\\] Where: \\[\nq_\\pi(s, a) = \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s', r \\mid s, a) \\left[ r + \\gamma v_\\pi(s') \\right]\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#monte-carlo-state-value",
    "href": "posts/reinforcement-learning/index.html#monte-carlo-state-value",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo (State Value)",
    "text": "Monte Carlo (State Value)\nGoal: Given samples under \\(\\pi\\), estimate \\(q_\\pi\\).\n\nWe can express \\(q_\\pi\\)-estimation as \\(v_\\pi\\)-estimation. Imagine a new problem where: \\[\nS_t^{\\text{new}} = (S_t, A_t)\n\\]\n\nAny evaluation algorithm estimating \\(v(s^{\\text{new}})\\) would be estimating \\(q_\\pi(s, a)\\).\nSo basically what happening is that we do not have a policy anymore, instead, policy is now a part of the environment. But why do we want to do that? Because: - This is a generalization anyway, something applied to state value function V, also applied for action value function Q. - It simplifies our analysis, reduces the problem to a simpler problem Markov Reward Process (MRP), which is MDP without actions (s1,r1,s2,r2,s3,r3,…)\nOK, but still, how to do it?\nStart with a random value function: \\(V(s) = \\frac{1}{|S|}\\)\nCollect sampling trajectories \\(M\\) trajectory samples: \\[\ns_0^m \\quad r_1^m \\quad s_1^m \\quad \\cdots \\quad s_{T_m}^m \\qquad m = 1, \\ldots, M\n\\]\nThe Goal: Use averages to approximate \\(v_\\pi(s)\\): \\[\nv_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s] \\approx \\frac{1}{C(s)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[s_\\tau^m = s] \\, g_\\tau^m\n\\] where: \\[\n\\mathbb{I}[s_\\tau^m = s] =\n\\begin{cases}\n1 & \\text{if } s_\\tau^m = s \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] \\[\ng_\\tau^m = R_{t+1}^m + \\gamma R_{t+2}^m + \\gamma^2 R_{t+3}^m + \\dots + \\gamma^{T - t - 1} R_T^m\n\\]\nFor every sample trajectory \\(m\\), at any step \\(\\tau\\) in that trajectory, check if the state \\(g_\\tau^m\\) of that step is the \\(s\\) we are interested in, then include its return \\(g_\\tau^m\\) in the sum, then normalize by \\(C(s)\\), the total number of times state \\(s\\) was visited.\nAt this moment I just realized that: the state will get higher return, if its nearer to the beginning of a trajectory, if u dont believe, have a look at \\(g_\\tau^m\\) again ^^.\nBtw, to calculate return \\(g_\\tau^m\\), maybe you already know, we have to calculate from the terminate state first \\(R_T^m\\), where we know if the reward \\(R_T^m\\) is 0 or 1 (reached the goal or not), then slowly trace backward with addding \\(\\gamma\\)\nAnd to make sure you understand it, \\(v_\\pi(s)\\) is just like \\(G\\), but \\(G\\) is mostly binded to the trajectory and a policy, therefore the function \\(v_\\pi(s)\\) is actually \\(G\\)!!!\nHow to get to that Goal? to apply after the \\(m\\)-th sample: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha \\left( g_t^m - V(s_t^m) \\right)\n\\]\n… then it will slowly converge to the Goal above …\nBUT, how do we extend this to update our action ?"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#monte-carlo-action-value",
    "href": "posts/reinforcement-learning/index.html#monte-carlo-action-value",
    "title": "Reinforcement Learning",
    "section": "Monte Carlo (Action Value)",
    "text": "Monte Carlo (Action Value)\nSince we also have a deterministic set of action \\(a \\in \\mathcal{A}(s)\\), therefore we can extend the state value above to action value like this, it is equivalent:\nStart also with \\(Q(s,a) = \\frac{1}{|SxA|}\\) or just simply 0\nBasically it just create a finer Q-table.\nThe Goal \\[\nQ_\\pi(s,a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a] \\approx \\frac{1}{C(s, a)} \\sum_{m=1}^M \\sum_{\\tau=0}^{T_m - 1} \\mathbb{I}[(s,a)_\\tau^m = (s,a)] \\, g_\\tau^m\n\\]\nHow to get to that goal? \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha \\left( g_t^m - Q(s_t^m, a_t^m) \\right)\n\\]\n… Then it will slowly converge the Goal above …\nThen we just argmax over action at each state, thats how we get optimal action."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-constant-α-mc-algorithm-pi-approx-pi",
    "href": "posts/reinforcement-learning/index.html#sum-up-constant-α-mc-algorithm-pi-approx-pi",
    "title": "Reinforcement Learning",
    "section": "Sum up: Constant-α MC Algorithm \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Constant-α MC Algorithm \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(\\epsilon &gt; 0\\), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some \\(\\epsilon\\)-soft policy\n\n\\(Q(s,a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S},\\ a \\in \\mathcal{A}(s)\\) (like a random Q-Table hehe)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample a trajectory under policy \\(\\pi\\):\n\\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\)\nFor (literally EACH - EVERY SINGLE) \\(t = 0, \\dots, T_m - 1\\):\n\nCompute return (the best way is just to calculate backwards then slowly add \\(\\gamma\\) like Gonkee ^^):\n\\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nUpdate Q-value:\n\\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\n\\]\n\nUpdate policy:\n\\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\)\n\nWhere \\(\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\\) is specified as follows: \\[\na^* \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a) \\quad \\text{(ties broken arbitrarily)}\n\\]\nFor all \\(a \\in \\mathcal{A}(s_t^m)\\): (this means to balance the policy to avoid a local optimal) \\[\n\\pi(a|s_t^m) \\leftarrow\n\\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a = a^* \\\\\n\\frac{\\epsilon}{|\\mathcal{A}(s_t^m)|} & \\text{if } a \\neq a^*\n\\end{cases}\n\\]\n(\\(|\\mathcal{A}(s_t^m)| = \\text{number of actions in } \\mathcal{A}(s_t^m)\\))\nthen back to the loop For \\(m = 1, \\dots, M\\) again and again …"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#off-policy",
    "href": "posts/reinforcement-learning/index.html#off-policy",
    "title": "Reinforcement Learning",
    "section": "Off-Policy",
    "text": "Off-Policy\nThe problem is, as seen in the Blackjack example, lack of sample leads to local-optimal bias model\n=&gt; Goal: more variance\nSample a trajectory under a different policy \\(b\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m - 1}^m, r_{T_m}^m\\). But the rest of the algorithm stays the same.\nOK, but how to make sampling policy \\(b\\) effect the main behavior policy \\(\\pi\\)?\n\nRelationship between sampling policy \\(b\\) vs main behavior policy \\(\\pi\\)?\nWe want: \\[\nq_\\pi(s, a) = \\mathbb{E}_\\pi[G_t|S_t = s, A_t = a]\n\\]\nSampled data under \\(b\\) means this is what we actually estimated: \\[\n\\mathbb{E}_b[G_t|S_t = s, A_t = a]\n\\]\nTherefore we use Importance Sampling to bring them to \\(\\pi\\): \\[\nq_\\pi(s, a) = \\mathbb{E}_b\\left[\\frac{p_\\pi(G_t)}{p_b(G_t)}G_t|S_t = s, A_t = a\\right]\n\\] where \\(\\rho\\) is the importance sampling ratio: \\[\n\\frac{p_\\pi(G_t)}{p_b(G_t)} = \\rho = \\prod_{\\tau=t+1}^{T-1} \\frac{\\pi(A_\\tau|S_\\tau)}{b(A_\\tau|S_\\tau)}\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "href": "posts/reinforcement-learning/index.html#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi",
    "title": "Reinforcement Learning",
    "section": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)",
    "text": "Sum up: Off-Policy Constant-\\(\\alpha\\) MC for \\(\\pi \\approx \\pi^*\\)\nAlgorithm inputs: \\(b\\) (behavior policy), \\(\\alpha \\in (0, 1]\\), \\(M \\in \\mathbb{N}\\)\nInitialize arbitrarily:\n\n\\(\\pi \\leftarrow\\) some policy\n\\(Q(s, a) \\leftarrow\\) some value for \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}(s)\\) (also the random Q-Table above hehe)\n\nFor \\(m = 1, \\dots, M\\):\nUnder \\(b\\) sample: \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\)\n\nFor \\(t = 0, \\dots, T_m - 1\\):\n\n\\(\\rho_t^m \\leftarrow \\prod_{\\tau=t+1}^{T_m-1} \\frac{\\pi(a_\\tau^m|s_\\tau^m)}{b(a_\\tau^m|s_\\tau^m)}\\) (or 1 if \\(t+1 &gt; T_m-1\\))\nCompute return: \\(g_t^m \\leftarrow \\rho_t^m(r_{t+1}^m + \\gamma r_{t+2}^m + \\dots)\\)\nUpdate Q-Value: \\(Q(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_t^m - Q(s_t^m, a_t^m))\\)\nUpdate policy: \\(\\pi(s_t^m) \\leftarrow \\operatorname{argmax}_a Q(s_t^m, a)\\) (ties broken arbitrarily)\n\nNote that at Update Policy: we do not need the \\(\\pi\\)-greedy as above, because now using behavior-policy \\(b\\), we could already diverse out for a more global view\n\nBUT, off policy MC has too much variance, therefore the next technique … Temporal Difference\nBefore we continue, let’s see where is exactly the point of model-free MC learning:"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "href": "posts/reinforcement-learning/index.html#n-step-temporal-difference-learning",
    "title": "Reinforcement Learning",
    "section": "n-step Temporal Difference Learning",
    "text": "n-step Temporal Difference Learning\nRecall from the MC approach: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\nwhere: \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\)\nn-STEP TD: Replace the target, \\(g_t^m\\), with: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n V(s_{t+n}^m)\n\\]\nwhere \\(V(s_{t+n}^m)\\) is actually no different than \\(g_{t+n}^m\\), but instead of waiting for the trajectory to finish the episode then calculate backward, we just need to wait for \\(n\\) steps to BOOTSTRAPPING the existing \\(V(s_{t+n})\\) calculated from older trajectories, think a little bit, it means the same thing with \\(g_{t+n}\\) (accumulated return). If \\(n = \\infty\\): TD is identical to MC."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#why-is-td-better",
    "href": "posts/reinforcement-learning/index.html#why-is-td-better",
    "title": "Reinforcement Learning",
    "section": "Why is TD better?",
    "text": "Why is TD better?\n\nMarkov property: The RL environment has Markov property, that means the future depends only on the current state, not the history. And TD works based on that design, the update is just between neighboring states. Not like waiting for the whole trajectory like MC, \\(g_t^m\\) needs backward calculation for the whole trajectory =&gt; strongly history based. \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(g_t^m - V(s_t^m))\n\\]\n\nFor example in TD(0) the use of \\(V(s_{t+1}^m)\\) is very Markov property: \\[\nV(s_t^m) \\leftarrow V(s_t^m) + \\alpha(r_{t+1}^m + \\gamma V(s_{t+1}^m) - V(s_t^m))\n\\]\n\nReduced Variance: The effect of MC is calculating accumulated result at each single trajectory, then averaging then in the end, this makes the result very specific to the sampled trajectories. But TD directly add the already smoothed \\(+ \\gamma^n V(s_{t+n}^m)\\)\nOnline-learning we all know what that means"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#what-does-larger-n-means",
    "href": "posts/reinforcement-learning/index.html#what-does-larger-n-means",
    "title": "Reinforcement Learning",
    "section": "What does larger n means?",
    "text": "What does larger n means?\nIncrease the bias = utilize more from the knowledge from this trajectory, instead of keep learning from the average.\nE.g., for a single episode with TD(8):\n\nAll states from \\(S_0\\) up to \\(S_{k-8}\\) (if \\(k \\ge 8\\)): Will be updated using a full 8-step return, bootstrapping from \\(V(S_{t+8})\\) =&gt; very average in the beginning\nThe last 7 states (\\(S_{k-7}, \\dots, S_{k-1}\\)): Will be updated using a return that effectively “runs out of steps” before 8. Their targets will be a mix of actual rewards leading to the terminal state, like Monte Carlo =&gt; direct reward in the end"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-on-policy-temporal-difference-n-step-sarsa",
    "href": "posts/reinforcement-learning/index.html#sum-up-on-policy-temporal-difference-n-step-sarsa",
    "title": "Reinforcement Learning",
    "section": "Sum up: On-Policy Temporal Difference: n-step Sarsa",
    "text": "Sum up: On-Policy Temporal Difference: n-step Sarsa\nModel-free control \\(\\rightarrow\\) use \\(Q(s, a)\\), not \\(V(s)\\).\nRedefine: \\[\ng_{t:t+n}^m = r_{t+1}^m + \\dots + \\gamma^{n-1}r_{t+n}^m + \\gamma^n Q(s_{t+n}^m, a_{t+n}^m)\n\\]\nUpdate rule: \\[\nQ(s_t^m, a_t^m) \\leftarrow Q(s_t^m, a_t^m) + \\alpha(g_{t:t+n}^m - Q(s_t^m, a_t^m))\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#q-learning",
    "href": "posts/reinforcement-learning/index.html#q-learning",
    "title": "Reinforcement Learning",
    "section": "Q-Learning",
    "text": "Q-Learning\n1-step TD Control—-target adjustment—-&gt; Q-Learning (off-policy).\nInstead of using \\(r_{t+1}^m + \\gamma Q(s_{t+1}^m, a_{t+1}^m)\\) (which is used in SARSA and relies on the next action taken by the policy), Q-Learning uses: \\[\nr_{t+1}^m + \\gamma \\max_{a} Q(s_{t+1}^m, a)\n\\]\nThe \\(\\max\\) operator means, regardless of which action the behavior policy \\(b\\) actually took, this target is formed by the best possible action from the next state \\(s_{t+1}^m\\) =&gt; Q-Learning an off-policy.\nTo describe what actually happens, it is like this: 1-step TD (SARSA-like): \\[\n\\dots s_0^m, \\underset{\\uparrow}{\\underline{a_0^m}}, r_1^m, s_1^m, \\underset{\\uparrow}{\\underline{a_1^m}}, r_2^m, s_2^m, \\underset{\\uparrow}{\\underline{a_2^m}}, r_3^m, s_3^m, \\dots\n\\] Updates for \\(Q(s_t, a_t)\\) occur after observing \\(s_{t+1}, a_{t+1}\\), using the target \\(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\) where \\(a_{t+1}\\) is the action taken by the current policy.\n1-step Q-Learning: \\[\n\\dots  \\underset{\\uparrow}{\\underline{s_0^m}},a_0^m, r_1^m, \\underset{\\uparrow}{\\underline{s_1^m}},a_1^m, r_2^m, \\underset{\\uparrow}{\\underline{s_2^m}},a_2^m, r_3^m, \\dots\n\\] Updates for \\(Q(s_t, a_t)\\) occur after observing \\(s_{t+1}\\), using the target \\(r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a)\\), where action \\(a_{t+1}\\) is observed but not used in forming the target for \\(Q(s_t, a_t)\\)."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#expected-sarsa",
    "href": "posts/reinforcement-learning/index.html#expected-sarsa",
    "title": "Reinforcement Learning",
    "section": "Expected Sarsa",
    "text": "Expected Sarsa\n1-step Q-Learning —– \\(\\max\\) operator-&gt;average operator—&gt; Expected Sarsa \\[\nr_{t+1}^m + \\gamma \\max_{a} Q(s_{t+1}^m, a)\n\\] to using an expectation over all possible actions, weighted by the policy \\(\\pi\\): \\[\nr_{t+1}^m + \\gamma \\sum_{a} Q(s_{t+1}^m, a) \\pi(a|s_{t+1}^m)\n\\]\nAs presented (when the policy \\(\\pi\\) used in the target is the same as the behavior policy generating the data), this is an on-policy method.\nBut to make it off-policy, just need \\(\\text{policy generating the trajectory} \\neq \\pi \\text{ in target}\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#compare",
    "href": "posts/reinforcement-learning/index.html#compare",
    "title": "Reinforcement Learning",
    "section": "Compare",
    "text": "Compare\n\nSarsa has longer path, because it just took the action it actually took, which was most of the time exploratory, the one with \\(\\epsilon \\text{exploration}\\) policy that balance out\nQ-Learning does not use this \\(\\epsilon \\text{exploration}\\) policy, it uses \\(\\max\\) operator\nExpected Sarsa use weighted average, so yeah, always a safe choice."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#summary-of-td",
    "href": "posts/reinforcement-learning/index.html#summary-of-td",
    "title": "Reinforcement Learning",
    "section": "Summary of TD",
    "text": "Summary of TD\nGoal of Q-Learning is updating Q-Table to optimal where: \\[\nQ_*(s_t, a_t) = \\mathbb{E} \\left[ r_t + \\gamma \\max_a Q_*(s_{t+1}, a) \\right]\n\\]\nAlso called Bellman Optimality Equation: every subsequence of a optimal sequence must also be optimal\nLearning Q-values:\n\nSARSA:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n\\]\nExpected SARSA:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\sum_a \\pi(a \\mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\nQ-Learning:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n\\]\n\nLearning V-values:\n\\[\nV(s_t) \\leftarrow V(s_t) + \\alpha \\left[ r_t + \\gamma V(s_{t+1}) - V(s_t) \\right]\n\\]\nWhere \\(\\alpha\\) is the learning rate."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#on-policy-evaluation-with-function-approximation",
    "href": "posts/reinforcement-learning/index.html#on-policy-evaluation-with-function-approximation",
    "title": "Reinforcement Learning",
    "section": "On-Policy Evaluation with Function Approximation",
    "text": "On-Policy Evaluation with Function Approximation\nGoal remains to approximate the state-value function \\(v_\\pi(s)\\). Data generated from a given fixed policy \\(\\pi\\).\nWe now learn a parameterized function \\(\\hat{v}(s, \\mathbf{w})\\), where:\n\n\\(\\mathbf{w} \\in \\mathbb{R}^d\\) is a vector of parameters\na state \\(s\\) as input\n\nWe learn \\(w\\) and hope that \\(v_\\pi(s) \\approx \\hat{v}(s, \\mathbf{w})\\)\nSince \\(d \\ll |\\mathcal{S}|\\), any change to \\(\\mathbf{w}\\) can simultaneously change \\(\\hat{v}(s, \\mathbf{w})\\) for many (or all) states \\(s\\). Different from tabular methods, where an update to \\(V(s)\\) for a specific state \\(s\\) affects only that state’s value."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-represent-mathbfxs",
    "href": "posts/reinforcement-learning/index.html#how-to-represent-mathbfxs",
    "title": "Reinforcement Learning",
    "section": "How to represent \\(\\mathbf{x}(s)\\)?",
    "text": "How to represent \\(\\mathbf{x}(s)\\)?\nExample 1: two simple features for given image state \\(s\\):\n\n\\(x_1(s)\\): The average of all pixel values in the image.\n\\(x_2(s)\\): The standard deviation of all pixel values in the image.\n\n=&gt; feature vector \\(\\mathbf{x}(s) = \\begin{bmatrix} x_1(s) \\\\ x_2(s) \\end{bmatrix}\\)\nWith these features, we can construct a linear value function to approximate \\(v_\\pi(s)\\): \\[\n\\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s)^T \\mathbf{w}\n\\]\nExample 2: Mountain Car below with a dynamic model of velocity \\(dx\\) and position \\(x\\)\nExample 3: Proto Points and Radius Basis Function will be discussed in next chapter Policy Gradient Method"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#goal-how-to-get-mathbfw",
    "href": "posts/reinforcement-learning/index.html#goal-how-to-get-mathbfw",
    "title": "Reinforcement Learning",
    "section": "Goal: How to get \\(\\mathbf{w}\\)?",
    "text": "Goal: How to get \\(\\mathbf{w}\\)?\nThe ‘best’ \\(\\mathbf{w}\\) minimizes: \\[\n\\overline{VE}(\\mathbf{w}) = \\sum_{s \\in \\mathcal{S}} \\mu(s)[v_\\pi(s) - \\hat{v}(s, \\mathbf{w})]^2\n\\] where \\(\\mu(\\cdot)\\) is a distribution over states (frequency of visiting each state).\n\nWe observe a surrogate for \\(v_\\pi(S_t)\\): \\(U_t\\): Since we don’t know \\(v_\\pi(S_t)\\) exactly, we use a sample-based estimate or target, \\(U_t\\), as a stand-in. This \\(U_t\\) could be the Monte Carlo return \\(G_t\\), or an n-step TD target \\(g_{t:t+n}\\), or a 1-step TD target \\((R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}))\\).\n\nUpdate Rule we don’t have direct access to \\(v_\\pi(s)\\) for all states =&gt; Stochastic Gradient Descent (SGD) for updating our parameters \\(\\mathbf{w}\\): \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\] Where:\n\n\\(\\alpha\\) is the learning rate (step size).\n\\(\\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right]\\) is the TD error (or prediction error) based on our sample \\(U_t\\).\n\\(\\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\\) is the gradient of our estimated value function with respect to its parameters \\(\\mathbf{w}\\), evaluated at state \\(S_t\\). This gradient tells us how to adjust \\(\\mathbf{w}\\) to change \\(\\hat{v}(S_t, \\mathbf{w})\\) in the desired direction."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-obtain-the-target-u_t",
    "href": "posts/reinforcement-learning/index.html#how-to-obtain-the-target-u_t",
    "title": "Reinforcement Learning",
    "section": "How to Obtain the Target \\(U_t\\)",
    "text": "How to Obtain the Target \\(U_t\\)\nIn the Stochastic Gradient Descent update rule: \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\] The term \\(U_t\\) serves as our sample-based target for the true value \\(v_\\pi(S_t)\\). Since \\(v_\\pi(S_t)\\) is unknown, we must derive \\(U_t\\) from our observed experience. The choice of \\(U_t\\) determines whether our method leans towards Monte Carlo or Temporal Difference approaches:\n1. Monte Carlo Target: If the target \\(U_t\\) is the full Monte Carlo return from state \\(S_t\\) to the end of the episode, then we are using a Gradient Monte Carlo method: \\[\nU_t = G_t\n\\] where \\(G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1}R_T\\) is the total discounted return observed from time step \\(t\\) until the terminal state \\(T\\).\n\nCharacteristics:\n\nUnbiased: If \\(G_t\\) is an unbiased estimate of \\(v_\\pi(S_t)\\) (which it is, on average), then using it as \\(U_t\\) can lead to the parameters \\(\\mathbf{w}\\) converging to a local optimum of the Mean Squared Value Error (\\(\\overline{VE}(\\mathbf{w})\\)).\nHigh Variance: \\(G_t\\) can be noisy due to the sum of many random rewards.\nRequires complete episodes: We must wait until the episode ends to compute \\(G_t\\).\n\n\n2. Temporal Difference (TD) Target: If the target \\(U_t\\) is derived using bootstrapping (i.e., using an estimate of the value of a future state), then we are using a Semi-Gradient TD method. The most common is the 1-step TD target: \\[\nU_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\n\\] Here, \\(R_{t+1}\\) is the actual reward observed, and \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\) is our current estimate of the value of the next state \\(S_{t+1}\\). For this specific update, \\(\\mathbf{w}\\) in \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\) is usually the online network’s weights, not a target network’s weights in this basic formulation.\n\nCharacteristics:\n\nBiased: especially in the beginning with crappy initialized value \\(\\hat{v}(S_{t+1}, \\mathbf{w})\\).\nLower Variance: It typically has lower variance than the Monte Carlo target because it depends on only one actual reward and then uses a smoothed estimate for the rest of the future.\nOnline Learning: Updates can be performed after each single time step, without waiting for the end of an episode.\nSemi-Gradient: Since \\(U_t\\) depends on \\(\\mathbf{w}\\), our update rule is not a true gradient step. The gradient \\(\\nabla_{\\mathbf{w}} L(\\mathbf{w})\\) for the loss \\(\\left( (R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})) - \\hat{v}(S_t, \\mathbf{w}) \\right)^2\\) would actually involve the derivative of \\(U_t\\) (\\(\\hat{v}(S_{t+1}, \\mathbf{w})\\)) with respect to \\(\\mathbf{w}\\). =&gt; semi-gradient means: \\(\\nabla_{\\mathbf{w}} \\hat{v}(S_{t+1}, \\mathbf{w})\\) is omitted for simplicity and stability, only taking \\(\\nabla_{\\mathbf{w}} \\hat{v}(S_{t}, \\mathbf{w})\\).\nNo Guarantee of Convergence (to Global Optimum): Because it’s not a true gradient of the overall \\(\\overline{VE}(\\mathbf{w})\\), we generally don’t guarantee convergence to the global optimum of the Mean Squared Value Error, even if the optimal \\(\\mathbf{w}\\) is unique. However, for linear function approximation, it can still converge to a local optimum. For non-linear approximators like neural networks, theoretical guarantees are weaker, but these methods still perform very well in practice.\n\n\nThe update rule remains: \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation",
    "href": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation",
    "title": "Reinforcement Learning",
    "section": "Extension to On-Policy Control with Function Approximation",
    "text": "Extension to On-Policy Control with Function Approximation\nSo far is just policy evaluation (approximating \\(v_\\pi(s)\\)). Now extend directly to control problems (finding an optimal policy), typically by approximating the action-value function \\(q_\\pi(s,a)\\) or \\(q_*(s,a)\\). \\[\n\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\left[ U_t - \\hat{q}(S_t, A_t, \\mathbf{w}) \\right] \\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})\n\\] Here, \\(\\hat{q}(S_t, A_t, \\mathbf{w})\\) is our function approximator’s estimate of the action-value for the state-action pair \\((S_t, A_t)\\) using parameters \\(\\mathbf{w}\\). The term \\(\\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})\\) is the gradient of this estimate with respect to \\(\\mathbf{w}\\).\nFor Semi-gradient 1-step Sarsa, the target \\(U_t\\) is defined as: \\[\nU_t = R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\mathbf{w})\n\\]\nSince both the action being evaluated (\\(A_t\\)) and the action used to construct the target (\\(A_{t+1}\\)) are chosen according to the same behavior policy (which is actively being improved based on \\(\\hat{q}\\)), this method is on-policy. The agent learns the value of the policy it is currently following, and this policy itself is improved through action selection methods (e.g., \\(\\epsilon\\)-greedy) based on the learned \\(\\hat{q}\\) values.\nExample: Linear Action-Value Function for the Mountain Car Task\nTo illustrate how function approximation can be used for action-value functions, let’s consider a scenario like The Mountain Car Task. We approximate the action-value function \\(\\hat{q}(s, a, \\mathbf{w})\\) using a linear function approximator instead of a Q-Table: \\[\n\\hat{q}(s, a, \\mathbf{w}) = \\begin{cases}\n    \\mathbf{w}_{-1}^T \\mathbf{x}(s) & \\text{if } a = -1 \\\\\n    \\mathbf{w}_{0}^T \\mathbf{x}(s) & \\text{if } a = 0 \\\\\n    \\mathbf{w}_{1}^T \\mathbf{x}(s) & \\text{if } a = 1\n\\end{cases}\n\\] Where:\n\naction \\(a\\) (-1, 0 -1)\n\\(\\mathbf{x}(s)\\) is the feature representation of the state \\(s\\) (length 120 decoded from position & velocity)\n\\(\\mathbf{w}_{-1}\\), \\(\\mathbf{w}_{0}\\), and \\(\\mathbf{w}_{1}\\) are distinct weight vectors, each corresponding to one of the possible actions.\nThe overall parameter vector \\(\\mathbf{w}\\) for the entire function approximator would be the concatenation of these individual action-specific weight vectors (i.e., \\(\\mathbf{w} = [\\mathbf{w}_{-1}, \\mathbf{w}_{0}, \\mathbf{w}_{1}]\\))."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation-1",
    "href": "posts/reinforcement-learning/index.html#extension-to-on-policy-control-with-function-approximation-1",
    "title": "Reinforcement Learning",
    "section": "Extension to On-Policy Control with Function Approximation",
    "text": "Extension to On-Policy Control with Function Approximation\nWe all know what that is, just that when we combine three things together:\n\noff-policy\nfunction approximation\nbootstrapping\n\nwe will have problem with convergence, which may be solved by the next topic …"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#how-to-represent-xs",
    "href": "posts/reinforcement-learning/index.html#how-to-represent-xs",
    "title": "Reinforcement Learning",
    "section": "How to represent \\(x(s)\\)?",
    "text": "How to represent \\(x(s)\\)?\n\nProto Points and Radial Basis Functions (RBFs)\ninstead of having a neural network directly compute an output, the state is first transformed into a set of features by measuring its “similarity” or “proximity” to a predefined set of “proto points” or “centers.”\n\nDefine Proto Points (Centers):\nFirst, you select a number of “proto points” or “centers” in your state space. Let’s call these centers \\(c_1, c_2, \\dots, c_k\\). These centers are essentially fixed, representative points in your environment’s state space. They could be chosen: manually or randomly\n\n\nDefine Basis Functions (e.g., Radial Basis Functions):\nFor each center \\(c_j\\), you define a basis function \\(\\phi_j(s)\\). A common choice is a Gaussian Radial Basis Function: \\[\n\\phi_j(s) = \\exp \\left( -\\frac{\\|s - c_j\\|^2}{2\\sigma_j^2} \\right)\n\\]\nwhere:\n\n\\(\\|s - c_j\\|^2\\) is the squared Euclidean distance between the current state \\(s\\) and the center \\(c_j\\).\n\\(\\sigma_j^2\\) is a variance or width parameter for that basis function, controlling how broad its “influence” is.\n\nWhat does \\(\\phi_j(s)\\) mean? It’s a measure of how “similar” or “close” the current state \\(s\\) is to the center \\(c_j\\). It peaks at 1 when \\(s = c_j\\) and decays to 0 as \\(s\\) moves away from \\(c_j\\).\n\n\nConstruct the Feature Vector:\nFor any given state \\(s\\), you compute the value of each basis function: \\[\n\\mathbf{x}(s) = \\begin{bmatrix} \\phi_1(s) \\\\ \\phi_2(s) \\\\ \\vdots \\\\ \\phi_k(s) \\end{bmatrix}\n\\] This \\(\\mathbf{x}(s)\\) is your feature vector, where each feature represents the activation of a “proto point.”\n\n\nLinear Function Approximation:\nNow, these features are used in a linear function approximator. The parameters you learn are the weights associated with each of these basis functions.\n\nFor a value function: \\[\n  \\hat{V}(s, \\mathbf{w}) = \\sum_{j=1}^k w_j \\phi_j(s) = \\mathbf{w}^T \\mathbf{x}(s)\n  \\] Here, \\(\\mathbf{w}\\) is the vector \\([w_1, \\dots, w_k]\\), and \\(w_j\\) is the weight for the \\(j\\)-th proto point’s influence.\nFor a policy (e.g., logits in a softmax): For each action \\(a\\), you’d have a separate weight vector \\(\\mathbf{\\theta}_a\\), and the logits for the policy could be: \\[\n  h(s,a,\\mathbf{\\theta}_a) = \\mathbf{\\theta}_a^T \\mathbf{x}(s)\n  \\] Then, \\(\\pi(a|s,\\mathbf{\\theta}) = \\text{softmax}(h(s,a,\\mathbf{\\theta}_a))\\).\n\n\n\n\nExample: Policy Gradient with Proto Points (RBFs) for Mountain Car\nImagine our Mountain Car environment again. The state \\(s\\) is (position, velocity), which are continuous. We want to learn a policy \\(\\pi(a|s, \\mathbf{\\theta})\\) directly.\n1. Define Proto Points (Centers) in State Space:\nLet’s say we define \\(K=4\\) proto points (centers) \\(c_j\\) in our 2D (position, velocity) state space. For simplicity, let’s pick them:\n\n\\(c_1 = (-0.5, 0.0)\\) (Mid-left, still)\n\\(c_2 = (0.0, 0.0)\\) (Center, still)\n\\(c_3 = (0.5, 0.0)\\) (Mid-right, still)\n\\(c_4 = (-0.2, 0.05)\\) (Slightly left, moving right)\n\nWe also define a width \\(\\sigma^2\\) for all RBFs (or separate \\(\\sigma_j^2\\) values).\n2. Create the State Feature Vector \\(\\mathbf{x}(s)\\) using RBFs:\nFor any state \\(s = (\\text{pos}, \\text{vel})\\), we calculate its similarity to each of these 4 proto points using a Gaussian RBF. Our feature vector \\(\\mathbf{x}(s)\\) will have 4 dimensions:\n\\[\n\\mathbf{x}(s) = \\begin{bmatrix}\n\\phi_1(s) = \\exp \\left( -\\frac{\\|s - c_1\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_2(s) = \\exp \\left( -\\frac{\\|s - c_2\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_3(s) = \\exp \\left( -\\frac{\\|s - c_3\\|^2}{2\\sigma^2} \\right) \\\\\n\\phi_4(s) = \\exp \\left( -\\frac{\\|s - c_4\\|^2}{2\\sigma^2} \\right)\n\\end{bmatrix}\n\\] This \\(\\mathbf{x}(s)\\) now serves as our “meaningful” representation of the state, telling us how similar \\(s\\) is to certain key points in the environment.\n3. Define the Policy \\(\\pi(a|s, \\mathbf{\\theta})\\) using these Features:\nFor each action \\(a \\in \\{-1, 0, 1\\}\\), we define a linear combination of these features to get a “score” or “logit” for that action.\nLet \\(\\mathbf{\\theta}_{-1}\\), \\(\\mathbf{\\theta}_{0}\\), \\(\\mathbf{\\theta}_{1}\\) be our learnable parameter vectors (weights), each of size \\(K=4\\). The total policy parameters \\(\\mathbf{\\theta}\\) would be the concatenation of these three vectors.\nThe logits for each action are: * \\(h(s, a=-1, \\mathbf{\\theta}) = \\mathbf{\\theta}_{-1}^T \\mathbf{x}(s)\\) * \\(h(s, a=0, \\mathbf{\\theta}) = \\mathbf{\\theta}_{0}^T \\mathbf{x}(s)\\) * \\(h(s, a=1, \\mathbf{\\theta}) = \\mathbf{\\theta}_{1}^T \\mathbf{x}(s)\\)\nAnd the policy probabilities are then given by the softmax function: \\[\\pi(a|s, \\mathbf{\\theta}) = \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_{a'} e^{h(s,a',\\mathbf{\\theta})}}\\]\n4. Update the Policy Parameters (\\(\\mathbf{\\theta}\\)) using Policy Gradients (REINFORCE):\nWhen we collect a trajectory and compute returns \\(G_t\\), the REINFORCE update rule applies: \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha G_t \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t|s_t, \\mathbf{\\theta})\\]\nLet’s look at a specific component of this update, say for \\(\\mathbf{\\theta}_{-1}\\): \\[\\mathbf{\\theta}_{-1} \\leftarrow \\mathbf{\\theta}_{-1} + \\alpha G_t \\nabla_{\\mathbf{\\theta}_{-1}} \\ln \\pi(a_t|s_t, \\mathbf{\\theta})\\]\nIf \\(a_t = -1\\), and \\(G_t\\) is high, the gradient \\(\\nabla_{\\mathbf{\\theta}_{-1}} \\ln \\pi(-1|s_t, \\mathbf{\\theta})\\) will push \\(\\mathbf{\\theta}_{-1}\\) to increase the score for action -1 when in state \\(s_t\\). Since \\(h(s, a, \\mathbf{\\theta}_a) = \\mathbf{\\theta}_a^T \\mathbf{x}(s)\\), the gradient with respect to \\(\\mathbf{\\theta}_a\\) is simply \\(\\mathbf{x}(s)\\). So, the update to \\(\\mathbf{\\theta}_{-1}\\) will be proportional to \\(\\mathbf{x}(s_t)\\). This means:\n\nIf \\(s_t\\) is very similar to \\(c_1\\) (so \\(\\phi_1(s_t)\\) is high), then \\(w_{-1,1}\\) (the weight for \\(c_1\\) and action -1) will be adjusted significantly.\nIf \\(s_t\\) is far from \\(c_1\\) (so \\(\\phi_1(s_t)\\) is near zero), then \\(w_{-1,1}\\) will be adjusted very little by this particular sample.\n\nThis means the learning process adjusts the weights for each proto point for each action based on the returns received.\n\n\nAre Probabilities Fixed When Starting with Proto Points?\nNo, the probabilities of actions are not fixed when you start with proto points.\n\nFixed: The proto points \\(c_j\\) themselves are fixed in the state space, and the basis functions \\(\\phi_j(s)\\) are fixed (their shape and location are determined at the start).\nLearned: However, the parameters \\(\\mathbf{\\theta}\\) (the weights associated with each proto point for each action) are learnable.\n\nThe idea of relying action parameter \\(\\theta\\) on fixed state representation \\(x(s)\\) is because action \\(a\\) depends on the state \\((a|s)\\), so we need to fix states representation first, use it as a basis so that action parameter can learn."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#a-monte-carlo-style-policy-gradient-algorithm",
    "href": "posts/reinforcement-learning/index.html#a-monte-carlo-style-policy-gradient-algorithm",
    "title": "Reinforcement Learning",
    "section": "A Monte Carlo Style Policy Gradient Algorithm",
    "text": "A Monte Carlo Style Policy Gradient Algorithm\nThe core idea is to increase the probability of actions that lead to high returns and decrease the probability of actions that lead to low returns.\nInitialize:\n\nFunctional form for the policy: \\(\\pi(a|s, \\mathbf{\\theta})\\) (e.g., a neural network that outputs action probabilities given a state, parameterized by \\(\\mathbf{\\theta}\\)).\nInitial parameters: \\(\\mathbf{\\theta}\\) (e.g., randomly initialized weights for a neural network).\nStep size (learning rate): \\(\\alpha\\)\n\nAlgorithm:\nFor \\(m = 1, \\dots, M\\) (for each episode):\n\nSample a trajectory under the current policy \\(\\pi(\\cdot|\\cdot, \\mathbf{\\theta})\\): \\(s_0^m, a_0^m, r_1^m, \\dots, a_{T_m-1}^m, r_{T_m}^m\\) (where \\(T_m\\) is the length of the episode).\nFor \\(t = 0, \\dots, T_m - 1\\) (for each time step in the trajectory):\n\nCompute the return from time \\(t\\): \\(g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots + \\gamma^{T_m - t - 1} r_{T_m}^m\\) (This is the total discounted reward from \\(t+1\\) until the end of the episode).\nUpdate the policy parameters: \\[\n  \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t g_t^m \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\n  \\]\n\n\nExplanation of the Update Rule:\n\n\\(\\alpha\\): The learning rate, controlling the step size of the update.\n\\(\\gamma^t\\): The discount factor raised to the power of \\(t\\). This term accounts for the discounting of future rewards, ensuring that immediate rewards have a stronger influence on early actions.\n\\(g_t^m\\): Scaling stepsize with the Monte Carlo return from time step \\(t\\). e.g. If return high =&gt; big step size, if return negative =&gt; step backward\n\\(\\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\): This is the gradient of the log-probability of the action taken.\n\n\\(\\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\): The logarithm of the probability that the policy \\(\\pi\\) would choose action \\(a_t^m\\) in state \\(s_t^m\\) with current parameters \\(\\mathbf{\\theta}\\). This is very important!!! It is a normalizer for cases like a random policy \\(\\pi\\) pick a positive (but low return) action too often, leading to pushing the probability too much, so by taking the gradient of \\(ln\\) of it, it limit down it, (and also the opposite for high reward actions but the behavior policy did not pick as many time to push enough higher). These situations usually happen with initialization of that lower reward already too high, and then add up that the policy random pick it too often pushing it even further.\n\\(\\nabla_{\\mathbf{\\theta}}\\): The gradient operator with respect to the policy parameters \\(\\mathbf{\\theta}\\). This term tells us how to change \\(\\mathbf{\\theta}\\) to increase the log-probability of taking action \\(a_t^m\\) in state \\(s_t^m\\).\n\n\nIntuition:\nThe update rule essentially says: if action \\(a_t^m\\) taken in state \\(s_t^m\\) leads to a high return (\\(g_t^m\\) is large and positive), then increase the probability of taking that action in that state. If it leads to a low (or negative) return, decrease its probability. The \\(\\gamma^t\\) term ensures that actions taken earlier in the trajectory, which influence more of the subsequent rewards, are given appropriate credit.\nREINFORCE is a fundamental algorithm that demonstrates the direct optimization of a policy, laying the groundwork for more advanced policy gradient methods."
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#variance-problem",
    "href": "posts/reinforcement-learning/index.html#variance-problem",
    "title": "Reinforcement Learning",
    "section": "Variance Problem",
    "text": "Variance Problem\nReturn \\(g_t^m\\) rewards the action when it is positive, and punish the action when it is negative. But some cases all actions can be positive, just some are less positive than other =&gt; it should not be encouraged.\nSolution: With baseline (e.g., \\(V_\\pi(s,w)\\)) =&gt; then we have \\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t (g_t^m - b(s_t^m)) \\nabla_{\\mathbf{\\theta}} \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\] so that:\n\nif \\(g_t^m &gt; V_\\pi(s,w)\\), the scaling factor is positive =&gt; increase probability of action \\(a\\)\nif \\(g_t^m &lt; V_\\pi(s,w)\\), the scaling factor is negative =&gt; decrease probability of action \\(a\\)"
  },
  {
    "objectID": "posts/reinforcement-learning/index.html#sum-up-reinforce-gradient-policy-with-baseline",
    "href": "posts/reinforcement-learning/index.html#sum-up-reinforce-gradient-policy-with-baseline",
    "title": "Reinforcement Learning",
    "section": "Sum up: REINFORCE Gradient Policy with Baseline",
    "text": "Sum up: REINFORCE Gradient Policy with Baseline\nTo specify upfront:\n\nFunctional forms \\(\\pi(a|s, \\mathbf{\\theta})\\), \\(\\hat{v}(s, \\mathbf{w})\\)\nInitial \\(\\mathbf{\\theta}, \\mathbf{w}\\)\nStep sizes \\(\\alpha^{\\theta}, \\alpha^{w}\\)\n\nFor \\(m = 1, \\dots, M\\):\n\nSample: \\(s_0^m, a_0^m, r_1^m \\dots a_{T_m-1}^m, r_{T_m}^m\\)\nFor \\(t = 0, \\dots, T_m - 1\\): \\[g_t^m \\leftarrow r_{t+1}^m + \\gamma r_{t+2}^m + \\dots\\]\n\n\\[\\delta \\leftarrow g_t^m - \\hat{v}(s_t^m, \\mathbf{w})\\]\n\\[\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^w \\delta \\nabla_{\\mathbf{w}} \\hat{v}(s_t^m, \\mathbf{w})\\]\n\\[\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^{\\theta} \\gamma^t \\delta \\nabla \\ln \\pi(a_t^m|s_t^m, \\mathbf{\\theta})\\]"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]