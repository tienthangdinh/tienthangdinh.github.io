---
title: "From Autoencoder to Dreamer"
date: 2025-07-12
categories: [Machine Learning, Image Processing, Model-based Learning, Reinforcement Learning]
format:
  html:
    toc: true
    code-fold: true
    math: mathjax
---
# Variational Autoencoders (VAEs)

**Key Idea:** Instead of mapping an input to a single point in the latent space, VAE maps it to parameters of a probability distribution (typically a Gaussian distribution) in the latent space:

* An encoder that maps data to a distribution in latent space.
* A decoder that maps samples from this latent distribution back to data space.


## Mathematical Formulation of VAE

Let's denote:

* $\mathbf{x}$: An input data point (e.g., an image)
* $\mathbf{z}$: A latent variable (vector) in the lower-dimensional latent space
* $p(\mathbf{x})$: The true, unknown data distribution we want to model
* $p(\mathbf{z})$: The prior distribution over the latent variables (typically a simple distribution like $\mathcal{N}(0,I)$)
* $p_\theta(\mathbf{x}|\mathbf{z})$: The decoder (also called generative model or likelihood). This is a neural network parameterized by $\theta$ that outputs the parameters of the distribution over $\mathbf{x}$ given $\mathbf{z}$.
* $q_\phi(\mathbf{z}|\mathbf{x})$: The encoder (also called inference model or approximate posterior). This is a neural network parameterized by $\phi$ that outputs the parameters of the distribution over $\mathbf{z}$ given $\mathbf{x}$.

**Goal:** is to maximize the marginal likelihood:
$$p(\mathbf{x}) = \frac{p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})}$$ which we do not know the other element to compute, or
$$\log p_\theta(\mathbf{x}) = \log \int p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}$$ which is intractable

Therefore, VAEs optimize the Evidence Lower Bound (ELBO), which is a lower bound on the log-likelihood of the data, we will derive it now

### Derivation of ELBO:

The log-likelihood of a data point $\mathbf{x}$ can be written as:

$$\log p_\theta(\mathbf{x}) = \log p_\theta(\mathbf{x})$$
$$= \log p_\theta(\mathbf{x}) \int q_\phi(\mathbf{z}|\mathbf{x}) d\mathbf{z} \quad \text{Multiply by 1}$$
$$= \int q_\phi(\mathbf{z}|\mathbf{x}) \log p_\theta(\mathbf{x}) d\mathbf{z} \quad \text{Bring inside the integral}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x})] \quad \text{Definition of expectation}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{p_\theta(\mathbf{z}|\mathbf{x})} \right] \quad \text{Apply the equation } p_\theta(\mathbf{x}) = \frac{p_\theta(\mathbf{x}, \mathbf{z})}{p_\theta(\mathbf{z}|\mathbf{x})}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z}) q_\phi(\mathbf{z}|\mathbf{x})}{p_\theta(\mathbf{z}|\mathbf{x}) q_\phi(\mathbf{z}|\mathbf{x})} \right] \quad \text{Multiply by 1}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} + \log \frac{q_\phi(\mathbf{z}|\mathbf{x})}{p_\theta(\mathbf{z}|\mathbf{x})} \right] \quad \text{Split the expectation}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] + D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p_\theta(\mathbf{z}|\mathbf{x})) \quad \text{KL divergence}$$

The second term is the Kullback-Leibler (KL) divergence, it is non-negative, now we only focus on maximizing the first term ELBO:

$$\log p_\theta(\mathbf{x}) \ge \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] \quad \text{Chain rule of probability}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] + \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p(\mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] \quad \text{Split the expectation}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z})) \quad \text{KL divergence}$$

**This is the objective function for the VAE (Loss Function), also the ELBO** to maximize, or minimizing the negative ELBO.

$$\mathcal{L}(\theta, \phi, \mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))$$


### First Component: Reconstruction Loss (Negative Expected Log-Likelihood): $\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})]$

This term encourages the decoder to reconstruct the input $\mathbf{x}$ accurately from a latent sample $\mathbf{z}$ drawn from the encoder's output distribution.

We will derive it now

#### Assumption: Data is Gaussian Distributed

We assume that each data point $\mathbf{x}$ (e.g., an image, where each pixel is a dimension) is drawn from a Gaussian distribution, given the latent variable $\mathbf{z}$ that the decoder outputs. For simplicity, let's assume the dimensions of $\mathbf{x}$ are independent given $\mathbf{z}$, and they share a fixed variance $\sigma^2$.

So, for each dimension $j$ of $\mathbf{x}$, $x_j$ is distributed as:
$p_\theta(x_j|\mathbf{z}) = \mathcal{N}(x_j; \mu_j(\mathbf{z}), \sigma^2)$

Here:

* $\mu_j(\mathbf{z})$ is the mean for the $j$-th dimension, which is the output of your decoder network for that dimension when given $\mathbf{z}$.
* $\sigma^2$ is the variance. For simplicity, we often assume a fixed $\sigma^2$ (e.g., $\sigma^2=1$, or a small constant, or even absorbed into the weights). More advanced VAEs can learn this variance as well (the decoder outputs both mean and variance).

#### Probability Density Function (PDF) of a Gaussian

The PDF for a single-dimensional Gaussian variable $x_j$ is:

$f(x_j) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2}\right)$

#### Log-Likelihood for a Multi-dimensional Data Point $\mathbf{x}$

Since we assume the dimensions of $\mathbf{x}$ are independent given $\mathbf{z}$, the joint probability $p_\theta(\mathbf{x}|\mathbf{z})$ is the product of the individual probabilities:

$p_\theta(\mathbf{x}|\mathbf{z}) = \prod_{j=1}^{D_x} p_\theta(x_j|\mathbf{z})$

where $D_x$ is the dimensionality of $\mathbf{x}$ (e.g., number of pixels in an image).
Now, let's take the logarithm of this product:

$$\log p_\theta(\mathbf{x}|\mathbf{z}) = \log \left( \prod_{j=1}^{D_x} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2}\right) \right)$$
$$= \sum_{j=1}^{D_x} \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2}\right) \right)$$
$$= \sum_{j=1}^{D_x} \left( \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \log\left(\exp\left(-\frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2}\right)\right) \right)$$
$$= \sum_{j=1}^{D_x} \left( -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2} \right)$$
$$= \sum_{j=1}^{D_x} \left( -\frac{1}{2}\log(2\pi\sigma^2) \right) - \sum_{j=1}^{D_x} \left( \frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2} \right)$$The first term is a constant multiplied by $D_x$:
$$= -\frac{D_x}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{j=1}^{D_x} (x_j - \mu_j(\mathbf{z}))^2$$
$$\text{MSE} = \frac{1}{D_x} \sum_{j=1}^{D_x} (x_j - \mu_j(\mathbf{z}))^2$$


### Second Term: KL Divergence (Regularization Term): $D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))$

This term acts as a regularizer. It pushes the approximate posterior $q_\phi(\mathbf{z}|\mathbf{x})$ (the distribution output by the encoder for a given $\mathbf{x}$) to be close to the prior distribution $p(\mathbf{z})$ (e.g., $\mathcal{N}(0,I)$).

If $q_\phi(\mathbf{z}|\mathbf{x})$ is assumed to be a diagonal Gaussian $\mathcal{N}(\mu, \Sigma)$ (where $\Sigma$ is diagonal) and $p(\mathbf{z})$ is $\mathcal{N}(0,I)$, the KL divergence has a closed-form solution:

$$D_{KL}(\mathcal{N}(\mu,\Sigma)||\mathcal{N}(0,I)) = \frac{1}{2} \sum_{j=1}^{D_z} (\exp(\sigma_j) + \mu_j^2 - 1 - \sigma_j)$$
where $D_z$ is the dimensionality of $\mathbf{z}$, and $\mu_j$ and $\sigma_j$ are the mean and log-variance (diagonal elements of $\Sigma$) for the $j$-th latent dimension, as output by the encoder.

#### Numerical Example for $q_\phi(\mathbf{z}|\mathbf{x})$

Let's say our latent space $\mathbf{z}$ is 2-dimensional ($D_z=2$).
Our input is a specific image $\mathbf{x}_{cat}$ (an image of a cat).

The encoder network takes $\mathbf{x}_{cat}$ as input. Its output layer (after processing through several hidden layers) has two sets of $D_z=2$ nodes:

* **Mean Output Nodes:** For $\mu_\phi(\mathbf{x}_{cat})$
* **Log-Variance Output Nodes:** For $\log(\sigma_\phi(\mathbf{x}_{cat})^2)$ (we use log-variance for numerical stability, as variance must be positive).

Suppose for this specific $\mathbf{x}_{cat}$, the encoder outputs:

$\mu_\phi(\mathbf{x}_{cat})=\begin{bmatrix} 0.8 \\ -1.2 \end{bmatrix}$

$\log(\sigma_\phi(\mathbf{x}_{cat})^2)=\begin{bmatrix} -0.2 \\ 0.1 \end{bmatrix}$

From the log-variances, we calculate the variances:

$\sigma_1^2 = \exp(-0.2) \approx 0.8187$
$\sigma_2^2 = \exp(0.1) \approx 1.1052$

So, for this input $\mathbf{x}_{cat}$, the encoder defines the latent distribution:
$$q_\phi(\mathbf{z}|\mathbf{x}_{cat})=\mathcal{N}\left(\mathbf{z};\begin{bmatrix} 0.8 \\ -1.2 \end{bmatrix},\begin{bmatrix} 0.8187 & 0 \\ 0 & 1.1052 \end{bmatrix}\right)$$

This means:

* The first latent dimension ($z_1$) is modeled by a Gaussian with mean 0.8 and variance 0.8187.
* The second latent dimension ($z_2$) is modeled by a Gaussian with mean $-1.2$ and variance 1.1052.

These two dimensions are assumed to be independent (because the off-diagonal elements of the covariance matrix are zero).

When we "sample $\mathbf{z}$ from $q_\phi(\mathbf{z}|\mathbf{x}_{cat})$" using the reparameterization trick, we would:

1.  Sample $\epsilon_1 \sim \mathcal{N}(0,1)$ and $\epsilon_2 \sim \mathcal{N}(0,1)$.
2.  Calculate $z_1 = 0.8 + \sqrt{0.8187} \cdot \epsilon_1$
3.  Calculate $z_2 = -1.2 + \sqrt{1.1052} \cdot \epsilon_2$

The resulting $\mathbf{z}=\begin{bmatrix} z_1 \\ z_2 \end{bmatrix}$ is then passed to the decoder.

This formulation allows the VAE to learn a distribution for each input, rather than a single point, enabling the desired properties of a continuous and meaningful latent space.


#### Numerical Example Regularization using KL

Imagine a very simplified VAE where our latent space $\mathbf{z}$ is just one-dimensional ($D_z=1$). Our prior $p(\mathbf{z})$ is $\mathcal{N}(0,1)$ (mean 0, variance 1). The VAE's job is to learn an encoder ($q_\phi(\mathbf{z}|\mathbf{x})$) and a decoder ($p_\theta(\mathbf{x}|\mathbf{z})$) such that:

* The decoder can reconstruct $\mathbf{x}_A$ from samples of $q_\phi(\mathbf{z}|\mathbf{x}_A)$.
* The decoder can reconstruct $\mathbf{x}_B$ from samples of $q_\phi(\mathbf{z}|\mathbf{x}_B)$.
* The KL divergence $D_{KL}(q_\phi(\mathbf{z}|\mathbf{x})||p(\mathbf{z}))$ is minimized for both $\mathbf{x}_A$ and $\mathbf{x}_B$.

**Scenario 1: No KL Regularization (like a vanilla Autoencoder)**

If there were no KL term, the encoder might learn to map $\mathbf{x}_A$ to a specific point $\mathbf{z}_A$ and $\mathbf{x}_B$ to a specific point $\mathbf{z}_B$.

* Encoder output for $\mathbf{x}_A$: $\mu_A=-5.0, \sigma_A=0.001$ (a very tight distribution at -5.0)
* Encoder output for $\mathbf{x}_B$: $\mu_B=+5.0, \sigma_B=0.001$ (a very tight distribution at +5.0)

**Scenario 2: With KL Regularization (VAE)**

Now, the KL term $D_{KL}(q_\phi(\mathbf{z}|\mathbf{x})||\mathcal{N}(0,1))$ is active.

Let's say the encoder tries to map $\mathbf{x}_A$ and $\mathbf{x}_B$ far apart again:

* Encoder output for $\mathbf{x}_A$: $\mu_A=-5.0, \sigma_A=0.1$
* Encoder output for $\mathbf{x}_B$: $\mu_B=+5.0, \sigma_B=0.1$

Let's calculate the KL divergence for $\mathbf{x}_A$:
$$D_{KL}(\mathcal{N}(-5.0,0.1^2)||\mathcal{N}(0,1)) = \frac{1}{2} (\exp(2\log(0.1))+(-5.0)^2-1-2\log(0.1))$$
$$= \frac{1}{2} (0.01+25-1-(-4.6)) = \frac{1}{2} (24.01+4.6)=14.3$$

This KL value (14.3) is very high! The VAE loss function will strongly penalize this. To minimize this KL term, the encoder is forced to:

* Pull the means towards 0: $\mu_A$ and $\mu_B$ must be closer to 0.
* Push the variances towards 1: $\sigma_A$ and $\sigma_B$ must be closer to 1.

So, for similar inputs $\mathbf{x}_A$ and $\mathbf{x}_B$, the trained encoder might output:

* Encoder output for $\mathbf{x}_A$: $q_\phi(\mathbf{z}|\mathbf{x}_A)=\mathcal{N}(\mathbf{z};\mu_A=-0.5,\sigma_A=0.8)$
* Encoder output for $\mathbf{x}_B$: $q_\phi(\mathbf{z}|\mathbf{x}_B)=\mathcal{N}(\mathbf{z};\mu_B=+0.5,\sigma_B=0.8)$

Now, let's evaluate the KL divergence again (for $\mathbf{x}_A$):
$$D_{KL}(\mathcal{N}(-0.5,0.8^2)||\mathcal{N}(0,1)) = \frac{1}{2} (\exp(2\log(0.8))+(-0.5)^2-1-2\log(0.8))$$
$$= \frac{1}{2} (0.64+0.25-1-(-0.446))= \frac{1}{2} (-0.11+0.446)=0.168$$

This KL value (0.168) is much smaller! The KL regularization successfully compressed the latent distributions closer to the prior.


**Why this helps:**

The only way for the encoder to reconstruct accurately AND stay close to the prior when presented with similar inputs is to map them to overlapping distributions.

## How two terms work with each other to ensure a smooth generative model for unseen data

Let's consider two distinct but very similar input data points, $\mathbf{x}_A$ and $\mathbf{x}_B$, both from training distribution

**Pressure from KL Divergence: Make them similar**

* For $\mathbf{x}_A$, the encoder will produce $q_\phi(\mathbf{z}|\mathbf{x}_A)=\mathcal{N}(\mu_A,\Sigma_A)$. The KL term wants $\mu_A \approx 0$ and $\Sigma_A \approx I$.
* For $\mathbf{x}_B$, the encoder will produce $q_\phi(\mathbf{z}|\mathbf{x}_B)=\mathcal{N}(\mu_B,\Sigma_B)$. The KL term wants $\mu_B \approx 0$ and $\Sigma_B \approx I$.

The mathematical consequence of minimizing $D_{KL}(Q||P)$ is that $Q$ is forced to be similar to $P$. Since $P$ is the same prior for all $\mathbf{x}$, this means all $q_\phi(\mathbf{z}|\mathbf{x})$ distributions for any $\mathbf{x}$ are pulled towards the same central region of the latent space (around 0). This inherently forces them to be closer to each other than they might be in a vanilla autoencoder without this regularization.

**Pressure from Reconstruction Loss: Make them distinct**

If the encoder were to map $\mathbf{x}_A$ and $\mathbf{x}_B$ to identical distributions (e.g., $\mu_A=\mu_B=0$ and $\Sigma_A=\Sigma_B=I$), the reconstruction loss would be high to penalize that.

**The Interplay (The "Dual Pressure"):**

1.  The KL term pushes all latent distributions for different $\mathbf{x}$ towards the same central region of the latent space and encourages them to have a certain "spread" (variance $\approx I$). This means they will naturally overlap.
2.  The reconstruction term pulls these distributions slightly apart (or adjusts their means and variances) just enough so that the decoder can distinguish between similar inputs and reconstruct them accurately.

The balance between these two forces is key. The optimal solution is where the encoder maps similar inputs $\mathbf{x}_A$ and $\mathbf{x}_B$ to latent distributions $q_\phi(\mathbf{z}|\mathbf{x}_A)$ and $q_\phi(\mathbf{z}|\mathbf{x}_B)$ that are:

* Close to each other (due to KL regularization towards the common prior).
* Significantly overlapping (due to the variances being pushed towards 1 by KL, and their means being close).
* Slightly distinct in their means/variances such that the decoder can still reconstruct $\mathbf{x}_A$ from samples of $q_\phi(\mathbf{z}|\mathbf{x}_A)$ and $\mathbf{x}_B$ from samples of $q_\phi(\mathbf{z}|\mathbf{x}_B)$ with low reconstruction error.

### The Reparameterization Trick:

A challenge arises because sampling $\mathbf{z}$ from $q_\phi(\mathbf{z}|\mathbf{x})$ is a stochastic operation, which makes backpropagation through the sampling step difficult. The reparameterization trick solves this.
Instead of sampling $\mathbf{z} \sim \mathcal{N}(\mu,\Sigma)$, we sample an auxiliary random variable $\boldsymbol{\epsilon} \sim \mathcal{N}(0,I)$ and then compute:

$$\mathbf{z} = \mu_\phi(\mathbf{x}) + \sigma_\phi(\mathbf{x}) \odot \boldsymbol{\epsilon}$$
(where $\odot$ is element-wise multiplication, and $\sigma_\phi(\mathbf{x})$ is the standard deviation, often computed from log-variance to ensure positivity).
Now, the stochasticity is moved to $\boldsymbol{\epsilon}$, and $\mathbf{z}$ becomes a deterministic function of $\mu$, $\sigma$, and $\boldsymbol{\epsilon}$, allowing gradients to flow back through $\mu_\phi(\mathbf{x})$ and $\sigma_\phi(\mathbf{x})$ to the encoder's parameters $\phi$.



## Key Points of VAE

* **Generative Model:** By sampling a $\mathbf{z}$ from the simple prior $p(\mathbf{z})$ (e.g., standard normal) and passing it through the decoder $p_\theta(\mathbf{x}|\mathbf{z})$, we can generate entirely new data that resembles the training data

* **Variational Inference & Trade-off between Reconstruction and Regularization:** The balance between the reconstruction loss and the KL divergence term is critical. If the KL term is too strong, the model might prioritize learning the prior over accurate reconstruction, leading to blurry reconstructions. If too weak, the latent space might not be well-regularized for generation. This balance is often controlled by a weighting factor (beta-VAE).


# Dreamer

Dreamer is a MBRL model that learns the model simultaneously with behavior.

## Modules Involved:

* **Encoder (Representation Model):** $q_\phi(s_t|h_t,o_t)$ maps observation $o_t$ and recurrent state $h_t$ to posterior distribution parameters ($\mu_{post},\sigma_{post}$) for $s_t$.
* **Decoder (Observation Model):** $p_\theta(o_t|s_t)$ maps stochastic state $s_t$ to observation reconstruction parameters.
* **Reward Model:** $p_\theta(r_t|s_t)$ maps stochastic state $s_t$ to reward prediction parameters.
* **Recurrent Model (Deterministic Dynamics):** $f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})$ updates $h_t$.
* **Prior Model (Transition Dynamics):** $p(s_t|h_t)$ maps recurrent state $h_t$ to prior distribution parameters ($\mu_{prior},\sigma_{prior}$) for $s_t$.
* **Actor (Policy Network):** $p_\psi(a_t|s_t)$ maps stochastic state $s_t$ to action distribution parameters.
* **Critic (Value Network):** $V_\psi(s_t)$ predicts the expected future value from state $s_t$.

## Training Steps (One Iteration):

### Phase 1: Data Collection (Real-World Interaction)

1.  **Observe Current State:** The agent receives the current observation $o_t$ from the real environment.
2.  **Infer Latent State (Encoder):** Using the Encoder and Recurrent Model, infer the current stochastic latent state $s_t$ and update the deterministic hidden state $h_t$:
    * $h_t=f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})$ (using the last actual state and action from the environment)
    * $q(s_t|h_t,o_t)$ is computed by the Encoder. A sample $s_t$ is drawn (using the reparameterization trick) from this posterior distribution.
3.  **Choose Action (Actor):** Feed the inferred latent state $s_t$ to the Actor (Policy Network) $p_\psi(a_t|s_t)$ to get an action distribution. Sample an action $a_t$ from this distribution (e.g., using $\epsilon$-greedy for exploration or pure sampling for stochastic policies).
4.  **Execute Action:** Execute action $a_t$ in the real environment.
5.  **Receive Feedback:** Get the next observation $o_{t+1}$ and reward $r_{t+1}$ from the environment.
6.  **Store Experience:** Store the tuple $(o_t,a_t,r_{t+1},o_{t+1})$ in a replay buffer.

### Phase 2: Model Training (World Model Update)

This phase happens after collecting a batch of new experiences (e.g., a short trajectory or a few steps) or can be done continuously from the replay buffer.

1.  **Sample Batch:** Sample a batch of (e.g., 50-step) sequences from the replay buffer.
2.  **Process Sequence (RSSM Pass):** For each sequence in the batch, process it through the Recurrent State-Space Model (RSSM):
    * For each step $t$ in the sequence:
        * **Update Recurrent State:** $h_t=f_{recurrent}(h_{t-1},s_{t-1},a_{t-1})$
        * **Compute Prior:** $p(s_t|h_t)$ (using the Prior Model)
        * **Compute Posterior:** $q(s_t|h_t,o_t)$ (using the Encoder and the observed $o_t$)
        * **Sample Latent State:** Sample $s_t$ from the posterior $q(s_t|h_t,o_t)$ (using reparameterization trick for gradients).
3.  **Calculate World Model Loss:** The total world model loss is calculated for the entire batch. It typically combines three terms for each step $t$:
    * **Reconstruction Loss:** $Loss_{obs} = -\log p_\theta(o_t|s_t)$ (from Decoder). This encourages $s_t$ to contain enough information to reconstruct the observation.
    * **Reward Loss:** $Loss_{reward} = -\log p_\theta(r_t|s_t)$ (from Reward Model). This encourages $s_t$ to contain enough information to predict the reward.
    * **KL Divergence (Consistency Loss):** $Loss_{KL} = D_{KL}(q(s_t|h_t,o_t)||p(s_t|h_t))$. This is crucial:
        * It regularizes the posterior $q$ towards the prior $p$.
        * It also forces the Prior Model to learn to accurately predict the next stochastic state before seeing the actual observation, by pulling it closer to the posterior that does see the observation. This is key for effective imagination.
    * **Total World Model Loss:** $L_{WM} = \sum_t (Loss_{obs} + Loss_{reward} + \beta \cdot Loss_{KL})$ (where $\beta$ is a KL weight, often 1 or adjusted).
4.  **Optimize World Model:** Perform an optimization step (e.g., Adam) to update the parameters of the Encoder, Decoder, Reward Model, Recurrent Model, and Prior Model based on $L_{WM}$.


### Phase 3: Behavior Learning (Policy & Value Update by Imagination)

This phase also happens concurrently with World Model training, typically after a few world model updates. It leverages the current learned world model.

1.  **Sample Initial States:** Sample a batch of latent states $s_k$ from recent past experiences in the replay buffer. These serve as starting points for imagined trajectories.
2.  **Imagine Trajectories:** For each sampled $s_k$:
    * **Rollout:** Use the learned Prior Model ($p(s_{t+1}|h_{t+1})$) and Recurrent Model ($h_{t+1}=f_{recurrent}(\dots)$) to simulate forward in the latent space for $H$ steps (the imagination horizon).
    * **Act in Imagination:** At each step $t'$ in the imagination, the Actor $p_\psi(a_{t'}|s_{t'})$ chooses an action $a_{t'}$.
    * **Predict Reward:** The Reward Model $p_\theta(r_{t'}|s_{t'})$ predicts the reward for that step.
    * **Predict Value:** The Critic $V_\psi(s_{t'})$ predicts the value of the future state.
    * This generates an imagined trajectory of states ($s_k,s'_{k+1},\dots,s'_{k+H}$) and rewards ($r'_k,\dots,r'_{k+H}$).
3.  **Calculate Policy Loss:**
    * **Value Target:** Calculate value targets (e.g., GAE-lambda returns or N-step returns) using the predicted imagined rewards $r'_{t'}$ and the Critic's predicted values $V_\psi(s'_{t'})$ along the imagined trajectory.
    * **Actor Loss:** Optimize the Actor to maximize the expected value from imagined trajectories. This typically involves maximizing the expected return from actions chosen by the policy. Gradients flow back through the entire imagined sequence and the dynamics of the world model.
    * **Critic Loss:** Optimize the Critic to accurately predict the value of imagined states, typically using an MSE loss between its prediction and the calculated value targets.
    * **Total Policy Loss:** $L_{Policy} = \text{Actor Loss} + \text{Critic Loss}$
4.  **Optimize Policy & Value Networks:** Perform an optimization step (e.g., Adam) to update the parameters of the Actor and Critic Networks based on $L_{Policy}$.

#### 1. Parameters We Are Trying to Learn (The $\psi$ Parameters)

In the behavior learning phase, we are trying to learn the parameters of two distinct neural networks:

* **Actor Network (Policy Network):** $p_\psi(a_t|s_t)$
    * **Parameters:** These are the weights and biases of the neural network that takes a latent state $s_t$ as input and outputs a distribution over actions $a_t$. Let's denote these specific parameters as $\psi_{actor}$.
    * **Purpose:** To learn a policy that chooses actions which maximize expected future rewards within the imagined world.

* **Critic Network (Value Network):** $V_\psi(s_t)$
    * **Parameters:** These are the weights and biases of the neural network that takes a latent state $s_t$ as input and outputs a single scalar value, representing the estimated expected future return (value) from that state. Let's denote these specific parameters as $\psi_{critic}$.
    * **Purpose:** To learn to accurately predict the "goodness" (value) of a given latent state. This value prediction is then used as a baseline and target for training the Actor.

So, when we talk about optimizing $\psi$, we are jointly optimizing $\psi_{actor}$ and $\psi_{critic}$.

#### 2. Loss Functions Summed Up (Mathematical Formulation)

The total policy loss, $L_{Policy}(\psi)$, is composed of two main parts, each designed to update its respective network:

$$L_{Policy}(\psi) = \underbrace{L_{Critic}(\psi_{critic})}_{\text{Value Estimation Loss}} + \underbrace{L_{Actor}(\psi_{actor})}_{\text{Policy Improvement Loss}}$$

Let's detail each:

##### Critic Loss: $L_{Critic}(\psi_{critic})$

**Goal:** Make the Critic's value predictions $V_{\psi_{critic}}(s''_{t'})$ as accurate as possible for the imagined states $s''_{t'}$. "Accurate" here means matching the calculated value targets (or $\lambda$-returns) $V_{target}(s''_{t'})$.

**Formulation:** It's a standard Mean Squared Error (MSE) loss.

$$L_{Critic}(\psi_{critic}) = \frac{1}{M} \sum_{m=1}^{M} (V_{\psi_{critic}}(s_m'') - V_{target}(s_m''))^2$$

Where:

* $M$ is the total number of (state, target) pairs from all imagined trajectories in the current batch.
* $s_m''$ is an imagined latent state at some time step $t'$ from an imagined trajectory.
* $V_{\psi_{critic}}(s_m'')$: The Critic's Prediction (The Thing We Want to Improve)

  * **What it is:** This is the current output of your Critic neural network. It's the network's best guess or estimate of the expected sum of future discounted rewards that the agent will receive if it starts from the imagined latent state $s_m''$ and then follows its current policy $p_{\psi_{actor}}$ thereafter.
  * **Source:** It comes directly from the forward pass of the Critic network ($V_{\psi_{critic}}$) with $s_m''$ as input.
  * **Purpose in Loss:** This is the value that we are trying to adjust during training. The Critic loss will compute how "wrong" this prediction is compared to the target, and then use that error to update the Critic's parameters $\psi_{critic}$ via backpropagation. We want to make this prediction get closer to the target value.
* $V_{target}(s_m'')$ is the computed target value for $s_m''$ (Ground Truth). This target is crucial and calculated using the imagined rewards $r''_{t'+1}$ and the bootstrapped value predictions from the Critic itself, specifically the $\lambda$-return (or GAE-based target):
    $$ V_{target}(s_{t'}' ) = \sum_{j=0}^{H-t'-1} (\gamma\lambda)^j r_{t'+j+1}' + (\gamma\lambda)^{H-t'} V_{\psi_{critic}}(s_{k+H}') \quad \text{(simplified } \lambda\text{-return)}$$
    More robustly, using GAE:
    $$ V_{target}(s_{t'}' ) = V_{\psi_{critic}}(s_{t'}' ) + A_{t'}^{\text{GAE}(\gamma, \lambda)}$$
    where $A_{t'}^{\text{GAE}(\gamma,\lambda)} = \sum_{j=0}^{H-t'-1} (\gamma\lambda)^j \delta''_{t'+j}$ and $\delta''_{t'} = r''_{t'+1} + \gamma V_{\psi_{critic}}(s''_{t'+1}) - V_{\psi_{critic}}(s''_{t'})$.

##### Actor Loss: $L_{Actor}(\psi_{actor})$

**Goal:** Adjust the Actor's policy parameters $\psi_{actor}$ so that actions that lead to higher rewards (higher advantages) become more probable. Also, encourage exploration (entropy).

**Formulation:** This is a policy gradient loss, typically based on the REINFORCE algorithm but with the addition of a value baseline (the Critic's prediction) and entropy regularization.

$$L_{Actor}(\psi_{actor}) = -\frac{1}{M} \sum_{m=1}^{M} (A_m^{\text{GAE}(\gamma,\lambda)} \cdot \log p_{\psi_{actor}}(a_m''|s_m'') + \beta_H \cdot H(p_{\psi_{actor}}(a_m''|s_m'')))$$

Where:

* $M$ is the total number of (state, action, advantage) triples from all imagined trajectories.
* $A_m^{\text{GAE}(\gamma,\lambda)}$ is the calculated Generalized Advantage Estimate for the imagined state $s_m''$ and action $a_m''$. This term serves as the "credit assignment" for the action.
* $\log p_{\psi_{actor}}(a_m''|s_m'')$ is the log-probability of the imagined action $a_m''$ under the current policy $p_{\psi_{actor}}$. This is the standard policy gradient term.
* $H(p_{\psi_{actor}}(a_m''|s_m''))$ is the entropy of the action distribution at state $s_m''$.
* $\beta_H$ is a hyperparameter for entropy regularization. A positive $\beta_H$ means we add a negative entropy term to the loss, which encourages maximizing entropy (more exploration).

I will explain more down here

###### 1. The Thing We Want to Improve: $\log p_{\psi_{actor}}(a_m''|s_m'')$ (The Policy Itself)

* **What it is:** This term represents the log-probability of the action $a_m''$ that was actually chosen by the Actor (policy) network for the imagined latent state $s_m''$. The Actor network $p_{\psi_{actor}}(a|s)$ outputs an action distribution, and this is the log-likelihood of a specific action under that distribution.
* **Source:** It comes directly from a forward pass of the Actor network $p_{\psi_{actor}}$ given input $s_m''$, and then evaluating the log-probability of $a_m''$.
* **Purpose in Loss:** This is the core part of the policy that we are trying to adjust. By optimizing the Actor loss, we are trying to change the parameters $\psi_{actor}$ such that the probability of "good" actions increases, and the probability of "bad" actions decreases.

###### 2. The "Ground Truth" (The Guidance Signal): $A_m^{\text{GAE}(\gamma,\lambda)}$ (The Advantage)

* **What it is:** This is the Generalized Advantage Estimate (GAE) for the imagined state-action pair ($s_m'',a_m''$). The advantage function tells us how much better (or worse) a specific action $a_m''$ taken in state $s_m''$ was compared to the average expected value of that state $V_{\psi_{critic}}(s_m'')$.
    * If $A_m^{\text{GAE}} > 0$: The action $a_m''$ led to better-than-expected rewards.
    * If $A_m^{\text{GAE}} < 0$: The action $a_m''$ led to worse-than-expected rewards.
* **Source:** This is computed based on the imagined rewards $r''_{t'}$ (from the Reward Model) and the Critic's value predictions $V_{\psi_{critic}}(s''_{t'})$ (from the Critic Network).
* **Purpose in Loss:** This is the signal that tells the Actor whether the action it just took (in imagination) was good or bad. It serves as the "ground truth" in the sense that it's the target direction and magnitude for policy improvement. We want to increase the likelihood of actions associated with positive advantages and decrease the likelihood of actions associated with negative advantages.

###### 3. The Entropy Term: $\beta_H \cdot H(p_{\psi_{actor}}(a_m''|s_m''))$

* **What it is:** This is the entropy of the action distribution output by the Actor for state $s_m''$. It measures the "randomness" or "predictability" of the policy. High entropy means the policy is more exploratory (less confident in a single action), low entropy means it's more deterministic.
* **Source:** It's calculated directly from the action distribution output by the Actor network.
* **Purpose in Loss:** This is a regularization term. It's not about "improving a prediction" in the same way as the other terms. Instead, we typically want to maximize entropy (hence the negative sign in the loss formulation) to encourage exploration and prevent the policy from collapsing to a single action too quickly. This helps the agent continue to discover better strategies.

#### How the Loss Helps Learn $\psi_{actor}$

The Actor loss, when minimized, works as follows:

* **Policy Gradient:** The core term $A_m^{\text{GAE}(\gamma,\lambda)} \cdot \log p_{\psi_{actor}}(a_m''|s_m'')$ is the standard policy gradient component. When we minimize the negative of this term:
    * If $A_m^{\text{GAE}} > 0$ (good action): We want to increase $\log p_{\psi_{actor}}(a_m''|s_m'')$, meaning we increase the probability of taking action $a_m''$ in state $s_m''$.
    * If $A_m^{\text{GAE}} < 0$ (bad action): We want to decrease $\log p_{\psi_{actor}}(a_m''|s_m'')$, meaning we decrease the probability of taking action $a_m''$ in state $s_m''$.
    * The magnitude of the advantage dictates the strength of this probability adjustment.
* **Entropy Regularization:** The $-\beta_H \cdot H(\dots)$ term ensures that even while the policy is being pushed towards high-advantage actions, it doesn't become overly deterministic. It retains some level of randomness, which is beneficial for continued exploration.