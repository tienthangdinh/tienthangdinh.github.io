---
title: "From Autoencoder to Dreamer"
date: 2025-07-12
categories: [Machine Learning, Image Processing, Model-based Learning, Reinforcement Learning]
format:
  html:
    toc: true
    code-fold: true
    math: mathjax
---
# Variational Autoencoders (VAEs)

**Key Idea:** Instead of mapping an input to a single point in the latent space, VAE maps it to parameters of a probability distribution (typically a Gaussian distribution) in the latent space:

* An encoder that maps data to a distribution in latent space.
* A decoder that maps samples from this latent distribution back to data space.


## Mathematical Formulation of VAE

Let's denote:

* $\mathbf{x}$: An input data point (e.g., an image)
* $\mathbf{z}$: A latent variable (vector) in the lower-dimensional latent space
* $p(\mathbf{x})$: The true, unknown data distribution we want to model
* $p(\mathbf{z})$: The prior distribution over the latent variables (typically a simple distribution like $\mathcal{N}(0,I)$)
* $p_\theta(\mathbf{x}|\mathbf{z})$: The decoder (also called generative model or likelihood). This is a neural network parameterized by $\theta$ that outputs the parameters of the distribution over $\mathbf{x}$ given $\mathbf{z}$.
* $q_\phi(\mathbf{z}|\mathbf{x})$: The encoder (also called inference model or approximate posterior). This is a neural network parameterized by $\phi$ that outputs the parameters of the distribution over $\mathbf{z}$ given $\mathbf{x}$.

**Goal:** is to maximize the marginal likelihood:
$$p(\mathbf{x}) = \frac{p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})}$$ which we do not know the other element to compute, or
$$\log p_\theta(\mathbf{x}) = \log \int p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}$$ which is intractable

Therefore, VAEs optimize the Evidence Lower Bound (ELBO), which is a lower bound on the log-likelihood of the data, we will derive it now

### Derivation of ELBO:

The log-likelihood of a data point $\mathbf{x}$ can be written as:

$$\log p_\theta(\mathbf{x}) = \log p_\theta(\mathbf{x})$$
$$= \log p_\theta(\mathbf{x}) \int q_\phi(\mathbf{z}|\mathbf{x}) d\mathbf{z} \quad \text{Multiply by 1}$$
$$= \int q_\phi(\mathbf{z}|\mathbf{x}) \log p_\theta(\mathbf{x}) d\mathbf{z} \quad \text{Bring inside the integral}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x})] \quad \text{Definition of expectation}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{p_\theta(\mathbf{z}|\mathbf{x})} \right] \quad \text{Apply the equation } p_\theta(\mathbf{x}) = \frac{p_\theta(\mathbf{x}, \mathbf{z})}{p_\theta(\mathbf{z}|\mathbf{x})}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z}) q_\phi(\mathbf{z}|\mathbf{x})}{p_\theta(\mathbf{z}|\mathbf{x}) q_\phi(\mathbf{z}|\mathbf{x})} \right] \quad \text{Multiply by 1}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} + \log \frac{q_\phi(\mathbf{z}|\mathbf{x})}{p_\theta(\mathbf{z}|\mathbf{x})} \right] \quad \text{Split the expectation}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] + D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p_\theta(\mathbf{z}|\mathbf{x})) \quad \text{KL divergence}$$

The second term is the Kullback-Leibler (KL) divergence, it is non-negative, now we only focus on maximizing the first term ELBO:

$$\log p_\theta(\mathbf{x}) \ge \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] \quad \text{Chain rule of probability}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] + \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p(\mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] \quad \text{Split the expectation}$$
$$= \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z})) \quad \text{KL divergence}$$

**This is the objective function for the VAE (Loss Function), also the ELBO** to maximize, or minimizing the negative ELBO.

$$\mathcal{L}(\theta, \phi, \mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))$$


### First Component: Reconstruction Loss (Negative Expected Log-Likelihood): $\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})]$

This term encourages the decoder to reconstruct the input $\mathbf{x}$ accurately from a latent sample $\mathbf{z}$ drawn from the encoder's output distribution.

We will derive it now

#### Assumption: Data is Gaussian Distributed

We assume that each data point $\mathbf{x}$ (e.g., an image, where each pixel is a dimension) is drawn from a Gaussian distribution, given the latent variable $\mathbf{z}$ that the decoder outputs. For simplicity, let's assume the dimensions of $\mathbf{x}$ are independent given $\mathbf{z}$, and they share a fixed variance $\sigma^2$.

So, for each dimension $j$ of $\mathbf{x}$, $x_j$ is distributed as:
$p_\theta(x_j|\mathbf{z}) = \mathcal{N}(x_j; \mu_j(\mathbf{z}), \sigma^2)$

Here:

* $\mu_j(\mathbf{z})$ is the mean for the $j$-th dimension, which is the output of your decoder network for that dimension when given $\mathbf{z}$.
* $\sigma^2$ is the variance. For simplicity, we often assume a fixed $\sigma^2$ (e.g., $\sigma^2=1$, or a small constant, or even absorbed into the weights). More advanced VAEs can learn this variance as well (the decoder outputs both mean and variance).

#### Probability Density Function (PDF) of a Gaussian

The PDF for a single-dimensional Gaussian variable $x_j$ is:

$f(x_j) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2}\right)$

#### Log-Likelihood for a Multi-dimensional Data Point $\mathbf{x}$

Since we assume the dimensions of $\mathbf{x}$ are independent given $\mathbf{z}$, the joint probability $p_\theta(\mathbf{x}|\mathbf{z})$ is the product of the individual probabilities:

$p_\theta(\mathbf{x}|\mathbf{z}) = \prod_{j=1}^{D_x} p_\theta(x_j|\mathbf{z})$

where $D_x$ is the dimensionality of $\mathbf{x}$ (e.g., number of pixels in an image).
Now, let's take the logarithm of this product:

$$\log p_\theta(\mathbf{x}|\mathbf{z}) = \log \left( \prod_{j=1}^{D_x} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2}\right) \right)$$
$$= \sum_{j=1}^{D_x} \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2}\right) \right)$$
$$= \sum_{j=1}^{D_x} \left( \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \log\left(\exp\left(-\frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2}\right)\right) \right)$$
$$= \sum_{j=1}^{D_x} \left( -\frac{1}{2}\log(2\pi\sigma^2) - \frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2} \right)$$
$$= \sum_{j=1}^{D_x} \left( -\frac{1}{2}\log(2\pi\sigma^2) \right) - \sum_{j=1}^{D_x} \left( \frac{(x_j - \mu_j(\mathbf{z}))^2}{2\sigma^2} \right)$$The first term is a constant multiplied by $D_x$:
$$= -\frac{D_x}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{j=1}^{D_x} (x_j - \mu_j(\mathbf{z}))^2$$
$$\text{MSE} = \frac{1}{D_x} \sum_{j=1}^{D_x} (x_j - \mu_j(\mathbf{z}))^2$$


### Second Term: KL Divergence (Regularization Term): $D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))$

This term acts as a regularizer. It pushes the approximate posterior $q_\phi(\mathbf{z}|\mathbf{x})$ (the distribution output by the encoder for a given $\mathbf{x}$) to be close to the prior distribution $p(\mathbf{z})$ (e.g., $\mathcal{N}(0,I)$).

If $q_\phi(\mathbf{z}|\mathbf{x})$ is assumed to be a diagonal Gaussian $\mathcal{N}(\mu, \Sigma)$ (where $\Sigma$ is diagonal) and $p(\mathbf{z})$ is $\mathcal{N}(0,I)$, the KL divergence has a closed-form solution:

$$D_{KL}(\mathcal{N}(\mu,\Sigma)||\mathcal{N}(0,I)) = \frac{1}{2} \sum_{j=1}^{D_z} (\exp(\sigma_j) + \mu_j^2 - 1 - \sigma_j)$$
where $D_z$ is the dimensionality of $\mathbf{z}$, and $\mu_j$ and $\sigma_j$ are the mean and log-variance (diagonal elements of $\Sigma$) for the $j$-th latent dimension, as output by the encoder.

#### Numerical Example for $q_\phi(\mathbf{z}|\mathbf{x})$

Let's say our latent space $\mathbf{z}$ is 2-dimensional ($D_z=2$).
Our input is a specific image $\mathbf{x}_{cat}$ (an image of a cat).

The encoder network takes $\mathbf{x}_{cat}$ as input. Its output layer (after processing through several hidden layers) has two sets of $D_z=2$ nodes:

* **Mean Output Nodes:** For $\mu_\phi(\mathbf{x}_{cat})$
* **Log-Variance Output Nodes:** For $\log(\sigma_\phi(\mathbf{x}_{cat})^2)$ (we use log-variance for numerical stability, as variance must be positive).

Suppose for this specific $\mathbf{x}_{cat}$, the encoder outputs:

$\mu_\phi(\mathbf{x}_{cat})=\begin{bmatrix} 0.8 \\ -1.2 \end{bmatrix}$

$\log(\sigma_\phi(\mathbf{x}_{cat})^2)=\begin{bmatrix} -0.2 \\ 0.1 \end{bmatrix}$

From the log-variances, we calculate the variances:

$\sigma_1^2 = \exp(-0.2) \approx 0.8187$
$\sigma_2^2 = \exp(0.1) \approx 1.1052$

So, for this input $\mathbf{x}_{cat}$, the encoder defines the latent distribution:
$$q_\phi(\mathbf{z}|\mathbf{x}_{cat})=\mathcal{N}\left(\mathbf{z};\begin{bmatrix} 0.8 \\ -1.2 \end{bmatrix},\begin{bmatrix} 0.8187 & 0 \\ 0 & 1.1052 \end{bmatrix}\right)$$

This means:

* The first latent dimension ($z_1$) is modeled by a Gaussian with mean 0.8 and variance 0.8187.
* The second latent dimension ($z_2$) is modeled by a Gaussian with mean $-1.2$ and variance 1.1052.

These two dimensions are assumed to be independent (because the off-diagonal elements of the covariance matrix are zero).

When we "sample $\mathbf{z}$ from $q_\phi(\mathbf{z}|\mathbf{x}_{cat})$" using the reparameterization trick, we would:

1.  Sample $\epsilon_1 \sim \mathcal{N}(0,1)$ and $\epsilon_2 \sim \mathcal{N}(0,1)$.
2.  Calculate $z_1 = 0.8 + \sqrt{0.8187} \cdot \epsilon_1$
3.  Calculate $z_2 = -1.2 + \sqrt{1.1052} \cdot \epsilon_2$

The resulting $\mathbf{z}=\begin{bmatrix} z_1 \\ z_2 \end{bmatrix}$ is then passed to the decoder.

This formulation allows the VAE to learn a distribution for each input, rather than a single point, enabling the desired properties of a continuous and meaningful latent space.


#### Numerical Example Regularization using KL

Imagine a very simplified VAE where our latent space $\mathbf{z}$ is just one-dimensional ($D_z=1$). Our prior $p(\mathbf{z})$ is $\mathcal{N}(0,1)$ (mean 0, variance 1). The VAE's job is to learn an encoder ($q_\phi(\mathbf{z}|\mathbf{x})$) and a decoder ($p_\theta(\mathbf{x}|\mathbf{z})$) such that:

* The decoder can reconstruct $\mathbf{x}_A$ from samples of $q_\phi(\mathbf{z}|\mathbf{x}_A)$.
* The decoder can reconstruct $\mathbf{x}_B$ from samples of $q_\phi(\mathbf{z}|\mathbf{x}_B)$.
* The KL divergence $D_{KL}(q_\phi(\mathbf{z}|\mathbf{x})||p(\mathbf{z}))$ is minimized for both $\mathbf{x}_A$ and $\mathbf{x}_B$.

**Scenario 1: No KL Regularization (like a vanilla Autoencoder)**

If there were no KL term, the encoder might learn to map $\mathbf{x}_A$ to a specific point $\mathbf{z}_A$ and $\mathbf{x}_B$ to a specific point $\mathbf{z}_B$.

* Encoder output for $\mathbf{x}_A$: $\mu_A=-5.0, \sigma_A=0.001$ (a very tight distribution at -5.0)
* Encoder output for $\mathbf{x}_B$: $\mu_B=+5.0, \sigma_B=0.001$ (a very tight distribution at +5.0)

**Scenario 2: With KL Regularization (VAE)**

Now, the KL term $D_{KL}(q_\phi(\mathbf{z}|\mathbf{x})||\mathcal{N}(0,1))$ is active.

Let's say the encoder tries to map $\mathbf{x}_A$ and $\mathbf{x}_B$ far apart again:

* Encoder output for $\mathbf{x}_A$: $\mu_A=-5.0, \sigma_A=0.1$
* Encoder output for $\mathbf{x}_B$: $\mu_B=+5.0, \sigma_B=0.1$

Let's calculate the KL divergence for $\mathbf{x}_A$:
$$D_{KL}(\mathcal{N}(-5.0,0.1^2)||\mathcal{N}(0,1)) = \frac{1}{2} (\exp(2\log(0.1))+(-5.0)^2-1-2\log(0.1))$$
$$= \frac{1}{2} (0.01+25-1-(-4.6)) = \frac{1}{2} (24.01+4.6)=14.3$$

This KL value (14.3) is very high! The VAE loss function will strongly penalize this. To minimize this KL term, the encoder is forced to:

* Pull the means towards 0: $\mu_A$ and $\mu_B$ must be closer to 0.
* Push the variances towards 1: $\sigma_A$ and $\sigma_B$ must be closer to 1.

So, for similar inputs $\mathbf{x}_A$ and $\mathbf{x}_B$, the trained encoder might output:

* Encoder output for $\mathbf{x}_A$: $q_\phi(\mathbf{z}|\mathbf{x}_A)=\mathcal{N}(\mathbf{z};\mu_A=-0.5,\sigma_A=0.8)$
* Encoder output for $\mathbf{x}_B$: $q_\phi(\mathbf{z}|\mathbf{x}_B)=\mathcal{N}(\mathbf{z};\mu_B=+0.5,\sigma_B=0.8)$

Now, let's evaluate the KL divergence again (for $\mathbf{x}_A$):
$$D_{KL}(\mathcal{N}(-0.5,0.8^2)||\mathcal{N}(0,1)) = \frac{1}{2} (\exp(2\log(0.8))+(-0.5)^2-1-2\log(0.8))$$
$$= \frac{1}{2} (0.64+0.25-1-(-0.446))= \frac{1}{2} (-0.11+0.446)=0.168$$

This KL value (0.168) is much smaller! The KL regularization successfully compressed the latent distributions closer to the prior.


**Why this helps:**

The only way for the encoder to reconstruct accurately AND stay close to the prior when presented with similar inputs is to map them to overlapping distributions.

## How two terms work with each other to ensure a smooth generative model for unseen data

Let's consider two distinct but very similar input data points, $\mathbf{x}_A$ and $\mathbf{x}_B$, both from training distribution

**Pressure from KL Divergence: Make them similar**

* For $\mathbf{x}_A$, the encoder will produce $q_\phi(\mathbf{z}|\mathbf{x}_A)=\mathcal{N}(\mu_A,\Sigma_A)$. The KL term wants $\mu_A \approx 0$ and $\Sigma_A \approx I$.
* For $\mathbf{x}_B$, the encoder will produce $q_\phi(\mathbf{z}|\mathbf{x}_B)=\mathcal{N}(\mu_B,\Sigma_B)$. The KL term wants $\mu_B \approx 0$ and $\Sigma_B \approx I$.

The mathematical consequence of minimizing $D_{KL}(Q||P)$ is that $Q$ is forced to be similar to $P$. Since $P$ is the same prior for all $\mathbf{x}$, this means all $q_\phi(\mathbf{z}|\mathbf{x})$ distributions for any $\mathbf{x}$ are pulled towards the same central region of the latent space (around 0). This inherently forces them to be closer to each other than they might be in a vanilla autoencoder without this regularization.

**Pressure from Reconstruction Loss: Make them distinct**

If the encoder were to map $\mathbf{x}_A$ and $\mathbf{x}_B$ to identical distributions (e.g., $\mu_A=\mu_B=0$ and $\Sigma_A=\Sigma_B=I$), the reconstruction loss would be high to penalize that.

**The Interplay (The "Dual Pressure"):**

1.  The KL term pushes all latent distributions for different $\mathbf{x}$ towards the same central region of the latent space and encourages them to have a certain "spread" (variance $\approx I$). This means they will naturally overlap.
2.  The reconstruction term pulls these distributions slightly apart (or adjusts their means and variances) just enough so that the decoder can distinguish between similar inputs and reconstruct them accurately.

The balance between these two forces is key. The optimal solution is where the encoder maps similar inputs $\mathbf{x}_A$ and $\mathbf{x}_B$ to latent distributions $q_\phi(\mathbf{z}|\mathbf{x}_A)$ and $q_\phi(\mathbf{z}|\mathbf{x}_B)$ that are:

* Close to each other (due to KL regularization towards the common prior).
* Significantly overlapping (due to the variances being pushed towards 1 by KL, and their means being close).
* Slightly distinct in their means/variances such that the decoder can still reconstruct $\mathbf{x}_A$ from samples of $q_\phi(\mathbf{z}|\mathbf{x}_A)$ and $\mathbf{x}_B$ from samples of $q_\phi(\mathbf{z}|\mathbf{x}_B)$ with low reconstruction error.

### The Reparameterization Trick:

A challenge arises because sampling $\mathbf{z}$ from $q_\phi(\mathbf{z}|\mathbf{x})$ is a stochastic operation, which makes backpropagation through the sampling step difficult. The reparameterization trick solves this.
Instead of sampling $\mathbf{z} \sim \mathcal{N}(\mu,\Sigma)$, we sample an auxiliary random variable $\boldsymbol{\epsilon} \sim \mathcal{N}(0,I)$ and then compute:

$$\mathbf{z} = \mu_\phi(\mathbf{x}) + \sigma_\phi(\mathbf{x}) \odot \boldsymbol{\epsilon}$$
(where $\odot$ is element-wise multiplication, and $\sigma_\phi(\mathbf{x})$ is the standard deviation, often computed from log-variance to ensure positivity).
Now, the stochasticity is moved to $\boldsymbol{\epsilon}$, and $\mathbf{z}$ becomes a deterministic function of $\mu$, $\sigma$, and $\boldsymbol{\epsilon}$, allowing gradients to flow back through $\mu_\phi(\mathbf{x})$ and $\sigma_\phi(\mathbf{x})$ to the encoder's parameters $\phi$.



## Key Points of VAE

* **Generative Model:** By sampling a $\mathbf{z}$ from the simple prior $p(\mathbf{z})$ (e.g., standard normal) and passing it through the decoder $p_\theta(\mathbf{x}|\mathbf{z})$, we can generate entirely new data that resembles the training data

* **Variational Inference & Trade-off between Reconstruction and Regularization:** The balance between the reconstruction loss and the KL divergence term is critical. If the KL term is too strong, the model might prioritize learning the prior over accurate reconstruction, leading to blurry reconstructions. If too weak, the latent space might not be well-regularized for generation. This balance is often controlled by a weighting factor (beta-VAE).