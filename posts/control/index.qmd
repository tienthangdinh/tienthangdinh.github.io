---
title: "From Control to Model-based Learning"
date: 2025-07-07
categories: [Control Theory, Optimization, Dynamics, Model-based Learning, Reinforcement Learning]
format:
  html:
    toc: true
    code-fold: true
    math: mathjax
---
# PID Controller and its Problem

## Mathematical Formulation

A PID controller is a feedback control loop that continuously calculates an "error" value $e(t)$ as the difference between a desired setpoint $r(t)$ and a measured process variable $y(t)$:

$$e(t) = r(t) - y(t)$$

Based on this error, the PID controller generates a control output $u(t)$ by combining three distinct terms:

* **Proportional Term ($P$-term):** Accounts for the *current* error.
* **Integral Term ($I$-term):** Accounts for the *accumulation* of past errors.
* **Derivative Term ($D$-term):** Accounts for the *rate of change* of the error.

Combining these, the **continuous-time PID control law** is given by:

$$u(t) = K_p e(t) + K_i \int e(t) dt + K_d \frac{de(t)}{dt}$$

Where:

* $u(t)$ is the controller's output.
* $e(t)$ is the error at time $t$.
* $K_p$ is the proportional gain.
* $K_i$ is the integral gain.
* $K_d$ is the derivative gain.


## But Why P, I, and D? Why not just use the current error?


### The Proportional (P) Term: Present
$$u_P(t) = K_p e(t)$$

* **Present:** If the error is large, the controller acts strongly; if the error is small, it acts weakly. => quickly drive the system towards the setpoint.

* **Gradual Loss Problem**: There are some system where the output just naturally decays over time (like heat loss from a room, or friction in a motor). A simplified model could be, notice that for the output $y$ to be maintained at a constant setpoint $r$ (i.e., $\frac{dy}{dt}=0$), the control input $u$ must provide a continuous effort to compensate: $u_{required} = \frac{ay}{b}$. **This system is the core problem**:

    $$\frac{dy}{dt} = -ay + bu$$


    So what we require is that for this type of model to be at steady state, $\frac{dy}{dt}=0 \implies ay_{ss} = bu_{ss}$.

    **But the problem is here!!!** With P-control, we can only have $u_{ss} = K_p (r - y_{ss})$ that kinda only acts based on the last timestep error.
    $$ay_{ss} = bK_p (r - y_{ss})$$
    $$ay_{ss} = bK_pr - bK_py_{ss}$$
    $$y_{ss} = \frac{K_p r}{(K_p + a/b)}$$

    Since $a,b,K_p$ are positive, $y_{ss}$ will always be less than $r$, meaning there will always be a **non-zero steady-state error**: $e_{ss} = r - y_{ss} \ne 0$. 
    
    **Therefore** In this time of time-decaying systems, a sole P-Term simply cannot provide a sustained, non-zero output, simply because it **only acts with the current error, and never act for the upcoming decay**

### The Integral (I) Term: Gradual Push Effort

$$u_I(t) = K_i \int e(t) dt$$

* **Compensate the gradual Loss:** Probably now you know what to do... we **push a little more**, in such **time-decaying system, we need continuous effort** to to maintain the setpoint. It does this by continuously accumulating errors over time.
* **Analogy:** You're driving at 99 km/h when the limit is 100 km/h (small error). The P-term might give only a tiny gas pedal press. But you know, car on the street is exactly this type of **time-decaying system** ($\frac{dy}{dt} = -ay + bu$) => To maintain 99 km/h for a long time, the I-term "notices" this persistent deficit and *gradually* pushes the gas pedal a little harder and holds it there until you finally reach 100 km/h.
* **Drawback:** The integral term can make the system slower to respond and potentially cause overshoot or oscillations if its gain $K_i$ is set too high, because it's reacting to *past* errors, not current or future ones.
* **When does this accumulated stop?** I would say almost never, because we have a time-decaying system, so we always need it.
* **BUT**, of course sometimes we want do stop overshooting it, therefore we have another term down here...

### The Derivative (D) Term: Anticipation and Damping
$$u_D(t) = K_d \frac{de(t)}{dt}$$

* **Anticipation and Damping
:**This is really nice.
    * If the error is **rapidly increasing** (either negative or positive quantitatively), the D-term will counteract it quickly.
    * If the error is **rapidly decreasing** (meaning the system is approaching the setpoint quickly), the D-term will reduce the control action to prevent overshoot.
* **Analogy:** You see a sharp turn (error changing rapidly) approaching in your car. You start braking *before* the turn to slow down smoothly and avoid overshooting the curve. Or, you're speeding towards the 100 km/h limit; as you get closer, the D-term will gradually ease off the gas, preventing you from overshooting.
* **Benefits:** Reduces overshoot, reduces oscillations, and improves the transient response (how quickly and smoothly the system reaches the setpoint).
* **Drawback:** The D-term is very sensitive to noise in the measurement signal. Rapid changes in noisy signals can lead to large, jerky control actions.




# The LTI System Dynamics: The State-Space Model

Specifically in **Linear Time-Invariant (LTI) systems**, the differential equation $\dot{\mathbf{x}}(t)=\mathbf{A}\mathbf{x}(t)+\mathbf{B}\mathbf{u}(t)$ is known as the **state-space representation** of a dynamic system. $\mathbf{x}(t)$ contains the **state variables** of the system.

**It can always be First-Order Form like this!!!:** Even if a physical system is described by a single high-order differential equation (e.g., a second-order equation for a mass-spring-damper) as example below, it can *always* be converted into a set of coupled **first-order differential equations**, will be described in the below example. Each row of the state-space equation represents the time derivative of one of other state variables.

**What about nonlinear or time-varying systems?**
For these systems, we can **linearize it** by approximating around a specific operating point. This linear approximation is valid only close to that operating point. THen apply LQR and MPC as normal.

## Example LTI System: The Mass-Spring-Damper

**System Description:**
This is a common **second-order system**. Consider a mass $m$ (kg) connected to a spring with stiffness $k$ (N/m) and a damper with damping coefficient $b$ (Ns/m). An external force $F(t)$ (N) is applied to the mass, causing a displacement $y(t)$ (m) from its equilibrium position.

**1. Governing Differential Equation:** Applying Newton's Second Law ($\sum F = m \ddot{y}$) to the mass:

* Applied force: $+F(t)$
* Spring force (restoring): $-k y(t)$
* Damping force (opposing velocity): $-b \dot{y}(t)$

So, we have now a **second-order** linear ordinary differential equation:
$$m\ddot{y}(t) + b\dot{y}(t) + k y(t) = F(t)$$
$$m\ddot{y}(t) = F(t) - b\dot{y}(t) - k y(t)$$



**2. Converting to State-Space Form:**
To convert this second-order equation into the **first-order** state-space form, we define state variables. A common choice is to pick the position and velocity as states:

* $x_1(t) = y(t)$ (the position of the mass)
* $x_2(t) = \dot{y}(t)$ (the velocity of the mass)

Now, we need to express the derivatives of these state variables in terms of the states themselves and the input $F(t)$.
$$\dot{x_1}(t) = \dot{y}(t) = x_2(t)$$
$$\ddot{y}(t) = \frac{1}{m} F(t) - \frac{b}{m}\dot{y}(t) - \frac{k}{m}y(t)$$
Substituting our state variables ($y(t)=x_1(t)$ and $\dot{y}(t)=x_2(t)$) and our input $u(t) = F(t)$:
$$\dot{x_2}(t) = -\frac{k}{m}x_1(t) - \frac{b}{m}x_2(t) + \frac{1}{m}u(t)$$

**3. Writing in Matrix Form:**
Now, we can assemble these first-order equations into the state-space matrix form $\dot{\mathbf{x}}(t)=\mathbf{A}\mathbf{x}(t)+\mathbf{B}\mathbf{u}(t)$:

$$\begin{pmatrix} \dot{x_1}(t) \\ \dot{x_2}(t) \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ -\frac{k}{m} & -\frac{b}{m} \end{pmatrix} \begin{pmatrix} x_1(t) \\ x_2(t) \end{pmatrix} + \begin{pmatrix} 0 \\ \frac{1}{m} \end{pmatrix} u(t)$$

From this, we can identify the  statematrix $\mathbf{A}$ and input matrix $\mathbf{B}$:
$$\mathbf{A} = \begin{pmatrix} 0 & 1 \\ -\frac{k}{m} & -\frac{b}{m} \end{pmatrix}, \mathbf{B} = \begin{pmatrix} 0 \\ \frac{1}{m} \end{pmatrix}$$

# The Linear Quadratic Regulator (LQR) - Model-hint to Optimal Control

PID relies on heuristic tuning and does not handle multi-variable systems. This is where the **Linear Quadratic Regulator (LQR)** steps in. It operates on any system platform that follow this **linear time-invariant (LTI) state-space model**:
$$\dot{\mathbf{x}}(t) = \mathbf{A}\mathbf{x}(t) + \mathbf{B}\mathbf{u}(t)$$

This model is a linear approximation of a real system, assumed to be valid around an operating point (e.g., linearization around an equilibrium).

## Formulate into integral of terms using x and u

The formal LQR problem is to find the optimal control input $\mathbf{u}^*(t)$ that minimizes the cost function $J$, subject to the system dynamics:

$$\text{Minimize } J = \int_0^\infty (\mathbf{x}^T(t)\mathbf{Q}\mathbf{x}(t) + \mathbf{u}^T(t)\mathbf{R}\mathbf{u}(t)) dt$$
$$\text{Subject to: } \dot{\mathbf{x}}(t) = \mathbf{A}\mathbf{x}(t) + \mathbf{B}\mathbf{u}(t)$$

Our goal is now, choosing the weighting matrices $\mathbf{Q}$ and $\mathbf{R}$, we can tune the controller to prioritize different aspects of performance:

* **Larger $\mathbf{Q}$:** driving states to zero quickly
* **Larger $\mathbf{R}$:** minimizing control effort

You know what? LQR is a linear state-feedback control system, so it people from long time ago has found out it also satisfies this form:

$$\mathbf{u}(t) = -\mathbf{K}\mathbf{x}(t)$$

well, we do not know what $K$ is, we need to find $P$ to calculate $K$:

$$\mathbf{K} = \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}$$

But $\mathbf{P} \in \mathbb{R}^{n \times n}$ is also sth that we need to find. 

People long time ago just started by adding and subtracting $\mathbf{x}_0^T\mathbf{P}\mathbf{x}_0$ from $J$ to see what they can explore from here:

$$J = \mathbf{x}_0^T\mathbf{P}\mathbf{x}_0 - \mathbf{x}_0^T\mathbf{P}\mathbf{x}_0 + \int_0^\infty (\mathbf{x}^T\mathbf{Q}\mathbf{x} + \mathbf{u}^T\mathbf{R}\mathbf{u}) dt$$

Substituting the integral form of $-\mathbf{x}_0^T\mathbf{P}\mathbf{x}_0 = \int_0^\infty ( \frac{d}{dt}(\mathbf{x}^T\mathbf{P}\mathbf{x}))dt$:

$$J = \mathbf{x}_0^T\mathbf{P}\mathbf{x}_0 + \int_0^\infty \left( \frac{d}{dt}(\mathbf{x}^T\mathbf{P}\mathbf{x}) + \mathbf{x}^T\mathbf{Q}\mathbf{x} + \mathbf{u}^T\mathbf{R}\mathbf{u} \right) dt$$

Now we derive $\frac{d}{dt}(\mathbf{x}^T\mathbf{P}\mathbf{x})$. Since $\mathbf{P}$ is a constant, symmetric matrix ($\mathbf{P} = \mathbf{P}^T$):

$$\frac{d}{dt}(\mathbf{x}^T\mathbf{P}\mathbf{x}) = \dot{\mathbf{x}}^T\mathbf{P}\mathbf{x} + \mathbf{x}^T\mathbf{P}\dot{\mathbf{x}}$$

Now, substitute $\dot{\mathbf{x}} = \mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{u}$ into this expression:

$$\frac{d}{dt}(\mathbf{x}^T\mathbf{P}\mathbf{x}) = (\mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{u})^T\mathbf{P}\mathbf{x} + \mathbf{x}^T\mathbf{P}(\mathbf{A}\mathbf{x} + \mathbf{B}\mathbf{u})$$
$$\frac{d}{dt}(\mathbf{x}^T\mathbf{P}\mathbf{x}) = (\mathbf{x}^T\mathbf{A}^T + \mathbf{u}^T\mathbf{B}^T)\mathbf{P}\mathbf{x} + \mathbf{x}^T\mathbf{P}\mathbf{A}\mathbf{x} + \mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{u}$$
$$\frac{d}{dt}(\mathbf{x}^T\mathbf{P}\mathbf{x}) = \mathbf{x}^T\mathbf{A}^T\mathbf{P}\mathbf{x} + \mathbf{u}^T\mathbf{B}^T\mathbf{P}\mathbf{x} + \mathbf{x}^T\mathbf{P}\mathbf{A}\mathbf{x} + \mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{u}$$

Substitute this back into the expression for $J$:

$$J = \mathbf{x}_0^T\mathbf{P}\mathbf{x}_0 + \int_0^\infty \left( \mathbf{x}^T\mathbf{A}^T\mathbf{P}\mathbf{x} + \mathbf{u}^T\mathbf{B}^T\mathbf{P}\mathbf{x} + \mathbf{x}^T\mathbf{P}\mathbf{A}\mathbf{x} + \mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{u} + \mathbf{x}^T\mathbf{Q}\mathbf{x} + \mathbf{u}^T\mathbf{R}\mathbf{u} \right) dt$$

Now, let's group the terms. We can gather terms involving $\mathbf{x}^T (\cdot) \mathbf{x}$ and terms involving $\mathbf{u}$. Note that $\mathbf{u}^T\mathbf{B}^T\mathbf{P}\mathbf{x}$ and $\mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{u}$ are scalars and transposes of each other (and since $\mathbf{P}=\mathbf{P}^T$), they are equal. So their sum is $2\mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{u}$.

$$J = \mathbf{x}_0^T\mathbf{P}\mathbf{x}_0 + \int_0^\infty \left( \mathbf{x}^T(\mathbf{A}^T\mathbf{P} + \mathbf{P}\mathbf{A} + \mathbf{Q})\mathbf{x} + \mathbf{u}^T\mathbf{R}\mathbf{u} + 2\mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{u} \right) dt$$

Now, we want to rewrite the terms dependent on $\mathbf{u}$ into a perfect square like:

$$(a + b)² = a² + 2ab + b²$$
$$a² + 2ab = (a + b)² - b²$$

we somehow figuredout it looks like this, I also cannot derive it how, but thats the result:

$$\mathbf{u}^T\mathbf{R}\mathbf{u} + 2\mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{u} = (\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x})^T \mathbf{R} (\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x}) - \mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x}$$

Substitute this back into the expression for $J$:

$$J = \mathbf{x}_0^T\mathbf{P}\mathbf{x}_0 + \int_0^\infty \left( \mathbf{x}^T(\mathbf{A}^T\mathbf{P} + \mathbf{P}\mathbf{A} + \mathbf{Q})\mathbf{x} + (\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x})^T \mathbf{R} (\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x}) - \mathbf{x}^T\mathbf{P}\mathbf{B}\mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x} \right) dt$$

### Important part
Finally, group the $\mathbf{x}^T(\cdot)\mathbf{x}$ terms **AGAIN, we did this twice aigoo**:

$$J = \mathbf{x}_0^T\mathbf{P}\mathbf{x}_0 + \int_0^\infty \left( \mathbf{x}^T(\mathbf{A}^T\mathbf{P} + \mathbf{P}\mathbf{A} + \mathbf{Q} - \mathbf{P}\mathbf{B}\mathbf{R}^{-1}\mathbf{B}^T\mathbf{P})\mathbf{x} + (\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x})^T \mathbf{R} (\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x}) \right) dt$$

To minimize $J$, we need to make the integral as small as possible. Let's analyze the terms within the integral:

1.  The term $\mathbf{x}^T(\mathbf{A}^T\mathbf{P} + \mathbf{P}\mathbf{A} + \mathbf{Q} - \mathbf{P}\mathbf{B}\mathbf{R}^{-1}\mathbf{B}^T\mathbf{P})\mathbf{x}$ depends only on the state $\mathbf{x}$, which is a consequence of the control.
2.  The term $(\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x})^T \mathbf{R} (\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x})$ is a quadratic form involving $\mathbf{u}$. Since $\mathbf{R}$ is a positive definite matrix, this term is always greater than or equal to zero.

To minimize $J$, we must choose $\mathbf{u}$ such that the second term in the integral is zero (its minimum possible value). This occurs when:

$$\mathbf{u} + \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x} = \mathbf{0}$$

Imagine we already have a **optimal control**:
$$\mathbf{u}^*(t) = -\mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x}(t)$$

Imagine a specific case at a convergence, this $\mathbf{u}^*(t)$ term is only 0, only when $\mathbf{x}$ must also be zero. Therefore, interestingly:

$$\mathbf{A}^T\mathbf{P} + \mathbf{P}\mathbf{A} + \mathbf{Q} - \mathbf{P}\mathbf{B}\mathbf{R}^{-1}\mathbf{B}^T\mathbf{P} = \mathbf{0}$$

This is the famous **Algebraic Riccati Equation (ARE)** which we can solve numerically.


**FInally Back to the Top: Now Calculate $\mathbf{K}$** using the obtained $\mathbf{P}$ to get optimal control $\mathbf{u}(t)$:

$$\mathbf{K} = \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}$$
$$\mathbf{u}(t) = -\mathbf{K}\mathbf{x}(t)$$

This means the controller takes the current state $\mathbf{x}(t)$, multiplies it by the pre-computed gain matrix $\mathbf{K}$, and applies this as the control input. The negative sign indicates feedback (driving the state towards zero).


## Derivation of Linear Optimal Control ($u^* = -Kx$) and Quadratic Value Function ($V(x)=x^T Px$)

### Bellman Optimality -> The Value Function (Cost-to-Go):
if a path from point A to point C is optimal, then any segment of that path (e.g., from point B to point C, where B is on the path) must also be optimal from point B. This motivated the transformation of cost function J to the value function below.

**The Cost Function (J):** Our problem statement is to find $u(t)$ that minimizes:
$$J = \int_{0}^{\infty} (x^T(\tau)Qx(\tau) + u^T(\tau)Ru(\tau))d\tau$$

This is the objective.

**The Value Function (V) is Defined in Terms of J:** The value function V(x(t),t)$, as the minimum possible future cost from the current state $x(t)$ at time $t$ to the end of the control horizon (which is $\infty$ for infinite-horizon LQR).
$$V(x(t),t) = \min_{u(\tau),\tau \ge t} \int_{t}^{\infty} (x^T(\tau)Qx(\tau) + u^T(\tau)Ru(\tau))d\tau$$

So, $V(x(t),t)$ is literally the minimum value of a section of the integral $J$.

**Time-Invariance of $V(x)$:** For an LTI system with an infinite horizon and constant cost weights, the optimal cost-to-go function $V$ will eventually reach a steady-state. This means it will no longer explicitly depend on time $t$. Therefore, $\frac{\partial V}{\partial t} = 0$.
$$-\frac{\partial V}{\partial t} = \min_{u} \left[ x^T Q x + u^T R u + \left(\frac{\partial V}{\partial x}\right)^T (Ax + Bu) \right]$$

$$0 = \min_{u} \left[ x^T Q x + u^T R u + \frac{\partial V}{\partial t} + \left(\frac{\partial V}{\partial x}\right)^T (Ax + Bu) \right]$$


### Deriving $u^* = -Kx$

Let's focus on the term inside the $\min_{u}$ operator. This is a function of $u$. To find the $u$ that minimizes it, we take the partial derivative with respect to $u$ and set it to zero.

Let $g(u) = x^T Q x + u^T R u + \left(\frac{\partial V}{\partial x}\right)^T (Ax + Bu)$.
We're minimizing $g(u)$ with respect to $u$. Only terms involving $u$ are relevant:

$$g(u) = u^T R u + \left(\frac{\partial V}{\partial x}\right)^T Bu$$

Taking the derivative with respect to $u$:

$$\frac{\partial g}{\partial u} = 2Ru + B^T \frac{\partial V}{\partial x}$$

Set to zero to find the optimal $u^*$:

$$2Ru^* + B^T \frac{\partial V}{\partial x} = 0$$
$$u^* = -\frac{1}{2} R^{-1} B^T \frac{\partial V}{\partial x}$$

This is the crucial step: The optimal control is found to be a linear function of the gradient of the value function!

### Deriving $V(x)=x^T Px$

At this point, we have $u^*$ expressed in terms of $\frac{\partial V}{\partial x}$. Now we need to solve for $V(x)$. This is where the specific structure of the LQR problem (linear dynamics, quadratic cost) becomes paramount.

Since the problem is Linear-Quadratic, it is a known property from optimal control theory that the optimal value function $V(x)$ will be a quadratic form of the state. This is not just a guess, but a deduction based on the inherent structure of LQ problems.

* **Why Quadratic?** If $V(x)$ were linear, its second derivative would be zero, which wouldn't match the quadratic terms in the HJB. If it were higher order, the derivatives would lead to more complex non-linear equations, which would contradict the simplicity and linearity that arise from the problem. The quadratic form $x^T Px$ is the lowest-order non-trivial form that is consistent with the problem's structure.
* **Symmetry:** $P$ is typically chosen to be symmetric ($P=P^T$) because $x^T Px = x^T P^T x$. Any asymmetric part of $P$ cancels out in the quadratic form, so we enforce symmetry for uniqueness and consistency.

So, we propose (or infer) the form:

$$V(x) = x^T Px$$

where $P$ is a symmetric positive definite matrix.

Now, calculate the gradient of $V(x)$ with respect to $x$:

$$\frac{\partial V}{\partial x} = 2Px$$

### Deriving ARE

Substitute $\frac{\partial V}{\partial x} = 2Px$ back into the expression for $u^*$:

$$u^* = -\frac{1}{2} R^{-1} B^T (2Px)$$
$$u^* = -R^{-1} B^T Px$$

This is our desired linear state feedback law! Here, $K = R^{-1} B^T P$.

Now, substitute $u^*$ and $\frac{\partial V}{\partial x}$ back into the simplified HJB equation:

$$0 = x^T Q x + (-R^{-1} B^T Px)^T R (-R^{-1} B^T Px) + (2Px)^T (Ax + B(-R^{-1} B^T Px))$$
$$0 = x^T Q x + x^T P^T (B^T)^T (R^{-1})^T R R^{-1} B^T Px + 2x^T P^T (Ax - BR^{-1} B^T Px)$$
Since $P = P^T$ and $R$ is symmetric ($R=R^T$), $R^{-1}$ is also symmetric ($(R^{-1})^T = R^{-1}$). Also, $(B^T)^T = B$.

$$0 = x^T Q x + x^T P B R^{-1} B^T Px + 2x^T PAx - 2x^T P B R^{-1} B^T Px$$
$$0 = x^T Q x + 2x^T PAx - x^T P B R^{-1} B^T Px$$

Recognizing that $2x^T PAx = x^T PAx + x^T A^T P^T x = x^T (A^T P + PA)x$ (since $P$ is symmetric):

$$0 = x^T (Q + A^T P + PA - P B R^{-1} B^T P)x$$

For this equation to hold for any state $x$, the matrix in the parenthesis must be identically zero.

$$A^T P + PA - P B R^{-1} B^T P + Q = 0$$

This is the **Algebraic Riccati Equation (ARE)**.


# MPC

## Transformation to a Standard Quadratic Program (QP)

The goal is to eliminate the state variables $x(k+i|k)$ from the optimization problem, leaving only the control input sequence $U_k$ as the decision variables. This is possible because the state evolution is precisely defined by the linear system dynamics, which act as equality constraints.

### 1. Prediction of Future States:

We start by recursively expanding the system dynamics equation: $x(k+i+1|k) = Ax(k+i|k) + Bu(k+i|k)$.

* **Initial State:** $x(k|k) = x(k)$ (the current measured state).
* **1-step ahead prediction:**
    $x(k+1|k) = Ax(k|k) + Bu(k|k)$
* **2-step ahead prediction:**
    $x(k+2|k) = Ax(k+1|k) + Bu(k+1|k)$
    Substitute $x(k+1|k)$:
    $x(k+2|k) = A(Ax(k|k) + Bu(k|k)) + Bu(k+1|k)$
    $x(k+2|k) = A^2 x(k|k) + ABu(k|k) + Bu(k+1|k)$

And so on, up to $H_p$ steps:

Generalizing, the predicted state at any future time $k+i$ can be expressed as a sum of terms related to the initial state $x(k)$ and the future control inputs $u(k|k), \dots, u(k+i-1|k)$:

$$x(k+i|k) = A^i x(k) + \sum_{j=0}^{i-1} A^{i-1-j} Bu(k+j|k)$$

Now, let's stack all the predicted states and controls into large vectors.

Let $X_k = \begin{bmatrix} x(k+1|k) \\ x(k+2|k) \\ \vdots \\ x(k+H_p|k) \end{bmatrix}$ and $U_k = \begin{bmatrix} u(k|k) \\ u(k+1|k) \\ \vdots \\ u(k+H_c-1|k) \end{bmatrix}$ (remembering the assumption that $u$ is constant after $H_c-1$).

We can write the entire sequence of future states as:

$$X_k = \mathbf{\Phi} x(k) + \mathbf{\Gamma} U_k$$

where:

* $\mathbf{\Phi}$ is a large block matrix derived from powers of $A$.
* $\mathbf{\Gamma}$ is a large block lower triangular matrix (often called the Toeplitz matrix or controllability matrix) containing terms like $B, AB, A^2 B, \dots$. Its structure reflects how current and past controls affect future states.

### 2. Substituting into the Objective Function:

Recall the objective function:

$$J(U_k, x(k)) = \sum_{i=0}^{H_p-1} \left( \|x(k+i|k) - x_{ref}(k+i)\|_Q^2 + \|u(k+i|k) - u_{ref}(k+i)\|_R^2 \right) + \|x(k+H_p|k) - x_{ref}(k+H_p)\|_P^2$$

We can rewrite this in a compact quadratic form. Let's simplify by assuming $x_{ref}=0$ and $u_{ref}=0$ for now to highlight the structure. The general case simply introduces linear terms.

$$J(U_k, x(k)) = \sum_{i=0}^{H_p-1} x(k+i|k)^T Q x(k+i|k) + \sum_{i=0}^{H_c-1} u(k+i|k)^T R u(k+i|k) + x(k+H_p|k)^T P x(k+H_p|k)$$

By substituting $x(k+i|k) = A^i x(k) + \sum_{j=0}^{i-1} A^{i-1-j} Bu(k+j|k)$ into the expression for $J$, the objective function becomes a quadratic function of $U_k$ and $x(k)$:

$$J(U_k, x(k)) = \frac{1}{2} U_k^T H U_k + G^T U_k + J_{const}$$

where:

* $H$ is a symmetric positive definite matrix (or positive semi-definite, depending on $R$). It encapsulates the weights $Q, R, P$ and system matrices $A, B$.
* $G$ is a vector that depends on the current state $x(k)$ and the reference trajectories.
* $J_{const}$ is a term that depends only on $x(k)$ and the reference trajectories, which doesn't affect the minimization with respect to $U_k$.

### 3. Substituting into Constraints:

Similarly, all constraints (input, state, output) are originally expressed in terms of $x(k+i|k)$ and $u(k+i|k)$. By substituting the state prediction equation $x(k+i|k) = \mathbf{\Phi}_i x(k) + \mathbf{\Gamma}_i U_k$ (where $\mathbf{\Phi}_i$ and $\mathbf{\Gamma}_i$ are parts of $\mathbf{\Phi}$ and $\mathbf{\Gamma}$ corresponding to time $k+i$), all constraints can be rewritten purely in terms of $x(k)$ (which is known) and $U_k$ (the decision variables).

For example, a state constraint $x_{min} \le x(k+i|k) \le x_{max}$ becomes:

$$x_{min} \le \mathbf{\Phi}_{i} x(k) + \mathbf{\Gamma}_{i} \mathbf{U}_k \le x_{max}$$

This can be rearranged into the standard inequality form:

$$-\mathbf{\Gamma}_{i} \mathbf{U}_k \le \mathbf{\Phi}_{i} x(k) - x_{min}$$
$$\mathbf{\Gamma}_{i} \mathbf{U}_k \le x_{max} - \mathbf{\Phi}_{i} x(k)$$

These are stacked for all $i$ and all types of constraints (input, state, output) into the compact form:

$$\mathbf{L} \mathbf{U}_k \le \mathbf{W}$$

where $\mathbf{L}$ and $\mathbf{W}$ are matrices and vectors that encapsulate all the constraint bounds, system matrices, and the current state $x(k)$. Equality constraints (like the dynamics, if kept explicit rather than condensed) would form $\mathbf{M}U_k = \mathbf{V}$.

## The Result: A Standard QP Form

After this transformation, the MPC problem at each time step $k$ is reduced to:

**Minimize:**
$$\frac{1}{2} U_k^T H U_k + G^T U_k$$
**Subject to:**
$$\mathbf{L}U_k \le \mathbf{W} \quad \text{(and possibly } \mathbf{M}U_k = \mathbf{V} \text{ for equality constraints)}$$

This is precisely the standard form of a Quadratic Program (QP), where $U_k$ is the optimization variable (what's called 'z' in a generic QP solver). The matrices $H, G, L, W, M, V$ are updated at each time step based on the current measured state $x(k)$.


## Solutions for MPC (Quadratic Programming (QP) Solvers)

* **Active Set Methods:** These methods work by iteratively selecting a subset of the inequality constraints to be "active" (i.e., treated as equality constraints). They solve an equality-constrained QP at each iteration, update the active set, and move towards the optimal solution.
    * **Pros:** Highly reliable, provide exact solutions (within machine precision), often used for small to medium-sized problems.
    * **Cons:** Can be slow for large problems, especially if many active set changes are required. Number of iterations can be high.
* **Interior-Point Methods:** These methods transform the constrained QP into a sequence of unconstrained problems by adding "barrier functions" to the objective that penalize approaching the constraint boundaries. They then use Newton's method to solve these unconstrained problems.
    * **Pros:** Generally scale much better with problem size (fewer iterations, though each iteration is more complex) and are often faster for large-scale QPs. They work well with warm-starting (using the previous solution as an initial guess).
    * **Cons:** Can be more complex to implement than active set methods.
* **Primal-Dual Methods:** A broader class that includes many interior-point methods. They simultaneously solve the primal (original) QP problem and its dual problem.


## Derivation

### State Prediction in Compact Matrix Form

**THE POINT: ONLY REPRESENT IN U, YOU CAN SEE IN THE MATRIX BELOW**

**Recap:** We want to express all future predicted states $x(k+i|k)$ as a function of the current state $x(k)$ and the sequence of future control inputs $U_k$.

**System:** $x(k+1)=Ax(k)+Bu(k)$
**Prediction Horizon:** $H_p$
**Control Horizon:** $H_c$ (For simplicity, let's assume $H_c=H_p$ for now. The case $H_c<H_p$ just means some $u$s are repeated, which is a minor modification.)

**Step-by-step prediction expansion:**

$x(k+1|k)=Ax(k)+Bu(k|k)$
$x(k+2|k)=Ax(k+1|k)+Bu(k+1|k)=A(Ax(k)+Bu(k|k))+Bu(k+1|k)=A^2x(k)+ABu(k|k)+Bu(k+1|k)$
$x(k+3|k)=Ax(k+2|k)+Bu(k+2|k)=A(A^2x(k)+ABu(k|k)+Bu(k+1|k))+Bu(k+2|k)=A^3x(k)+A^2Bu(k|k)+ABu(k+1|k)+Bu(k+2|k)$
... and so on, up to $x(k+H_p|k)$.

**Stacking the Predictions:**

Now, let's define the stacked vectors:

* **Future States Vector:**
    $X_k = \begin{bmatrix} x(k+1|k) \\ x(k+2|k) \\ \vdots \\ x(k+H_p|k) \end{bmatrix}$ (size $n \cdot H_p \times 1$)
* **Control Input Sequence (Decision Variable):**
    $U_k = \begin{bmatrix} u(k|k) \\ u(k+1|k) \\ \vdots \\ u(k+H_c-1|k) \end{bmatrix}$ (size $m \cdot H_c \times 1$)

Substitute the expanded predictions into $X_k$:

$$X_k = \begin{bmatrix} Ax(k)+Bu(k|k) \\ A^2x(k)+ABu(k|k)+Bu(k+1|k) \\ A^3x(k)+A^2Bu(k|k)+ABu(k+1|k)+Bu(k+2|k) \\ \vdots \\ A^{H_p}x(k)+A^{H_p-1}Bu(k|k)+\dots+Bu(k+H_p-1|k) \end{bmatrix}$$

Now, we separate the terms involving $x(k)$ from the terms involving $U_k$:

$$X_k = \begin{bmatrix} A \\ A^2 \\ A^3 \\ \vdots \\ A^{H_p} \end{bmatrix} x(k) + \begin{bmatrix} B & 0 & 0 & \dots & 0 \\ AB & B & 0 & \dots & 0 \\ A^2B & AB & B & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ A^{H_p-1}B & A^{H_p-2}B & A^{H_p-3}B & \dots & B \end{bmatrix} \begin{bmatrix} u(k|k) \\ u(k+1|k) \\ u(k+2|k) \\ \vdots \\ u(k+H_p-1|k) \end{bmatrix}$$

This is exactly the form: $X_k = \mathbf{\Phi} x(k) + \mathbf{\Gamma} U_k$

Where:

* $\mathbf{\Phi}$ (State Contribution Matrix):
    $$\mathbf{\Phi} = \begin{bmatrix} A \\ A^2 \\ \vdots \\ A^{H_p} \end{bmatrix}$$
    **Size:** ($n \cdot H_p$) $\times n$. Each block is $n \times n$. This matrix shows how the initial state $x(k)$ propagates to all future states if no control were applied.
* $\mathbf{\Gamma}$ (Control Contribution Matrix / Block Controllability Matrix):
    $$\mathbf{\Gamma} = \begin{bmatrix} B & 0 & 0 & \dots & 0 \\ AB & B & 0 & \dots & 0 \\ A^2B & AB & B & \dots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ A^{H_p-1}B & A^{H_p-2}B & A^{H_p-3}B & \dots & B \end{bmatrix}$$
    **Size:** ($n \cdot H_p$) $\times$ ($m \cdot H_p$). Each block is $n \times m$. This lower triangular block matrix (sometimes called a block Toeplitz matrix) shows how the sequence of control inputs $U_k$ influences the future states. The "controllability" aspect comes from the powers of $A$ multiplying $B$, indicating how inputs at different times propagate through the system.

#### Numerical Example

Let's use a very simple system: $x(k+1)=Ax(k)+Bu(k)$
$A=\begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$, $B=\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ (This is a discrete-time integrator/position system)
Let $H_p=2$ and $H_c=2$.

**Predictions:**

$x(k+1|k)=Ax(k)+Bu(k|k)$
$x(k+2|k)=Ax(k+1|k)+Bu(k+1|k)=A^2x(k)+ABu(k|k)+Bu(k+1|k)$

**Calculate powers of A and products with B:**
$A^2 = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix}$
$AB = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$

**Stacked Vectors:**
$X_k = \begin{bmatrix} x(k+1|k) \\ x(k+2|k) \end{bmatrix}$
$U_k = \begin{bmatrix} u(k|k) \\ u(k+1|k) \end{bmatrix}$

**Forming $\mathbf{\Phi}$ and $\mathbf{\Gamma}$:**

$$X_k = \begin{bmatrix} A \\ A^2 \end{bmatrix} x(k) + \begin{bmatrix} B & 0 \\ AB & B \end{bmatrix} U_k$$

Substitute the numerical matrices:

$$\mathbf{\Phi} = \begin{bmatrix} \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix} \\ \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix} \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 0 \\ 2 & 1 \end{bmatrix} \quad \text{(size } (2 \cdot 2) \times 2 = 4 \times 2 \text{)}$$

$$\mathbf{\Gamma} = \begin{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} & \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\ \begin{bmatrix} 0 \\ 1 \end{bmatrix} & \begin{bmatrix} 0 \\ 1 \end{bmatrix} \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \\ 0 & 0 \\ 1 & 1 \end{bmatrix} \quad \text{(size } (2 \cdot 2) \times (1 \cdot 2) = 4 \times 2 \text{)}$$

So, $X_k = \mathbf{\Phi} x(k) + \mathbf{\Gamma} U_k$ where all components are explicitly defined. This is how the state prediction is condensed.

---

### Substituting into the Objective Function (Deriving H, G, J_const)

**Recap:** We want to transform $J(U_k,x(k))$ into $\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}$.

**Objective Function:**

$$J(U_k,x(k)) = \sum_{i=0}^{H_p-1} (\|x(k+i|k)-x_{ref}(k+i)\|_Q^2 + \|u(k+i|k)-u_{ref}(k+i)\|_R^2) + \|x(k+H_p|k)-x_{ref}(k+H_p)\|_P^2$$

Let $e_x(k+i|k)=x(k+i|k)-x_{ref}(k+i)$ and $e_u(k+i|k)=u(k+i|k)-u_{ref}(k+i)$.
The term $\|v\|_W^2 = v^T W v$.

Let's adjust the sum for x states to start from $i=1$ to $H_p$ to align with the stacked $X_k$ derived in Part 1 (which starts from $x(k+1|k)$). The terminal cost (with P) is explicitly included at $H_p$.

$$J(U_k,x(k)) = \sum_{i=1}^{H_p} \|x(k+i|k)-x_{ref}(k+i)\|_Q^2 + \sum_{i=0}^{H_c-1} \|u(k+i|k)-u_{ref}(k+i)\|_R^2$$
*(Note: For clarity, the terminal cost is typically a separate term at $x(k+H_p|k)$ with weight $P$. If $P$ is used as a `Q` for the last state in the sum, then $Q_{H_p}$ is $P$. Let's explicitly keep it in the sum by making $Q_{H_p} = P$ for $i=H_p$.)*

We can write the sums in stacked form:

$$J = E_X^T \bar{Q} E_X + E_U^T \bar{R} E_U$$

Where:
* $E_X = \begin{bmatrix} e_x(k+1|k) \\ e_x(k+2|k) \\ \vdots \\ e_x(k+H_p|k) \end{bmatrix}$ (size $n \cdot H_p \times 1$)
* $E_U = \begin{bmatrix} e_u(k|k) \\ e_u(k+1|k) \\ \vdots \\ e_u(k+H_c-1|k) \end{bmatrix}$ (size $m \cdot H_c \times 1$)

* $\bar{Q} = \text{diag}(Q, Q, \dots, Q, P)$ (a block diagonal matrix of size $(n \cdot H_p) \times (n \cdot H_p)$, where the last block is $P$ for $x(k+H_p|k)$ and others are $Q$).
* $\bar{R} = \text{diag}(R, R, \dots, R)$ ($H_c$ times, block diagonal matrix, size $(m \cdot H_c) \times (m \cdot H_c)$).

Now, substitute $X_k=\mathbf{\Phi}x(k)+\mathbf{\Gamma}U_k$ into the objective. Let's define the full reference vector $X_{ref}=[x_{ref}(k+1)^T \dots x_{ref}(k+H_p)^T]^T$ and $U_{ref}=[u_{ref}(k)^T \dots u_{ref}(k+H_c-1)^T]^T$.

The stacked state error is $E_X = X_k - X_{ref} = (\mathbf{\Phi}x(k)+\mathbf{\Gamma}U_k) - X_{ref}$.
The stacked input error is $E_U = U_k - U_{ref}$.

Then:

$$J = (\mathbf{\Phi}x(k)+\mathbf{\Gamma}U_k - X_{ref})^T \bar{Q} (\mathbf{\Phi}x(k)+\mathbf{\Gamma}U_k - X_{ref}) + (U_k - U_{ref})^T \bar{R} (U_k - U_{ref})$$

Now, expand and collect terms by powers of $U_k$:

* **Quadratic term in $U_k$ (for $H$):**
    From the first term: $(\mathbf{\Gamma}U_k)^T \bar{Q} (\mathbf{\Gamma}U_k) = U_k^T \mathbf{\Gamma}^T \bar{Q} \mathbf{\Gamma} U_k$
    From the second term: $U_k^T \bar{R} U_k$
    So, the full quadratic term is $U_k^T (\mathbf{\Gamma}^T \bar{Q} \mathbf{\Gamma} + \bar{R}) U_k$.
    Therefore, $H = 2(\mathbf{\Gamma}^T \bar{Q} \mathbf{\Gamma} + \bar{R})$. (The factor of 2 comes from the standard $\frac{1}{2}z^T H z$ form of QP objective).

* **Linear term in $U_k$ (for $G$):**
    Let $c_x = \mathbf{\Phi}x(k) - X_{ref}$.
    From the first term (cross-term $2 c_x^T \bar{Q} \mathbf{\Gamma} U_k$): $2(\mathbf{\Phi}x(k) - X_{ref})^T \bar{Q} \mathbf{\Gamma} U_k$
    From the second term (cross-term $-2 U_{ref}^T \bar{R} U_k$): $-2U_{ref}^T \bar{R} U_k$
    So, the full linear term is $2(\mathbf{\Phi}x(k) - X_{ref})^T \bar{Q} \mathbf{\Gamma} U_k - 2U_{ref}^T \bar{R} U_k$.
    Therefore, $G^T = 2(\mathbf{\Phi}x(k) - X_{ref})^T \bar{Q} \mathbf{\Gamma} - 2U_{ref}^T \bar{R}$.
    Or, $G = 2\mathbf{\Gamma}^T \bar{Q} (\mathbf{\Phi}x(k) - X_{ref}) - 2\bar{R}^T U_{ref}$. Note that $\bar{R}$ is symmetric, so $\bar{R}^T=\bar{R}$.

* **Constant term (for $J_{const}$):**
    This term does not depend on $U_k$ and is effectively ignored by the optimizer.
    It includes: $(\mathbf{\Phi}x(k) - X_{ref})^T \bar{Q} (\mathbf{\Phi}x(k) - X_{ref}) + U_{ref}^T \bar{R} U_{ref}$.
    This term is important if you want to know the actual minimum cost, but not for finding the optimal $U_k$.

#### Numerical Example

Continue with our previous example: $A=\begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$, $B=\begin{bmatrix} 0 \\ 1 \end{bmatrix}$
$H_p=2$, $H_c=2$.
Let $Q=\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ (identity matrix for state penalty), $R=[0.1]$ (scalar for input penalty).
Let $P=Q$ (for simplicity of $\bar{Q}$ structure), $x_{ref}=0$, $u_{ref}=0$.

From Part 1:
$\mathbf{\Phi}=\begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 0 \\ 2 & 1 \end{bmatrix}$
$\mathbf{\Gamma}=\begin{bmatrix} 0 & 0 \\ 1 & 0 \\ 0 & 0 \\ 1 & 1 \end{bmatrix}$

Also:
$\bar{Q}=\begin{bmatrix} Q & 0 \\ 0 & P \end{bmatrix} = \begin{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} & \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} \\ \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} & \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$ (size $4 \times 4$)
$\bar{R}=\begin{bmatrix} R & 0 \\ 0 & R \end{bmatrix}=\begin{bmatrix} 0.1 & 0 \\ 0 & 0.1 \end{bmatrix}$ (size $2 \times 2$)

Now, let's compute $H$ (assuming $H = 2(\mathbf{\Gamma}^T \bar{Q} \mathbf{\Gamma} + \bar{R})$ as per the QP formulation):
First, $\mathbf{\Gamma}^T \bar{Q}$:
$$\begin{bmatrix} 0 & 1 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0 & 1 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix}$$
Then, $(\mathbf{\Gamma}^T \bar{Q})\mathbf{\Gamma}$:
$$\begin{bmatrix} 0 & 1 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 1 & 0 \\ 0 & 0 \\ 1 & 1 \end{bmatrix} = \begin{bmatrix} (0)(0)+(1)(1)+(0)(0)+(1)(1) & (0)(0)+(1)(0)+(0)(0)+(1)(1) \\ (0)(0)+(0)(1)+(0)(0)+(1)(1) & (0)(0)+(0)(0)+(0)(0)+(1)(1) \end{bmatrix} = \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix}$$
Finally, $H = 2 \left( \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix} + \begin{bmatrix} 0.1 & 0 \\ 0 & 0.1 \end{bmatrix} \right) = 2 \begin{bmatrix} 2.1 & 1 \\ 1 & 1.1 \end{bmatrix} = \begin{bmatrix} 4.2 & 2 \\ 2 & 2.2 \end{bmatrix}$

Now, let's compute $G$ (assuming $x_{ref}=0, u_{ref}=0$, so $X_{ref}=0, U_{ref}=0$).
In this case, $G = 2\mathbf{\Gamma}^T \bar{Q} \mathbf{\Phi}x(k)$.
Let $x(k)=\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$.

First, $\mathbf{\Phi}x(k)$:
$$\begin{bmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 0 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} x_1 \\ x_1+x_2 \\ x_1 \\ 2x_1+x_2 \end{bmatrix}$$
Then, $\bar{Q} \mathbf{\Phi}x(k)$:
$$\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_1+x_2 \\ x_1 \\ 2x_1+x_2 \end{bmatrix} = \begin{bmatrix} x_1 \\ x_1+x_2 \\ x_1 \\ 2x_1+x_2 \end{bmatrix}$$
Finally, $G = 2\mathbf{\Gamma}^T (\bar{Q} \mathbf{\Phi}x(k))$:
$$G = 2 \begin{bmatrix} 0 & 1 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_1+x_2 \\ x_1 \\ 2x_1+x_2 \end{bmatrix} = 2 \begin{bmatrix} (0)x_1 + (1)(x_1+x_2) + (0)x_1 + (1)(2x_1+x_2) \\ (0)x_1 + (0)(x_1+x_2) + (0)x_1 + (1)(2x_1+x_2) \end{bmatrix}$$
$$G = 2 \begin{bmatrix} x_1+x_2 + 2x_1+x_2 \\ 2x_1+x_2 \end{bmatrix} = 2 \begin{bmatrix} 3x_1+2x_2 \\ 2x_1+x_2 \end{bmatrix} = \begin{bmatrix} 6x_1+4x_2 \\ 4x_1+2x_2 \end{bmatrix}$$

The $J_{const}$ term would be $x(k)^T \mathbf{\Phi}^T \bar{Q} \mathbf{\Phi} x(k)$, which would be a scalar depending only on $x(k)$.

This detailed breakdown shows how the matrices $H$ and $G$ are explicitly constructed from the system matrices, weights, and the current state. This allows a standard QP solver to take these numerical matrices and solve for the optimal $U_k$ sequence at each time step.

### But why do we need to transform the cost function into separated linear, quadratic and Jconst term?

QP solvers are designed to solve problems in a very specific mathematical form. The standard general form of a Quadratic Program is:

**Minimize:**
$$f(z) = \frac{1}{2} z^T H z + g^T z$$

**Subject to:**
$$A_{eq} z = b_{eq}$$
$$A_{ineq} z \le b_{ineq}$$

Where:
* $z$ is the vector of optimization variables.
* $H$ is the Hessian matrix (symmetric). It determines the curvature of the objective function.
* $g$ is the linear term vector.
* $A_{eq}, b_{eq}, A_{ineq}, b_{ineq}$ define the linear equality and inequality constraints.

By transforming the MPC objective into the form $\frac{1}{2} U_k^T H U_k + G^T U_k + J_{const}$:

* Our decision variable $U_k$ maps directly to the generic $z$.
* Our calculated $H$ matrix maps directly to the generic $H$.
* Our calculated $G$ vector maps directly to the generic $g$.

## Simple Example: 1D Car Path Following with MPC

**Scenario:** Imagine a car that can only move along a straight line (1D). Its goal is to follow a predefined "path" (a sequence of desired positions) along this line. We want to control its acceleration to achieve this.

### 1. Dynamics Model:

Let's use a very simple discrete-time model for longitudinal motion:

* **States ($x$):**
    * $p(k)$: Position of the car at time $k$
    * $v(k)$: Velocity of the car at time $k$
    So, $x(k)=\begin{bmatrix} p(k) \\ v(k) \end{bmatrix}$
* **Input ($u$):**
    * $a(k)$: Acceleration applied by the car at time $k$
    So, $u(k)=[a(k)]$

**Discrete-Time Equations:**
Assume a sampling time $\Delta t=1$ second for simplicity.
$p(k+1)=p(k)+v(k)\Delta t+\frac{1}{2}a(k)(\Delta t)^2$
$v(k+1)=v(k)+a(k)\Delta t$

Plugging in $\Delta t=1$:
$p(k+1)=p(k)+v(k)+\frac{1}{2}a(k)$
$v(k+1)=v(k)+a(k)$

In state-space form $x(k+1)=Ax(k)+Bu(k)$:
$A=\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$, $B=\begin{bmatrix} 0.5 \\ 1 \end{bmatrix}$
*(Self-correction: The position update $p(k+1)=p(k)+v(k)\Delta t$ means the `v(k)` term is in the first row of A. The velocity update $v(k+1)=v(k)+a(k)\Delta t$ means `v(k)` is in the second row of A, and `a(k)` is in the second row of B. The original matrix A was incorrect for this model. The corrected A matrix is $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ and B is $\begin{bmatrix} 0.5 \\ 1 \end{bmatrix}$.)*

### 2. Path Representation & Reference Trajectory:

Let our desired path be a set of target positions. For this 1D example, let the car's goal be to reach position 10 and stop.
* **Target positions (path):** $p_{ref}=[1,2,3,4,5,6,7,8,9,10,10,10,\dots]$
* **Target velocities:** $v_{ref}=[0,0,0,0,0,0,0,0,0,0,0,0,\dots]$ (we want it to stop at 10)

So, $x_{ref}(k+i)=\begin{bmatrix} p_{ref}(k+i) \\ v_{ref}(k+i) \end{bmatrix}$

### 3. MPC Parameters:

* **Prediction Horizon ($H_p$):** Let's choose $H_p=3$ steps.
* **Control Horizon ($H_c$):** Let's choose $H_c=2$ steps. This means we'll optimize $a(k)$ and $a(k+1)$, and assume $a(k+2)$ (and beyond) is zero.
* **Weights:**
    * $Q=\begin{bmatrix} 1 & 0 \\ 0 & 0.1 \end{bmatrix}$ (Penalize position error strongly, velocity error lightly)
    * $R=[0.01]$ (Penalize acceleration lightly to allow motion)
    * $P=\begin{bmatrix} 5 & 0 \\ 0 & 1 \end{bmatrix}$ (Strong terminal penalty on position at the end of horizon)
* **Constraints:**
    * **Acceleration limits:** $-1 \le a(k) \le 1$ ($m/s^2$)
    * **Velocity limits:** $0 \le v(k) \le 5$ ($m/s$) (Car cannot go backwards, max speed 5 m/s)
    * **Position limits:** $0 \le p(k) \le 11$ (Stay within a narrow road segment)

### 4. MPC at Time $k=0$ (First Iteration):

Assume current measured state: $x(0)=\begin{bmatrix} p(0) \\ v(0) \end{bmatrix}=\begin{bmatrix} 0 \\ 0 \end{bmatrix}$ (car starts at origin, at rest).

**a) Define Decision Variables:**
The control sequence we need to find is $U_0=\begin{bmatrix} u(0|0) \\ u(1|0) \end{bmatrix}=\begin{bmatrix} a(0) \\ a(1) \end{bmatrix}$. (Length $m \cdot H_c = 1 \cdot 2 = 2$)

**b) Generate Reference Trajectory for the Horizon:**
From our path, for $H_p=3$ steps:

$x_{ref}(1)=\begin{bmatrix} 1 \\ 0 \end{bmatrix}$
$x_{ref}(2)=\begin{bmatrix} 2 \\ 0 \end{bmatrix}$
$x_{ref}(3)=\begin{bmatrix} 3 \\ 0 \end{bmatrix}$

**c) Formulate Objective Function (QP style):**
The goal is to find $U_0$ that minimizes:

$$J = \sum_{i=1}^{3} \|x(i|0)-x_{ref}(i)\|_Q^2 + \sum_{i=0}^{1} \|u(i|0)-u_{ref}(i)\|_R^2$$

(Here, $u_{ref}(i)=0$ as we just want to follow the path, not target specific acceleration)
**Note:** The terminal cost is explicit if $P$ is the $Q$ for the final state in the sum. Let's make it explicit for clarity:

$$J = \|x(1|0)-x_{ref}(1)\|_Q^2 + \|x(2|0)-x_{ref}(2)\|_Q^2 + \|x(3|0)-x_{ref}(3)\|_P^2 + \|u(0|0)\|_R^2 + \|u(1|0)\|_R^2$$

**Condensing into $\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}$:**

First, state predictions:
$x(1|0)=Ax(0)+Bu(0|0)$
$x(2|0)=A^2x(0)+ABu(0|0)+Bu(1|0)$
$x(3|0)=A^3x(0)+A^2Bu(0|0)+ABu(1|0)+Bu(2|0)$ (Since $H_c=2$, $u(2|0)$ would be zero here or $u(1|0)$)
Let's assume $u(k+i|k)=0$ for $i \ge H_c$. So $u(2|0)=0$.

Calculate powers of A and products with B:
$A^2 = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}$
$A^3 = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 3 \\ 0 & 1 \end{bmatrix}$
$B = \begin{bmatrix} 0.5 \\ 1 \end{bmatrix}$
$AB = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 0.5 \\ 1 \end{bmatrix} = \begin{bmatrix} 1.5 \\ 1 \end{bmatrix}$
$A^2B = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 0.5 \\ 1 \end{bmatrix} = \begin{bmatrix} 2.5 \\ 1 \end{bmatrix}$

$\mathbf{\Phi} = \begin{bmatrix} A \\ A^2 \\ A^3 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 2 \\ 0 & 1 \\ 1 & 3 \\ 0 & 1 \end{bmatrix}$ (size $6 \times 2$)

$\mathbf{\Gamma} = \begin{bmatrix} B & 0 \\ AB & B \\ A^2B & AB \end{bmatrix} = \begin{bmatrix} 0.5 & 0 \\ 1 & 0 \\ 1.5 & 0.5 \\ 1 & 1 \\ 2.5 & 1.5 \\ 1 & 1 \end{bmatrix}$ (size $6 \times 2$)

$\bar{Q}_{stacked} = \text{diag}(Q, Q, P) = \begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0.1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0.1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 5 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 \end{bmatrix}$ (size $6 \times 6$)
$\bar{R} = \text{diag}(R,R) = \begin{bmatrix} 0.01 & 0 \\ 0 & 0.01 \end{bmatrix}$ (size $2 \times 2$)

Then, $H = 2(\mathbf{\Gamma}^T \bar{Q}_{stacked} \mathbf{\Gamma} + \bar{R})$.
And $G = 2\mathbf{\Gamma}^T \bar{Q}_{stacked}(\mathbf{\Phi}x(0) - X_{ref}) - 2\bar{R}U_{ref}$.
Since $x(0) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$ and $u_{ref}(i)=0$, $U_{ref}=0$.
$X_{ref} = \begin{bmatrix} 1 \\ 0 \\ 2 \\ 0 \\ 3 \\ 0 \end{bmatrix}$.
So, $G = 2\mathbf{\Gamma}^T \bar{Q}_{stacked}(\mathbf{\Phi}x(0) - X_{ref})$.

**d) Formulate Constraints (QP style):**

* **Input Constraints:** $-1 \le a(k) \le 1 \implies \begin{bmatrix} 1 \\ -1 \end{bmatrix} a(k) \le \begin{bmatrix} 1 \\ 1 \end{bmatrix}$
    Stacked for $U_0 = \begin{bmatrix} a(0) \\ a(1) \end{bmatrix}$:
    $$\begin{bmatrix} 1 & 0 \\ -1 & 0 \\ 0 & 1 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} a(0) \\ a(1) \end{bmatrix} \le \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}$$

* **State Constraints (Velocity):** $0 \le v(k+i|0) \le 5$ for $i=1,2,3$.
    $v(k+i|0)$ is the second component of $x(k+i|0)$.
    $x(k+i|0) = \mathbf{\Phi}_i x(0) + \mathbf{\Gamma}_i U_0$.
    The constraint can be written as:
    $\begin{bmatrix} 0 & 1 \end{bmatrix} x(k+i|0) \le 5$
    $-\begin{bmatrix} 0 & 1 \end{bmatrix} x(k+i|0) \le 0$
    Substitute $x(k+i|0)$:
    $\begin{bmatrix} 0 & 1 \end{bmatrix} (\mathbf{\Phi}_i x(0) + \mathbf{\Gamma}_i U_0) \le 5$
    $-\begin{bmatrix} 0 & 1 \end{bmatrix} (\mathbf{\Phi}_i x(0) + \mathbf{\Gamma}_i U_0) \le 0$

    For $x(0)=\begin{bmatrix} 0 \\ 0 \end{bmatrix}$:
    $v(1|0) = \begin{bmatrix} 0 & 1 \end{bmatrix} (A x(0) + B u(0|0)) = \begin{bmatrix} 0 & 1 \end{bmatrix} B u(0|0) = 1 \cdot a(0) = a(0)$.
    So, $0 \le a(0) \le 5$. (This is covered by $-1 \le a(0) \le 1$ limits).

    $v(2|0) = \begin{bmatrix} 0 & 1 \end{bmatrix} (A^2 x(0) + AB u(0|0) + B u(1|0)) = \begin{bmatrix} 0 & 1 \end{bmatrix} (AB u(0|0) + B u(1|0)) = 1 \cdot a(0) + 1 \cdot a(1) = a(0)+a(1)$.
    So, $0 \le a(0)+a(1) \le 5$.

    $v(3|0) = \begin{bmatrix} 0 & 1 \end{bmatrix} (A^3 x(0) + A^2B u(0|0) + AB u(1|0)) = \begin{bmatrix} 0 & 1 \end{bmatrix} (A^2B u(0|0) + AB u(1|0)) = 1 \cdot a(0) + 1 \cdot a(1) = a(0)+a(1)$.
    So, $0 \le a(0)+a(1) \le 5$.

* **State Constraints (Position):** $0 \le p(k+i|0) \le 11$ for $i=1,2,3$.
    $p(k+i|0)$ is the first component of $x(k+i|0)$.
    $\begin{bmatrix} 1 & 0 \end{bmatrix} x(k+i|0) \le 11$
    $-\begin{bmatrix} 1 & 0 \end{bmatrix} x(k+i|0) \le 0$

    For $x(0)=\begin{bmatrix} 0 \\ 0 \end{bmatrix}$:
    $p(1|0) = \begin{bmatrix} 1 & 0 \end{bmatrix} B u(0|0) = 0.5 \cdot a(0)$.
    So, $0 \le 0.5 a(0) \le 11$.

    $p(2|0) = \begin{bmatrix} 1 & 0 \end{bmatrix} (AB u(0|0) + B u(1|0)) = 1.5 \cdot a(0) + 0.5 \cdot a(1)$.
    So, $0 \le 1.5 a(0) + 0.5 a(1) \le 11$.

    $p(3|0) = \begin{bmatrix} 1 & 0 \end{bmatrix} (A^2B u(0|0) + AB u(1|0)) = 2.5 \cdot a(0) + 1.5 \cdot a(1)$.
    So, $0 \le 2.5 a(0) + 1.5 a(1) \le 11$.

You would formulate all of these into the final $\mathbf{L}U_k \le \mathbf{W}$ matrix.

### 5. Solve the QP:

At time $k=0$, a QP solver (like `quadprog` in MATLAB, or `cvxopt` in Python) is given $H, G, L, W$ (and $x(0)$ is embedded in $G$ and $W$).
The solver would return the optimal $U_0^*=\begin{bmatrix} a^*(0) \\ a^*(1) \end{bmatrix}$.

**Example Output (Illustrative - not numerically solved):**
Let's assume the solver returns:
$a^*(0)=1.0$ (accelerate to get moving)
$a^*(1)=0.8$ (continue accelerating)

### 6. Apply First Control Action:

The car's controller applies only the first element: $a_{applied}(0)=a^*(0)=1.0$.

### 7. Move to Next Timestep ($k=1$):

The car's actual state is measured: $x(1)$. Using $a_{applied}(0)=1.0$:
$p(1)=p(0)+v(0)+\frac{1}{2}a(0)=0+0+\frac{1}{2}(1.0)=0.5$
$v(1)=v(0)+a(0)=0+1.0=1.0$
So, $x(1)=\begin{bmatrix} 0.5 \\ 1.0 \end{bmatrix}$

The entire process repeats:
* MPC takes $x(1)$ as its new current state.
* It generates a new reference trajectory for $k=1,2,3$ (e.g., $x_{ref}(2), x_{ref}(3), x_{ref}(4)$ from the path).
* It re-formulates a new QP with updated $G$ and $W$ matrices (as they depend on $x(1)$).
* It solves the QP for a new optimal control sequence $U_1^*=\begin{bmatrix} a^*(1) \\ a^*(2) \end{bmatrix}$.
* Only $a^*(1)$ from this new sequence is applied.