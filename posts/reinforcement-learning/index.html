<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dinh Tien Thang">
<meta name="dcterms.date" content="2025-06-25">

<title>Reinforcement Learning – haha</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-db03927a41f77a8af5287a812d7101f4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">haha</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement Learning</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">machine-learning</div>
                <div class="quarto-category">RL</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dinh Tien Thang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 25, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#rl-environment-setup" id="toc-rl-environment-setup" class="nav-link active" data-scroll-target="#rl-environment-setup">RL Environment Setup</a>
  <ul class="collapse">
  <li><a href="#markov-decision-process-mdp" id="toc-markov-decision-process-mdp" class="nav-link" data-scroll-target="#markov-decision-process-mdp">Markov Decision Process (MDP)</a>
  <ul class="collapse">
  <li><a href="#markov-property" id="toc-markov-property" class="nav-link" data-scroll-target="#markov-property">Markov Property</a></li>
  </ul></li>
  <li><a href="#goal-of-reinforcement-learning" id="toc-goal-of-reinforcement-learning" class="nav-link" data-scroll-target="#goal-of-reinforcement-learning">Goal of Reinforcement Learning</a>
  <ul class="collapse">
  <li><a href="#policy" id="toc-policy" class="nav-link" data-scroll-target="#policy">Policy</a></li>
  <li><a href="#return" id="toc-return" class="nav-link" data-scroll-target="#return">Return</a></li>
  <li><a href="#goal" id="toc-goal" class="nav-link" data-scroll-target="#goal">Goal</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#world-models-vs.-model-free-learning" id="toc-world-models-vs.-model-free-learning" class="nav-link" data-scroll-target="#world-models-vs.-model-free-learning">World Models vs.&nbsp;Model-Free Learning</a></li>
  <li><a href="#monte-carlo-vs.-temporal-difference" id="toc-monte-carlo-vs.-temporal-difference" class="nav-link" data-scroll-target="#monte-carlo-vs.-temporal-difference">Monte Carlo vs.&nbsp;Temporal Difference</a>
  <ul class="collapse">
  <li><a href="#value-function" id="toc-value-function" class="nav-link" data-scroll-target="#value-function">Value Function</a></li>
  </ul></li>
  <li><a href="#policy-gradient-methods" id="toc-policy-gradient-methods" class="nav-link" data-scroll-target="#policy-gradient-methods">Policy Gradient Methods</a></li>
  <li><a href="#q-learning-variants" id="toc-q-learning-variants" class="nav-link" data-scroll-target="#q-learning-variants">Q-Learning Variants</a>
  <ul class="collapse">
  <li><a href="#sarsa-on-policy" id="toc-sarsa-on-policy" class="nav-link" data-scroll-target="#sarsa-on-policy">SARSA (On-policy)</a></li>
  <li><a href="#q-learning-off-policy" id="toc-q-learning-off-policy" class="nav-link" data-scroll-target="#q-learning-off-policy">Q-Learning (Off-policy)</a></li>
  </ul></li>
  <li><a href="#bellman-optimality-equation" id="toc-bellman-optimality-equation" class="nav-link" data-scroll-target="#bellman-optimality-equation">Bellman Optimality Equation</a></li>
  <li><a href="#exploration-vs.-exploitation" id="toc-exploration-vs.-exploitation" class="nav-link" data-scroll-target="#exploration-vs.-exploitation">Exploration vs.&nbsp;Exploitation</a></li>
  <li><a href="#sample-efficiency" id="toc-sample-efficiency" class="nav-link" data-scroll-target="#sample-efficiency">Sample Efficiency</a>
  <ul class="collapse">
  <li><a href="#credit-assignment-problem" id="toc-credit-assignment-problem" class="nav-link" data-scroll-target="#credit-assignment-problem">Credit assignment problem</a></li>
  </ul></li>
  <li><a href="#summary-of-value-updates" id="toc-summary-of-value-updates" class="nav-link" data-scroll-target="#summary-of-value-updates">Summary of Value Updates</a></li>
  <li><a href="#on-policy-vs.-off-policy" id="toc-on-policy-vs.-off-policy" class="nav-link" data-scroll-target="#on-policy-vs.-off-policy">On-Policy vs.&nbsp;Off-Policy</a>
  <ul class="collapse">
  <li><a href="#q-learning" id="toc-q-learning" class="nav-link" data-scroll-target="#q-learning">Q-Learning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Reinforcement Learning (RL) is a subfield of machine learning concerned with how agents ought to take actions in an environment in order to maximize cumulative reward.</p>
<section id="rl-environment-setup" class="level1">
<h1>RL Environment Setup</h1>
<section id="markov-decision-process-mdp" class="level2">
<h2 class="anchored" data-anchor-id="markov-decision-process-mdp">Markov Decision Process (MDP)</h2>
<p>An agent’s interaction with the environment is usually modeled as a <strong>Markov Decision Process (MDP)</strong>:</p>
<pre><code>s₀, a₀, r₀, s₁, a₁, r₁, s₂, a₂, r₂, ...</code></pre>
<p>This sequence is called an <strong>episode</strong>, and it may or may not terminate depending on the task.</p>
<section id="markov-property" class="level3">
<h3 class="anchored" data-anchor-id="markov-property">Markov Property</h3>
<blockquote class="blockquote">
<p>The probability of the next state depends <strong>only</strong> on the current state and action — not the full history: <span class="math display">\[
P(s_{t+1} \mid s_t, a_t) = P(s_{t+1} \mid s_1, a_1, ..., s_t, a_t)
\]</span></p>
</blockquote>
</section>
</section>
<section id="goal-of-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="goal-of-reinforcement-learning">Goal of Reinforcement Learning</h2>
<p>The goal is to learn a <strong>policy</strong> <span class="math inline">\(\pi\)</span> that tells the agent which action to take in each state in order to <strong>maximize cumulative reward</strong>.</p>
<p>This is often visualized as:</p>
<pre><code>s₀, a₀, r₀ → (s₁, a₁, r₁) → s₂, a₂, r₂ → ...</code></pre>
<p>We define the <strong>return</strong> <span class="math inline">\(G_t\)</span> as the total discounted reward:</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(t \in \{0, 1, 2, \dots\}\)</span></li>
<li><span class="math inline">\(s \in \mathcal{S}\)</span>: a state</li>
<li><span class="math inline">\(a \in \mathcal{A}(s)\)</span>: an action available in state <span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(r \in \mathcal{R} \subseteq \mathbb{R}\)</span>: a scalar reward</li>
</ul>
<p>The <strong>environment dynamics</strong> (transition model) are given by:</p>
<p><span class="math display">\[
p(s', r \mid s, a) = \text{Prob}(S_{t+1} = s',\ R_{t+1} = r \mid S_t = s,\ A_t = a)
\]</span></p>
<hr>
<section id="policy" class="level3">
<h3 class="anchored" data-anchor-id="policy">Policy</h3>
<ul>
<li>Stochastic: <span class="math inline">\(\pi(a \mid s)\)</span></li>
<li>Deterministic: <span class="math inline">\(a = \pi(s)\)</span></li>
</ul>
<hr>
</section>
<section id="return" class="level3">
<h3 class="anchored" data-anchor-id="return">Return</h3>
<p>The return <span class="math display">\[ G_t \]</span> is the total discounted reward from time $ t+1 $ to final time $ T $:</p>
<p><span class="math display">\[
G_t = \sum_{k = t+1}^{T} \gamma^{k - t - 1} R_k
\]</span></p>
<p>Expanded:</p>
<p><span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T - t - 1} R_T
\]</span></p>
</section>
<section id="goal" class="level3">
<h3 class="anchored" data-anchor-id="goal">Goal</h3>
<p><span class="math display">\[
\max_\pi \, \mathbb{E}_\pi \left[ G_t \right]
\]</span></p>
<p>How to determine a policy that will accumulate a lot of rewards? # Value Functions</p>
<p>Used to estimate expected returns:</p>
<ul>
<li><strong>State-value function</strong>: <span class="math inline">\(V_\pi(s) = \mathbb{E}_\pi[G_t \mid s_t = s]\)</span></li>
<li><strong>Action-value function</strong>: <span class="math inline">\(Q_\pi(s, a) = \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]\)</span></li>
</ul>
<p>Relationship: <span class="math inline">\(V_\pi(s) = \sum_a \pi(a \mid s) Q_\pi(s, a)\)</span></p>
</section>
</section>
</section>
<section id="world-models-vs.-model-free-learning" class="level1">
<h1>World Models vs.&nbsp;Model-Free Learning</h1>
<p>A <strong>world model</strong> gives access to <span class="math inline">\(p(s', r \mid s, a)\)</span> — but in practice we often don’t have it. Instead:</p>
<ol type="1">
<li>Start with a random policy: <span class="math inline">\(\pi(a \mid s) = \frac{1}{|A|}\)</span></li>
<li>Collect trajectories and estimate returns <span class="math inline">\(G_t\)</span></li>
<li>Use <strong>Monte Carlo</strong> methods to update estimates: calculate all return Gt backward from an episode Within a specific trajectory: <span class="math display">\[
G_t = r_t + \gamma G_{t+1}
\]</span></li>
</ol>
<p>FOr <strong>every</strong> episode (trjectory) update the return Gt <strong>BACKWARD</strong> <span class="math display">\[
G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots
\]</span></p>
</section>
<section id="monte-carlo-vs.-temporal-difference" class="level1">
<h1>Monte Carlo vs.&nbsp;Temporal Difference</h1>
<p><strong>Monte Carlo</strong>:</p>
<ul>
<li>Updates happen after entire episodes</li>
<li>Can’t assign credit to individual steps within an episode</li>
<li><strong>Given a (state, action), What is my return Gt?</strong></li>
<li>we get good episodes and bad episodes</li>
<li>good episode: all actions evaluated better</li>
<li>bad episode: all actions evaluated worse</li>
<li>have to wait for episode to end to wait for the return to adjust evaluation for actions</li>
<li>within an episode, we dont know what subpath made the episode bad or good</li>
</ul>
<p>=&gt; can only rely on large number of samplings</p>
<p><strong>Temporal Difference (TD)</strong>:</p>
<ul>
<li>Updates happen at each time step</li>
<li>More efficient and faster convergence</li>
<li>action by action based (instead of episode by episode based)</li>
<li>For each (state,action) compare rewards difference with (state+1, action+1)</li>
</ul>
<section id="value-function" class="level2">
<h2 class="anchored" data-anchor-id="value-function">Value Function</h2>
<p>keep track of the average return Gt expected when following a certain policy pi at a state (s) or (state, action) (s,a) - state value: V_pi(s) - action value: Q_pi(s,a) - actually V_pi(s) = Expected[Q_pi(a,s)]</p>
<p>Goal: best case scenario (optimal) V<em>(s), Q</em>(s,a)</p>
</section>
</section>
<section id="policy-gradient-methods" class="level1">
<h1>Policy Gradient Methods</h1>
<p>Learn policy directly, optionally with a value function baseline:</p>
<ul>
<li>Without baseline → high variance (different average like a state with average return G1 of 100 and change +-5 are not the same with another state with average return G2 of 10 that also +-5)</li>
<li>With baseline (e.g., <span class="math inline">\(V_\pi(s)\)</span>) → reduced variance</li>
</ul>
</section>
<section id="q-learning-variants" class="level1">
<h1>Q-Learning Variants</h1>
<ul>
<li>not have to worry about policy function, just update Q(s,a) from the table and trajectory and we are fine!</li>
</ul>
<p>for each trajectory: for each state instance: Q(s,a) = Q(s,a) + learning rate * return G(t)</p>
<section id="sarsa-on-policy" class="level2">
<h2 class="anchored" data-anchor-id="sarsa-on-policy">SARSA (On-policy)</h2>
<p><strong>Given a (state, action), how much better or worse is my return Gt relative to the new state and the SPECIFIC action that I ended up taking?</strong></p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\]</span> BUT: there are many actions, not just one action a+1 so what action to take? We do not have access to p(s’,a’|s,a) so we do not know But we can calculate what we can expect in the next state after we take an action a ## Expected SARSA <strong>Given a (state, action), how much better or worse is my return Gt relative to the new state withh ALL actions AVERAGED OUT acording to policy probability pi?</strong> - we can calculate all the possible action at the next state and take the expected average with the policy probability (expected SARSA) <span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \sum_a \pi(a \mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \right]
\]</span></p>
</section>
<section id="q-learning-off-policy" class="level2">
<h2 class="anchored" data-anchor-id="q-learning-off-policy">Q-Learning (Off-policy)</h2>
<p><strong>Given a (state, action), how much better or worse is my return Gt relative to the new state with the best action?</strong> <span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
\]</span></p>
</section>
</section>
<section id="bellman-optimality-equation" class="level1">
<h1>Bellman Optimality Equation</h1>
<p>Describes optimal Q-value function:</p>
<p><span class="math display">\[
Q_*(s_t, a_t) = \mathbb{E} \left[ r_t + \gamma \max_a Q_*(s_{t+1}, a) \right]
\]</span></p>
<p>if a whole sequence is optimal, every subsequence must also be optimal, thats why Bellman add the max of the subsequence <em>a Q</em>*(s_{t+1}, a) to ensure this suboptimality</p>
</section>
<section id="exploration-vs.-exploitation" class="level1">
<h1>Exploration vs.&nbsp;Exploitation</h1>
<ul>
<li><strong>Exploration</strong>: Random actions (e.g., <span class="math inline">\(\epsilon\)</span>-greedy with <span class="math inline">\(\epsilon &gt; 0\)</span>)</li>
<li><strong>Exploitation</strong>: Choose action that maximizes <span class="math inline">\(Q(s, a)\)</span></li>
</ul>
<p>Balance is essential during training.</p>
</section>
<section id="sample-efficiency" class="level1">
<h1>Sample Efficiency</h1>
<p>Refers to how quickly a method converges given the number of samples (episodes or steps).</p>
<section id="credit-assignment-problem" class="level2">
<h2 class="anchored" data-anchor-id="credit-assignment-problem">Credit assignment problem</h2>
<p>within an episode, figure out what subpath made the episode bad or good a.k.a individual action impact on the episode =&gt;</p>
</section>
</section>
<section id="summary-of-value-updates" class="level1">
<h1>Summary of Value Updates</h1>
<p>Learning action-value (Q) function, notice that we only need one time step!!! no need to add up the whole trajectory: <strong>Learning Q-values</strong>:</p>
<ul>
<li><p>SARSA:</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\]</span></p></li>
<li><p>Expected SARSA:</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \sum_a \pi(a \mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \right]
\]</span></p></li>
<li><p>Q-Learning:</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
\]</span></p></li>
</ul>
<p><strong>Learning V-values</strong>:</p>
<p><span class="math display">\[
V(s_t) \leftarrow V(s_t) + \alpha \left[ r_t + \gamma V(s_{t+1}) - V(s_t) \right]
\]</span></p>
<p>Where <span class="math inline">\(\alpha\)</span> is the learning rate.</p>
</section>
<section id="on-policy-vs.-off-policy" class="level1">
<h1>On-Policy vs.&nbsp;Off-Policy</h1>
<ul>
<li><strong>On-policy</strong>: Behavior and target policy are the same (SARSA), because they actually regarding what action they will be taking next, and use that to improve the pi, and the cycle again Qpi-&gt;Q*</li>
<li><strong>Off-policy</strong>: Behavior and target policy differ (Q-Learning). Q-Learning just evaluate based on the best possible action could be taking =&gt; no longer dependent on policy. It still use pi for policy and gather experiences, but it does not evaluate pi, it instead evaluate optimal policy directly</li>
</ul>
<p>Q-Learning learns the optimal policy <span class="math inline">\(\pi^*\)</span>, regardless of current behavior policy.</p>
<section id="q-learning" class="level2">
<h2 class="anchored" data-anchor-id="q-learning">Q-Learning</h2>
<p>Develop a function <span class="math display">\[ Q \]</span> to approximate <span class="math display">\[ Q_* \]</span>, by updating:</p>
<p><span class="math display">\[
Q(s_t, a_t) \rightarrow r_t + \gamma \max_a Q(s_{t+1}, a)
\]</span></p>
<p>Remember, the environment is <strong>random</strong> (many possible next states/rewards).<br>
According to the <strong>world model</strong>:</p>
<p><span class="math display">\[
p(s', r \mid s, a)
\]</span></p>
<p><em>(You don’t have access to this model directly.)</em></p>
<p>Therefore, We do Q-learning with enough sample trajectories, we can average them out Equation that describes what <span class="math display">\[ Q_* \]</span> is actually supposed to be: Result of samples averaging out <span class="math display">\[
Q_*(s_t, a_t) = \mathbb{E} \left[ r_t + \gamma \max_a Q_*(s_{t+1}, a) \right]
\]</span></p>
<blockquote class="blockquote">
<p><em>Note:</em> <span class="math display">\[ \mathbb{E}[\cdot] \]</span> = expected value = weighted average according to probabilities</p>
</blockquote>
<p>=&gt; That means, eventhough we do not have world model, when we do sampling, we still can gather enough information near to it</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/tienthangdinh\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>