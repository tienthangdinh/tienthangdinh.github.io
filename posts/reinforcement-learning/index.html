<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dinh Tien Thang">
<meta name="dcterms.date" content="2025-06-25">

<title>Reinforcement Learning – haha</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-db03927a41f77a8af5287a812d7101f4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">haha</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement Learning</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">machine-learning</div>
                <div class="quarto-category">RL</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dinh Tien Thang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 25, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#markov-decision-process-mdp" id="toc-markov-decision-process-mdp" class="nav-link active" data-scroll-target="#markov-decision-process-mdp">Markov Decision Process (MDP)</a>
  <ul class="collapse">
  <li><a href="#markov-property" id="toc-markov-property" class="nav-link" data-scroll-target="#markov-property">Markov Property</a></li>
  <li><a href="#policy" id="toc-policy" class="nav-link" data-scroll-target="#policy">Policy</a></li>
  <li><a href="#return" id="toc-return" class="nav-link" data-scroll-target="#return">Return</a></li>
  <li><a href="#goal" id="toc-goal" class="nav-link" data-scroll-target="#goal">Goal</a></li>
  </ul></li>
  <li><a href="#model-based-learning" id="toc-model-based-learning" class="nav-link" data-scroll-target="#model-based-learning">Model-based Learning</a>
  <ul class="collapse">
  <li><a href="#value-functions" id="toc-value-functions" class="nav-link" data-scroll-target="#value-functions">Value Functions</a></li>
  <li><a href="#the-bellman-equations" id="toc-the-bellman-equations" class="nav-link" data-scroll-target="#the-bellman-equations">The Bellman Equations</a>
  <ul class="collapse">
  <li><a href="#bellman-equation-for-v_pis" id="toc-bellman-equation-for-v_pis" class="nav-link" data-scroll-target="#bellman-equation-for-v_pis">Bellman Equation for <span class="math inline">\(v_\pi(s)\)</span></a></li>
  <li><a href="#bellman-equation-for-q_pis-a" id="toc-bellman-equation-for-q_pis-a" class="nav-link" data-scroll-target="#bellman-equation-for-q_pis-a">Bellman Equation for <span class="math inline">\(q_\pi(s, a)\)</span></a></li>
  <li><a href="#optimal-bellman" id="toc-optimal-bellman" class="nav-link" data-scroll-target="#optimal-bellman">Optimal Bellman</a></li>
  </ul></li>
  <li><a href="#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement" id="toc-generalized-policy-iteration-gpi-policy-evaluation-policy-improvement" class="nav-link" data-scroll-target="#generalized-policy-iteration-gpi-policy-evaluation-policy-improvement">Generalized Policy Iteration (GPI): Policy Evaluation &amp; Policy Improvement</a></li>
  </ul></li>
  <li><a href="#model-free-learning" id="toc-model-free-learning" class="nav-link" data-scroll-target="#model-free-learning">Model-Free Learning</a>
  <ul class="collapse">
  <li><a href="#monte-carlo-state-value" id="toc-monte-carlo-state-value" class="nav-link" data-scroll-target="#monte-carlo-state-value">Monte Carlo (State Value)</a></li>
  <li><a href="#monte-carlo-action-value" id="toc-monte-carlo-action-value" class="nav-link" data-scroll-target="#monte-carlo-action-value">Monte Carlo (Action Value)</a></li>
  <li><a href="#sum-up-constant-α-mc-algorithm-pi-approx-pi" id="toc-sum-up-constant-α-mc-algorithm-pi-approx-pi" class="nav-link" data-scroll-target="#sum-up-constant-α-mc-algorithm-pi-approx-pi">Sum up: Constant-α MC Algorithm <span class="math inline">\(\pi \approx \pi^*\)</span></a></li>
  <li><a href="#off-policy" id="toc-off-policy" class="nav-link" data-scroll-target="#off-policy">Off-Policy</a>
  <ul class="collapse">
  <li><a href="#relationship-between-sampling-policy-b-vs-main-behavior-policy-pi" id="toc-relationship-between-sampling-policy-b-vs-main-behavior-policy-pi" class="nav-link" data-scroll-target="#relationship-between-sampling-policy-b-vs-main-behavior-policy-pi">Relationship between sampling policy <span class="math inline">\(b\)</span> vs main behavior policy <span class="math inline">\(\pi\)</span>?</a></li>
  </ul></li>
  <li><a href="#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi" id="toc-sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi" class="nav-link" data-scroll-target="#sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi">Sum up: Off-Policy Constant-<span class="math inline">\(\alpha\)</span> MC for <span class="math inline">\(\pi \approx \pi^*\)</span></a></li>
  </ul></li>
  <li><a href="#compare-model-based-vs-model-free" id="toc-compare-model-based-vs-model-free" class="nav-link" data-scroll-target="#compare-model-based-vs-model-free">Compare: Model-Based vs Model-Free</a>
  <ul class="collapse">
  <li><a href="#q-why-do-model-free-methods-not-learn-the-environments-transition-probabilities-psrsa-why-do-they-learn-within-a-trajectory-instead-of-interacting-with-neighboring-states" id="toc-q-why-do-model-free-methods-not-learn-the-environments-transition-probabilities-psrsa-why-do-they-learn-within-a-trajectory-instead-of-interacting-with-neighboring-states" class="nav-link" data-scroll-target="#q-why-do-model-free-methods-not-learn-the-environments-transition-probabilities-psrsa-why-do-they-learn-within-a-trajectory-instead-of-interacting-with-neighboring-states">Q: Why do model-free methods not learn the environment’s transition probabilities <span class="math inline">\(p(s',r|s,a)\)</span>? Why do they learn “within a trajectory” instead of interacting with neighboring states?</a></li>
  </ul></li>
  <li><a href="#temporal-difference-still-a-model-free-without-p" id="toc-temporal-difference-still-a-model-free-without-p" class="nav-link" data-scroll-target="#temporal-difference-still-a-model-free-without-p">Temporal Difference (still a Model-free without p)</a>
  <ul class="collapse">
  <li><a href="#n-step-temporal-difference-learning" id="toc-n-step-temporal-difference-learning" class="nav-link" data-scroll-target="#n-step-temporal-difference-learning">n-step Temporal Difference Learning</a></li>
  <li><a href="#why-is-td-better" id="toc-why-is-td-better" class="nav-link" data-scroll-target="#why-is-td-better">Why is TD better?</a></li>
  <li><a href="#what-does-larger-n-means" id="toc-what-does-larger-n-means" class="nav-link" data-scroll-target="#what-does-larger-n-means">What does larger n means?</a></li>
  <li><a href="#sum-up-on-policy-temporal-difference-n-step-sarsa" id="toc-sum-up-on-policy-temporal-difference-n-step-sarsa" class="nav-link" data-scroll-target="#sum-up-on-policy-temporal-difference-n-step-sarsa">Sum up: On-Policy Temporal Difference: n-step Sarsa</a></li>
  <li><a href="#q-learning" id="toc-q-learning" class="nav-link" data-scroll-target="#q-learning">Q-Learning</a></li>
  <li><a href="#expected-sarsa" id="toc-expected-sarsa" class="nav-link" data-scroll-target="#expected-sarsa">Expected Sarsa</a></li>
  <li><a href="#compare" id="toc-compare" class="nav-link" data-scroll-target="#compare">Compare</a></li>
  <li><a href="#summary-of-td" id="toc-summary-of-td" class="nav-link" data-scroll-target="#summary-of-td">Summary of TD</a></li>
  </ul></li>
  <li><a href="#function-approximation" id="toc-function-approximation" class="nav-link" data-scroll-target="#function-approximation">Function Approximation</a>
  <ul class="collapse">
  <li><a href="#on-policy-evaluation-with-function-approximation" id="toc-on-policy-evaluation-with-function-approximation" class="nav-link" data-scroll-target="#on-policy-evaluation-with-function-approximation">On-Policy Evaluation with Function Approximation</a></li>
  <li><a href="#goal-how-to-get-mathbfw" id="toc-goal-how-to-get-mathbfw" class="nav-link" data-scroll-target="#goal-how-to-get-mathbfw">Goal: How to get <span class="math inline">\(\mathbf{w}\)</span>?</a></li>
  <li><a href="#how-to-obtain-the-target-u_t" id="toc-how-to-obtain-the-target-u_t" class="nav-link" data-scroll-target="#how-to-obtain-the-target-u_t">How to Obtain the Target <span class="math inline">\(U_t\)</span></a></li>
  <li><a href="#extension-to-on-policy-control-with-function-approximation" id="toc-extension-to-on-policy-control-with-function-approximation" class="nav-link" data-scroll-target="#extension-to-on-policy-control-with-function-approximation">Extension to On-Policy Control with Function Approximation</a></li>
  <li><a href="#extension-to-on-policy-control-with-function-approximation-1" id="toc-extension-to-on-policy-control-with-function-approximation-1" class="nav-link" data-scroll-target="#extension-to-on-policy-control-with-function-approximation-1">Extension to On-Policy Control with Function Approximation</a></li>
  </ul></li>
  <li><a href="#policy-gradient-methods" id="toc-policy-gradient-methods" class="nav-link" data-scroll-target="#policy-gradient-methods">Policy Gradient Methods</a>
  <ul class="collapse">
  <li><a href="#a-monte-carlo-style-policy-gradient-algorithm" id="toc-a-monte-carlo-style-policy-gradient-algorithm" class="nav-link" data-scroll-target="#a-monte-carlo-style-policy-gradient-algorithm">A Monte Carlo Style Policy Gradient Algorithm</a></li>
  <li><a href="#credit-assignment-problem" id="toc-credit-assignment-problem" class="nav-link" data-scroll-target="#credit-assignment-problem">Credit assignment problem</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Reinforcement Learning (RL) is a subfield of machine learning concerned with how agents ought to take actions in an environment in order to maximize cumulative reward.</p>
<section id="markov-decision-process-mdp" class="level1">
<h1>Markov Decision Process (MDP)</h1>
<p>An agent’s interaction with the environment is usually modeled as a <strong>Markov Decision Process (MDP)</strong>:</p>
<p>s₀, a₀, r₀ → (s₁, a₁, <strong>R₁</strong>) → s₂, a₂, <strong>R₂</strong> → …</p>
<p>This sequence is called an <strong>episode</strong>, and it may or may not terminate depending on the task.</p>
<p>Let:</p>
<ul>
<li><span class="math inline">\(t \in \{0, 1, 2, \dots\}\)</span></li>
<li><span class="math inline">\(s \in \mathcal{S}\)</span>: a state</li>
<li><span class="math inline">\(a \in \mathcal{A}(s)\)</span>: an action available in state <span class="math inline">\(s\)</span></li>
<li><span class="math inline">\(r \in \mathcal{R} \subseteq \mathbb{R}\)</span>: a scalar reward</li>
</ul>
<p>The <strong>environment dynamics</strong> (transition model) are given by:</p>
<p><span class="math display">\[
p(s', r \mid s, a) = \text{Prob}(S_{t+1} = s',\ R_{t+1} = r \mid S_t = s,\ A_t = a)
\]</span></p>
<section id="markov-property" class="level3">
<h3 class="anchored" data-anchor-id="markov-property">Markov Property</h3>
<blockquote class="blockquote">
<p>The probability of the next state depends <strong>only</strong> on the current state and action — not the full history: <span class="math display">\[
P(s_{t+1} \mid s_t, a_t) = P(s_{t+1} \mid s_1, a_1, ..., s_t, a_t)
\]</span></p>
</blockquote>
</section>
<section id="policy" class="level3">
<h3 class="anchored" data-anchor-id="policy">Policy</h3>
<ul>
<li>Stochastic: <span class="math inline">\(\pi(a \mid s)\)</span></li>
<li>Deterministic: <span class="math inline">\(a = \pi(s)\)</span></li>
</ul>
</section>
<section id="return" class="level3">
<h3 class="anchored" data-anchor-id="return">Return</h3>
<p>The return <span class="math inline">\(G_t\)</span> is the total discounted reward from time <span class="math inline">\(t+1\)</span> to final time <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[
G_t = \sum_{k = t+1}^{T} \gamma^{k - t - 1} R_k
\]</span></p>
<p>Expanded:</p>
<p><span class="math display">\[
G_t = R_{t+1} + \gamma G_{t+1}
\]</span></p>
<p><span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T - t - 1} R_T
\]</span></p>
</section>
<section id="goal" class="level3">
<h3 class="anchored" data-anchor-id="goal">Goal</h3>
<p><span class="math display">\[
\max_\pi \, \mathbb{E}_\pi \left[ G_t \right]
\]</span></p>
<p>How can we determine a policy that accumulates a high reward?</p>
</section>
</section>
<section id="model-based-learning" class="level1">
<h1>Model-based Learning</h1>
<p><strong>world model</strong> gives access to <span class="math inline">\(p(s', r \mid s, a)\)</span>, so we will use them for the estimation below</p>
<section id="value-functions" class="level2">
<h2 class="anchored" data-anchor-id="value-functions">Value Functions</h2>
<p>Very similar to return <span class="math inline">\(\max_\pi \, \mathbb{E}_\pi \left[ G_t \right]\)</span>.</p>
<p>Value functions are used to estimate expected returns:</p>
<ul>
<li><strong>State-value function</strong>: <span class="math inline">\(V_\pi(s_t) = \mathbb{E}_\pi[G_t \mid s_t = s]\)</span></li>
<li><strong>Action-value function</strong>: <span class="math inline">\(Q_\pi(s_t, a_t) = \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]\)</span></li>
</ul>
<p>Relationship: <span class="math inline">\(V_\pi(s) = \sum_a \pi(a \mid s) Q_\pi(s, a)\)</span> and <span class="math inline">\(V^*(s) = \arg\max_a Q^*(s, a)\)</span></p>
<p>Derivation of this relationship:</p>
<p><span class="math inline">\(V_\pi(s) = \sum_a \pi(a \mid s) Q_\pi(s, a)\)</span> (semantically true because)</p>
<p><span class="math inline">\(V_\pi(s) = \sum_a \pi(a \mid s) \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]\)</span></p>
<p><span class="math inline">\(V_\pi(s) = \sum_a \pi(a \mid s) \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} \mid s_t = s, a_t = a]\)</span> (because <span class="math inline">\(\mathbb{E}[G_{t+1} \mid s, a] = \mathbb{E}[V_\pi(S_{t+1}) \mid s, a]\)</span>)</p>
<p><span class="math inline">\(V_\pi(s) = \sum_a \pi(a \mid s) \mathbb{E}_\pi[R_{t+1} + \gamma V_\pi(S_{t+1}) \mid s_t = s, a_t = a]\)</span></p>
<p><span class="math inline">\(V_\pi(s) = \sum_a \pi(a \mid s) \sum_{s',r} p(s',r \mid s,a)[r + \gamma V_\pi(s')]\)</span></p>
</section>
<section id="the-bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="the-bellman-equations">The Bellman Equations</h2>
<p>For <em>any</em> policy <span class="math inline">\(\pi\)</span>, all <span class="math inline">\(s \in \mathcal{S}\)</span>, and all <span class="math inline">\(a \in \mathcal{A}(s)\)</span>:</p>
<p><strong>State-value function:</strong></p>
<p><span class="math display">\[
v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a \mid s) \, q_\pi(s, a) \tag{1}
\]</span></p>
<p><strong>Action-value function:</strong></p>
<p><span class="math display">\[
q_\pi(s, a) = \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) \left[ r + \gamma v_\pi(s') \right] \tag{2}
\]</span></p>
<p>After substitution as derived above:</p>
<section id="bellman-equation-for-v_pis" class="level3">
<h3 class="anchored" data-anchor-id="bellman-equation-for-v_pis">Bellman Equation for <span class="math inline">\(v_\pi(s)\)</span></h3>
<p><span class="math display">\[
v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a \mid s) \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) \left[ r + \gamma v_\pi(s') \right]
\]</span></p>
</section>
<section id="bellman-equation-for-q_pis-a" class="level3">
<h3 class="anchored" data-anchor-id="bellman-equation-for-q_pis-a">Bellman Equation for <span class="math inline">\(q_\pi(s, a)\)</span></h3>
<p><span class="math display">\[
q_\pi(s, a) = \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) \left[ r + \gamma \sum_{a' \in \mathcal{A}(s')} \pi(a' \mid s') q_\pi(s', a') \right]
\]</span></p>
</section>
<section id="optimal-bellman" class="level3">
<h3 class="anchored" data-anchor-id="optimal-bellman">Optimal Bellman</h3>
<p><span class="math inline">\(V^*(s) = \arg\max_a Q^*(s, a)\)</span></p>
</section>
</section>
<section id="generalized-policy-iteration-gpi-policy-evaluation-policy-improvement" class="level2">
<h2 class="anchored" data-anchor-id="generalized-policy-iteration-gpi-policy-evaluation-policy-improvement">Generalized Policy Iteration (GPI): Policy Evaluation &amp; Policy Improvement</h2>
<p>Using the Bellman Equation above and an initial estimate (e.g.&nbsp;all states are randomly initialized or uniformly negative, except the terminal state which is 0), we iteratively update:</p>
<p>We apply the Bellman equations above for every single state (for V), or (state, action) pair (for Q, which forms a finer table). Over many runs, it will slowly converge. <span class="math display">\[
\pi'(s) = \arg\max_a q_\pi(s, a)
\]</span> Where: <span class="math display">\[
q_\pi(s, a) = \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) \left[ r + \gamma v_\pi(s') \right]
\]</span></p>
</section>
</section>
<section id="model-free-learning" class="level1">
<h1>Model-Free Learning</h1>
<p>Until now, you can see we have been using <span class="math inline">\(p(s', r \mid s, a)\)</span> to update our policy, because a <strong>world model</strong> gives access to this — but in practice - we often don’t have it. So instead we use <strong>Monte Carlo</strong> to update <span class="math inline">\(q_\pi(s, a)\)</span> instead of <span class="math inline">\(v_\pi(s)\)</span>, because in the end we still have to choose action policy, in which we still have to calculate <span class="math inline">\(q_\pi(s,a)\)</span>, which is equivalent to this one from above, just that we <strong>dont calculate this this way</strong>, we have to estimate it stochastically somehow: <span class="math display">\[
q_\pi(s, a) = \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) \left[ r + \gamma v_\pi(s') \right]
\]</span></p>
<p>The only difference is that, we have to construct <span class="math inline">\(p(s', r \mid s, a)\)</span> ourselves from experience from sampling.</p>
<p>The next question is, how to do it?</p>
<section id="monte-carlo-state-value" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-state-value">Monte Carlo (State Value)</h2>
<p><strong>Goal</strong>: Given samples under <span class="math inline">\(\pi\)</span>, estimate <span class="math inline">\(q_\pi\)</span>.</p>
<blockquote class="blockquote">
<p><strong>We can express <span class="math inline">\(q_\pi\)</span>-estimation as <span class="math inline">\(v_\pi\)</span>-estimation.</strong> Imagine a new problem where: <span class="math display">\[
S_t^{\text{new}} = (S_t, A_t)
\]</span></p>
</blockquote>
<p>Any evaluation algorithm estimating <span class="math inline">\(v(s^{\text{new}})\)</span> would be estimating <span class="math inline">\(q_\pi(s, a)\)</span>.</p>
<p>So basically what happening is that we do not have a policy anymore, instead, policy is now a part of the environment. But why do we want to do that? Because: - This is a generalization anyway, something applied to state value function V, also applied for action value function Q. - It simplifies our analysis, <strong>reduces</strong> the problem to a <strong>simpler problem</strong> Markov Reward Process (MRP), which is MDP without actions (s1,r1,s2,r2,s3,r3,…)</p>
<p>OK, but still, how to do it?</p>
<p><strong>Start</strong> with a random value function: <span class="math inline">\(V(s) = \frac{1}{|S|}\)</span></p>
<p><strong>Collect sampling trajectories</strong> <span class="math inline">\(M\)</span> trajectory samples: <span class="math display">\[
s_0^m \quad r_1^m \quad s_1^m \quad \cdots \quad s_{T_m}^m \qquad m = 1, \ldots, M
\]</span></p>
<p><strong>The Goal</strong>: Use <em>averages</em> to approximate <span class="math inline">\(v_\pi(s)\)</span>: <span class="math display">\[
v_\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s] \approx \frac{1}{C(s)} \sum_{m=1}^M \sum_{\tau=0}^{T_m - 1} \mathbb{I}[s_\tau^m = s] \, g_\tau^m
\]</span> where: <span class="math display">\[
\mathbb{I}[s_\tau^m = s] =
\begin{cases}
1 &amp; \text{if } s_\tau^m = s \\
0 &amp; \text{otherwise}
\end{cases}
\]</span> <span class="math display">\[
g_\tau^m = R_{t+1}^m + \gamma R_{t+2}^m + \gamma^2 R_{t+3}^m + \dots + \gamma^{T - t - 1} R_T^m
\]</span></p>
<p>For every sample trajectory <span class="math inline">\(m\)</span>, at any step <span class="math inline">\(\tau\)</span> in that trajectory, check if the state <span class="math inline">\(g_\tau^m\)</span> of that step is the <span class="math inline">\(s\)</span> we are interested in, then include its return <span class="math inline">\(g_\tau^m\)</span> in the sum, then normalize by <span class="math inline">\(C(s)\)</span>, the total number of times state <span class="math inline">\(s\)</span> was visited.</p>
<p>At this moment I just realized that: the state will get <strong>higher return, if its nearer to the beginning of a trajectory</strong>, if u dont believe, have a look at <span class="math inline">\(g_\tau^m\)</span> again ^^.</p>
<p>Btw, to calculate return <span class="math inline">\(g_\tau^m\)</span>, maybe you already know, we have to calculate from the terminate state first <span class="math inline">\(R_T^m\)</span>, where we know if the reward <span class="math inline">\(R_T^m\)</span> is 0 or 1 (reached the goal or not), then slowly trace backward with addding <span class="math inline">\(\gamma\)</span></p>
<p>And to make sure you understand it, <span class="math inline">\(v_\pi(s)\)</span> is just like <span class="math inline">\(G\)</span>, but <span class="math inline">\(G\)</span> is mostly binded to the trajectory and a policy, therefore the function <span class="math inline">\(v_\pi(s)\)</span> is actually <span class="math inline">\(G\)</span>!!!</p>
<p><strong>How to get to that Goal?</strong> to apply after the <span class="math inline">\(m\)</span>-th sample: <span class="math display">\[
V(s_t^m) \leftarrow V(s_t^m) + \alpha \left( g_t^m - V(s_t^m) \right)
\]</span></p>
<p>… then it will slowly converge to <strong>the Goal above</strong> …</p>
<p>BUT, how do we extend this to update our <strong>action</strong> ?</p>
</section>
<section id="monte-carlo-action-value" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-action-value">Monte Carlo (Action Value)</h2>
<p>Since we also have a deterministic set of action <span class="math inline">\(a \in \mathcal{A}(s)\)</span>, therefore we can extend the <strong>state value</strong> above to <strong>action value</strong> like this, it is equivalent:</p>
<p><strong>Start</strong> also with <span class="math inline">\(Q(s,a) = \frac{1}{|SxA|}\)</span> or just simply 0</p>
<p>Basically it just create a finer Q-table.</p>
<p><strong>The Goal</strong> <span class="math display">\[
Q_\pi(s,a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] \approx \frac{1}{C(s, a)} \sum_{m=1}^M \sum_{\tau=0}^{T_m - 1} \mathbb{I}[(s,a)_\tau^m = (s,a)] \, g_\tau^m
\]</span></p>
<p><strong>How to get to that goal?</strong> <span class="math display">\[
Q(s_t^m, a_t^m) \leftarrow Q(s_t^m, a_t^m) + \alpha \left( g_t^m - Q(s_t^m, a_t^m) \right)
\]</span></p>
<p>… Then it will slowly converge <strong>the Goal above</strong> …</p>
<p>Then we just argmax over action at each state, thats how we get optimal action.</p>
</section>
<section id="sum-up-constant-α-mc-algorithm-pi-approx-pi" class="level2">
<h2 class="anchored" data-anchor-id="sum-up-constant-α-mc-algorithm-pi-approx-pi">Sum up: Constant-α MC Algorithm <span class="math inline">\(\pi \approx \pi^*\)</span></h2>
<p><strong>Algorithm inputs</strong>: <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math inline">\(\alpha \in (0, 1]\)</span>, <span class="math inline">\(M \in \mathbb{N}\)</span></p>
<p><strong>Initialize arbitrarily</strong>:</p>
<ul>
<li><span class="math inline">\(\pi \leftarrow\)</span> some <span class="math inline">\(\epsilon\)</span>-soft policy<br>
</li>
<li><span class="math inline">\(Q(s,a) \leftarrow\)</span> some value for <span class="math inline">\(s \in \mathcal{S},\ a \in \mathcal{A}(s)\)</span> (like a random Q-Table hehe)</li>
</ul>
<p><strong>For</strong> <span class="math inline">\(m = 1, \dots, M\)</span>:</p>
<ul>
<li><p>Sample a trajectory under policy <span class="math inline">\(\pi\)</span>:<br>
<span class="math inline">\(s_0^m, a_0^m, r_1^m, \dots, a_{T_m - 1}^m, r_{T_m}^m\)</span></p></li>
<li><p><strong>For (literally EACH - EVERY SINGLE)</strong> <span class="math inline">\(t = 0, \dots, T_m - 1\)</span>:</p>
<ul>
<li><p>Compute return (the best way is just to calculate backwards then slowly add <span class="math inline">\(\gamma\)</span> like Gonkee ^^):<br>
<span class="math inline">\(g_t^m \leftarrow r_{t+1}^m + \gamma r_{t+2}^m + \dots\)</span></p></li>
<li><p>Update Q-value:<br>
<span class="math display">\[
Q(s_t^m, a_t^m) \leftarrow Q(s_t^m, a_t^m) + \alpha(g_t^m - Q(s_t^m, a_t^m))
\]</span></p></li>
</ul></li>
<li><p>Update policy:<br>
<span class="math inline">\(\pi \leftarrow \epsilon\text{-greedy}(Q)\)</span></p></li>
</ul>
<p>Where <span class="math inline">\(\pi \leftarrow \epsilon\text{-greedy}(Q)\)</span> is specified as follows: <span class="math display">\[
a^* \leftarrow \operatorname{argmax}_a Q(s_t^m, a) \quad \text{(ties broken arbitrarily)}
\]</span></p>
<p>For all <span class="math inline">\(a \in \mathcal{A}(s_t^m)\)</span>: (this means to balance the policy to avoid a local optimal) <span class="math display">\[
\pi(a|s_t^m) \leftarrow
\begin{cases}
1 - \epsilon + \frac{\epsilon}{|\mathcal{A}(s_t^m)|} &amp; \text{if } a = a^* \\
\frac{\epsilon}{|\mathcal{A}(s_t^m)|} &amp; \text{if } a \neq a^*
\end{cases}
\]</span></p>
<p>(<span class="math inline">\(|\mathcal{A}(s_t^m)| = \text{number of actions in } \mathcal{A}(s_t^m)\)</span>)</p>
<p>then back to the loop <strong>For</strong> <span class="math inline">\(m = 1, \dots, M\)</span> again and again …</p>
</section>
<section id="off-policy" class="level2">
<h2 class="anchored" data-anchor-id="off-policy">Off-Policy</h2>
<p>The problem is, as seen in the Blackjack example, lack of sample leads to local-optimal bias model</p>
<p>=&gt; <strong>Goal: more variance</strong></p>
<p>Sample a trajectory under a different policy <span class="math inline">\(b\)</span>: <span class="math inline">\(s_0^m, a_0^m, r_1^m, \dots, a_{T_m - 1}^m, r_{T_m}^m\)</span>. But the rest of the algorithm stays the same.</p>
<p>OK, but how to make sampling policy <span class="math inline">\(b\)</span> effect the main behavior policy <span class="math inline">\(\pi\)</span>?</p>
<section id="relationship-between-sampling-policy-b-vs-main-behavior-policy-pi" class="level3">
<h3 class="anchored" data-anchor-id="relationship-between-sampling-policy-b-vs-main-behavior-policy-pi">Relationship between sampling policy <span class="math inline">\(b\)</span> vs main behavior policy <span class="math inline">\(\pi\)</span>?</h3>
<p>We want: <span class="math display">\[
q_\pi(s, a) = \mathbb{E}_\pi[G_t|S_t = s, A_t = a]
\]</span></p>
<p>Sampled data under <span class="math inline">\(b\)</span> means this is what we actually estimated: <span class="math display">\[
\mathbb{E}_b[G_t|S_t = s, A_t = a]
\]</span></p>
<p>Therefore we use Importance Sampling to bring them to <span class="math inline">\(\pi\)</span>: <span class="math display">\[
q_\pi(s, a) = \mathbb{E}_b\left[\frac{p_\pi(G_t)}{p_b(G_t)}G_t|S_t = s, A_t = a\right]
\]</span> where <span class="math inline">\(\rho\)</span> is the importance sampling ratio: <span class="math display">\[
\frac{p_\pi(G_t)}{p_b(G_t)} = \rho = \prod_{\tau=t+1}^{T-1} \frac{\pi(A_\tau|S_\tau)}{b(A_\tau|S_\tau)}
\]</span></p>
</section>
</section>
<section id="sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi" class="level2">
<h2 class="anchored" data-anchor-id="sum-up-off-policy-constant-alpha-mc-for-pi-approx-pi">Sum up: Off-Policy Constant-<span class="math inline">\(\alpha\)</span> MC for <span class="math inline">\(\pi \approx \pi^*\)</span></h2>
<p><strong>Algorithm inputs</strong>: <span class="math inline">\(b\)</span> (behavior policy), <span class="math inline">\(\alpha \in (0, 1]\)</span>, <span class="math inline">\(M \in \mathbb{N}\)</span></p>
<p><strong>Initialize arbitrarily</strong>:</p>
<ul>
<li><span class="math inline">\(\pi \leftarrow\)</span> some policy</li>
<li><span class="math inline">\(Q(s, a) \leftarrow\)</span> some value for <span class="math inline">\(s \in \mathcal{S}, a \in \mathcal{A}(s)\)</span> (also the random Q-Table above hehe)</li>
</ul>
<p><strong>For</strong> <span class="math inline">\(m = 1, \dots, M\)</span>:</p>
<p>Under <span class="math inline">\(b\)</span> sample: <span class="math inline">\(s_0^m, a_0^m, r_1^m, \dots, a_{T_m-1}^m, r_{T_m}^m\)</span></p>
<ul>
<li><p><strong>For</strong> <span class="math inline">\(t = 0, \dots, T_m - 1\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\rho_t^m \leftarrow \prod_{\tau=t+1}^{T_m-1} \frac{\pi(a_\tau^m|s_\tau^m)}{b(a_\tau^m|s_\tau^m)}\)</span> (or 1 if <span class="math inline">\(t+1 &gt; T_m-1\)</span>)</p></li>
<li><p>Compute return: <span class="math inline">\(g_t^m \leftarrow \rho_t^m(r_{t+1}^m + \gamma r_{t+2}^m + \dots)\)</span></p></li>
<li><p>Update Q-Value: <span class="math inline">\(Q(s_t^m, a_t^m) \leftarrow Q(s_t^m, a_t^m) + \alpha(g_t^m - Q(s_t^m, a_t^m))\)</span></p></li>
<li><p>Update policy: <span class="math inline">\(\pi(s_t^m) \leftarrow \operatorname{argmax}_a Q(s_t^m, a)\)</span> (ties broken arbitrarily)</p></li>
</ul>
<p>Note that at Update Policy: we do not need the <span class="math inline">\(\pi\)</span>-greedy as above, because now using behavior-policy <span class="math inline">\(b\)</span>, we could already diverse out for a more global view</p></li>
</ul>
<p><strong>BUT</strong>, off policy MC has too much variance, therefore the next technique … Temporal Difference</p>
<p>Before we continue, let’s see where is exactly the point of model-free MC learning:</p>
</section>
</section>
<section id="compare-model-based-vs-model-free" class="level1">
<h1>Compare: Model-Based vs Model-Free</h1>
<p><strong>Similarity:</strong> they actually does update the same thing: state value V, or action value Q</p>
<p><strong>Difference:</strong> <strong>Model-based (Breadth Approach)</strong> approaches assume knowledge of, or explicitly learn, a model of the environment <span class="math inline">\(p(s', r \mid s, a)\)</span>. =&gt; can update value functions by <strong>bootstrapping</strong> from all possible successor states and rewards. =&gt; Updates based on <em>predictions</em> from the model, even without direct experience of every transition.</p>
<p>Model-based update one state from its neighboring transition, therefore it updates this way, from its neighbors: <span class="math display">\[
v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a \mid s) \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) \left[ r + \gamma v_\pi(s') \right]
\]</span> <span class="math display">\[
q_\pi(s, a) = \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) \left[ r + \gamma \sum_{a' \in \mathcal{A}(s')} \pi(a' \mid s') q_\pi(s', a') \right]
\]</span></p>
<p><strong>Model-free (Depth Approach)</strong> approaches do not require or learn an explicit model of the environment. Instead, they learn value functions directly from <strong>experience sampled through interactions with the environment</strong>. Updates are performed based on actual observed transitions and returns within a single trajectory (or a small batch of trajectories). This means that updates are driven by the specific sequence of states, actions, and rewards encountered. Model-free methods effectively update “within their trajectory” rather than relying on a global model.</p>
<p>But Model-free updates within its trajectory: <span class="math display">\[
V(s_t^m) \leftarrow V(s_t^m) + \alpha \left( g_t^m - V(s_t^m) \right)
\]</span> <span class="math display">\[
Q(s_t^m, a_t^m) \leftarrow Q(s_t^m, a_t^m) + \alpha \left( g_t^m - Q(s_t^m, a_t^m) \right)
\]</span></p>
<section id="q-why-do-model-free-methods-not-learn-the-environments-transition-probabilities-psrsa-why-do-they-learn-within-a-trajectory-instead-of-interacting-with-neighboring-states" class="level3">
<h3 class="anchored" data-anchor-id="q-why-do-model-free-methods-not-learn-the-environments-transition-probabilities-psrsa-why-do-they-learn-within-a-trajectory-instead-of-interacting-with-neighboring-states">Q: Why do model-free methods not learn the environment’s transition probabilities <span class="math inline">\(p(s',r|s,a)\)</span>? Why do they learn “within a trajectory” instead of interacting with neighboring states?</h3>
<ol type="1">
<li><p><strong>Complexity of Learning the Model:</strong> Learning model of the environment (<span class="math inline">\(p(s',r|s,a)\)</span>) can be extremely challenging in complex, high-dimensional environments (e.g., video games, robotic control). gridworld with 100 states and 4 actions already requires estimating 100×4×100 = 40,000 transition probabilities.</p></li>
<li><p><strong>Trajectory has direct ground truth to the problem</strong> Ultimate Goal: to find an optimal policy <span class="math inline">\(\pi\)</span> or value function <span class="math inline">\(V_\pi\)</span>. =&gt; direct route: they learn the value functions or the policy <em>directly from real-world experience</em>. Instead of first learning a model <span class="math inline">\(p(s',r|s,a)\)</span>, then using that model to derive value functions <span class="math inline">\(V_\pi\)</span>, and finally using value functions to determine the policy <span class="math inline">\(\pi\)</span>.</p></li>
</ol>
</section>
</section>
<section id="temporal-difference-still-a-model-free-without-p" class="level1">
<h1>Temporal Difference (still a Model-free without p)</h1>
<section id="n-step-temporal-difference-learning" class="level2">
<h2 class="anchored" data-anchor-id="n-step-temporal-difference-learning">n-step Temporal Difference Learning</h2>
<p>Recall from the MC approach: <span class="math display">\[
V(s_t^m) \leftarrow V(s_t^m) + \alpha(g_t^m - V(s_t^m))
\]</span></p>
<p>where: <span class="math inline">\(g_t^m \leftarrow r_{t+1}^m + \gamma r_{t+2}^m + \dots\)</span></p>
<p><strong>n-STEP TD</strong>: Replace the target, <span class="math inline">\(g_t^m\)</span>, with: <span class="math display">\[
g_{t:t+n}^m = r_{t+1}^m + \gamma r_{t+2}^m + \dots + \gamma^{n-1}r_{t+n}^m + \gamma^n V(s_{t+n}^m)
\]</span></p>
<p>where <span class="math inline">\(V(s_{t+n}^m)\)</span> is actually no different than <span class="math inline">\(g_{t+n}^m\)</span>, but instead of waiting for the trajectory to finish the episode then calculate backward, we just need to wait for <span class="math inline">\(n\)</span> steps to <strong>BOOTSTRAPPING</strong> the existing <span class="math inline">\(V(s_{t+n})\)</span> calculated from older trajectories, think a little bit, it means the same thing with <span class="math inline">\(g_{t+n}\)</span> (accumulated return). If <span class="math inline">\(n = \infty\)</span>: TD is identical to MC.</p>
</section>
<section id="why-is-td-better" class="level2">
<h2 class="anchored" data-anchor-id="why-is-td-better">Why is TD better?</h2>
<ol type="1">
<li><strong>Markov property:</strong> The RL environment has Markov property, that means the future depends only on the current state, not the history. And TD works based on that design, the update is just between neighboring states. Not like waiting for the whole trajectory like MC, <span class="math inline">\(g_t^m\)</span> needs backward calculation for the whole trajectory =&gt; strongly history based. <span class="math display">\[
V(s_t^m) \leftarrow V(s_t^m) + \alpha(g_t^m - V(s_t^m))
\]</span></li>
</ol>
<p>For example in TD(0) the use of <span class="math inline">\(V(s_{t+1}^m)\)</span> is very <strong>Markov property</strong>: <span class="math display">\[
V(s_t^m) \leftarrow V(s_t^m) + \alpha(r_{t+1}^m + \gamma V(s_{t+1}^m) - V(s_t^m))
\]</span></p>
<ol start="2" type="1">
<li><p><strong>Reduced Variance:</strong> The effect of MC is calculating accumulated result at each single trajectory, then averaging then in the end, this makes the result very specific to the sampled trajectories. But TD directly add the already smoothed <span class="math inline">\(+ \gamma^n V(s_{t+n}^m)\)</span></p></li>
<li><p><strong>Online-learning</strong> we all know what that means</p></li>
</ol>
</section>
<section id="what-does-larger-n-means" class="level2">
<h2 class="anchored" data-anchor-id="what-does-larger-n-means">What does larger n means?</h2>
<p>Increase the bias = utilize more from the knowledge from this trajectory, instead of keep learning from the average.</p>
<p>E.g., for a single episode with TD(8):</p>
<ul>
<li>All states from <span class="math inline">\(S_0\)</span> up to <span class="math inline">\(S_{k-8}\)</span> (if <span class="math inline">\(k \ge 8\)</span>): Will be updated using a full 8-step return, bootstrapping from <span class="math inline">\(V(S_{t+8})\)</span> =&gt; very <strong>average in the beginning</strong></li>
<li>The last 7 states (<span class="math inline">\(S_{k-7}, \dots, S_{k-1}\)</span>): Will be updated using a return that effectively “runs out of steps” before 8. Their targets will be a mix of actual rewards leading to the terminal state, like Monte Carlo =&gt; <strong>direct reward in the end</strong></li>
</ul>
</section>
<section id="sum-up-on-policy-temporal-difference-n-step-sarsa" class="level2">
<h2 class="anchored" data-anchor-id="sum-up-on-policy-temporal-difference-n-step-sarsa">Sum up: On-Policy Temporal Difference: n-step Sarsa</h2>
<p>Model-free control <span class="math inline">\(\rightarrow\)</span> use <span class="math inline">\(Q(s, a)\)</span>, not <span class="math inline">\(V(s)\)</span>.</p>
<p><strong>Redefine:</strong> <span class="math display">\[
g_{t:t+n}^m = r_{t+1}^m + \dots + \gamma^{n-1}r_{t+n}^m + \gamma^n Q(s_{t+n}^m, a_{t+n}^m)
\]</span></p>
<p><strong>Update rule:</strong> <span class="math display">\[
Q(s_t^m, a_t^m) \leftarrow Q(s_t^m, a_t^m) + \alpha(g_{t:t+n}^m - Q(s_t^m, a_t^m))
\]</span></p>
</section>
<section id="q-learning" class="level2">
<h2 class="anchored" data-anchor-id="q-learning">Q-Learning</h2>
<p>1-step TD Control—-target adjustment—-&gt; Q-Learning (off-policy).</p>
<p>Instead of using <span class="math inline">\(r_{t+1}^m + \gamma Q(s_{t+1}^m, a_{t+1}^m)\)</span> (which is used in SARSA and relies on the <em>next action taken by the policy</em>), Q-Learning uses: <span class="math display">\[
r_{t+1}^m + \gamma \max_{a} Q(s_{t+1}^m, a)
\]</span></p>
<p>The <span class="math inline">\(\max\)</span> operator means, regardless of which action the behavior policy <span class="math inline">\(b\)</span> actually took, this target is formed by the <em>best possible action</em> from the next state <span class="math inline">\(s_{t+1}^m\)</span> =&gt; Q-Learning an <strong>off-policy</strong>.</p>
<p>To describe what actually happens, it is like this: <strong>1-step TD (SARSA-like):</strong> <span class="math display">\[
\dots s_0^m, \underset{\uparrow}{\underline{a_0^m}}, r_1^m, s_1^m, \underset{\uparrow}{\underline{a_1^m}}, r_2^m, s_2^m, \underset{\uparrow}{\underline{a_2^m}}, r_3^m, s_3^m, \dots
\]</span> Updates for <span class="math inline">\(Q(s_t, a_t)\)</span> occur after observing <span class="math inline">\(s_{t+1}, a_{t+1}\)</span>, using the target <span class="math inline">\(r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})\)</span> <strong>where <span class="math inline">\(a_{t+1}\)</span> is the action taken by the <em>current policy</em>.</strong></p>
<p><strong>1-step Q-Learning:</strong> <span class="math display">\[
\dots  \underset{\uparrow}{\underline{s_0^m}},a_0^m, r_1^m, \underset{\uparrow}{\underline{s_1^m}},a_1^m, r_2^m, \underset{\uparrow}{\underline{s_2^m}},a_2^m, r_3^m, \dots
\]</span> Updates for <span class="math inline">\(Q(s_t, a_t)\)</span> occur after observing <span class="math inline">\(s_{t+1}\)</span>, using the target <span class="math inline">\(r_{t+1} + \gamma \max_a Q(s_{t+1}, a)\)</span>, <strong>where action <span class="math inline">\(a_{t+1}\)</span> is observed but <em>not</em> used in forming the target for <span class="math inline">\(Q(s_t, a_t)\)</span>.</strong></p>
</section>
<section id="expected-sarsa" class="level2">
<h2 class="anchored" data-anchor-id="expected-sarsa">Expected Sarsa</h2>
<p>1-step Q-Learning —– <span class="math inline">\(\max\)</span> operator-&gt;<strong>average</strong> operator—&gt; Expected Sarsa <span class="math display">\[
r_{t+1}^m + \gamma \max_{a} Q(s_{t+1}^m, a)
\]</span> to using an expectation over all possible actions, weighted by the policy <span class="math inline">\(\pi\)</span>: <span class="math display">\[
r_{t+1}^m + \gamma \sum_{a} Q(s_{t+1}^m, a) \pi(a|s_{t+1}^m)
\]</span></p>
<p>As presented (when the policy <span class="math inline">\(\pi\)</span> used in the target is the same as the behavior policy generating the data), this is an <strong>on-policy</strong> method.</p>
<p>But to make it <strong>off-policy</strong>, just need <span class="math inline">\(\text{policy generating the trajectory} \neq \pi \text{ in target}\)</span></p>
</section>
<section id="compare" class="level2">
<h2 class="anchored" data-anchor-id="compare">Compare</h2>
<ul>
<li><strong>Sarsa</strong> has longer path, because it just took the action it actually took, which was most of the time exploratory, the one with <span class="math inline">\(\epsilon \text{exploration}\)</span> policy that balance out</li>
<li><strong>Q-Learning</strong> does not use this <span class="math inline">\(\epsilon \text{exploration}\)</span> policy, it uses <span class="math inline">\(\max\)</span> operator</li>
<li><strong>Expected Sarsa</strong> use weighted average, so yeah, always a safe choice.</li>
</ul>
</section>
<section id="summary-of-td" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-td">Summary of TD</h2>
<p>Goal of Q-Learning is updating Q-Table to optimal where: <span class="math display">\[
Q_*(s_t, a_t) = \mathbb{E} \left[ r_t + \gamma \max_a Q_*(s_{t+1}, a) \right]
\]</span></p>
<p>Also called Bellman Optimality Equation: every subsequence of a optimal sequence must also be optimal</p>
<p><strong>Learning Q-values</strong>:</p>
<ul>
<li><p>SARSA:</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\]</span></p></li>
<li><p>Expected SARSA:</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \sum_a \pi(a \mid s_{t+1}) Q(s_{t+1}, a) - Q(s_t, a_t) \right]
\]</span></p></li>
<li><p>Q-Learning:</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
\]</span></p></li>
</ul>
<p><strong>Learning V-values</strong>:</p>
<p><span class="math display">\[
V(s_t) \leftarrow V(s_t) + \alpha \left[ r_t + \gamma V(s_{t+1}) - V(s_t) \right]
\]</span></p>
<p>Where <span class="math inline">\(\alpha\)</span> is the learning rate.</p>
</section>
</section>
<section id="function-approximation" class="level1">
<h1>Function Approximation</h1>
<p>For problem where number of states (<span class="math inline">\(|\mathcal{S}|\)</span>) is extremely large, apply Unsupervised Learning (Generalization function of states (or also actions)) as a part of RL.</p>
<section id="on-policy-evaluation-with-function-approximation" class="level2">
<h2 class="anchored" data-anchor-id="on-policy-evaluation-with-function-approximation">On-Policy Evaluation with Function Approximation</h2>
<p><strong>Goal</strong> remains to approximate the state-value function <span class="math inline">\(v_\pi(s)\)</span>. Data generated from a given fixed policy <span class="math inline">\(\pi\)</span>.</p>
<p>We now learn a parameterized function <span class="math inline">\(\hat{v}(s, \mathbf{w})\)</span>, where:</p>
<ul>
<li><span class="math inline">\(\mathbf{w} \in \mathbb{R}^d\)</span> is a vector of parameters</li>
<li>a state <span class="math inline">\(s\)</span> as input</li>
</ul>
<p>We learn <span class="math inline">\(w\)</span> and hope that <span class="math inline">\(v_\pi(s) \approx \hat{v}(s, \mathbf{w})\)</span></p>
<p>Since <span class="math inline">\(d \ll |\mathcal{S}|\)</span>, any change to <span class="math inline">\(\mathbf{w}\)</span> can simultaneously change <span class="math inline">\(\hat{v}(s, \mathbf{w})\)</span> for many (or all) states <span class="math inline">\(s\)</span>. Different from tabular methods, where an update to <span class="math inline">\(V(s)\)</span> for a specific state <span class="math inline">\(s\)</span> affects <em>only</em> that state’s value.</p>
<p><strong>Example:</strong> two simple features for given image state <span class="math inline">\(s\)</span>:</p>
<ul>
<li><span class="math inline">\(x_1(s)\)</span>: The average of all pixel values in the image.</li>
<li><span class="math inline">\(x_2(s)\)</span>: The standard deviation of all pixel values in the image.</li>
</ul>
<p>=&gt; feature vector <span class="math inline">\(\mathbf{x}(s) = \begin{bmatrix} x_1(s) \\ x_2(s) \end{bmatrix}\)</span></p>
<p>With these features, we can construct a <strong>linear value function</strong> to approximate <span class="math inline">\(v_\pi(s)\)</span>: <span class="math display">\[
\hat{v}(s, \mathbf{w}) = \mathbf{x}(s)^T \mathbf{w}
\]</span></p>
</section>
<section id="goal-how-to-get-mathbfw" class="level2">
<h2 class="anchored" data-anchor-id="goal-how-to-get-mathbfw">Goal: How to get <span class="math inline">\(\mathbf{w}\)</span>?</h2>
<p>The ‘best’ <span class="math inline">\(\mathbf{w}\)</span> minimizes: <span class="math display">\[
\overline{VE}(\mathbf{w}) = \sum_{s \in \mathcal{S}} \mu(s)[v_\pi(s) - \hat{v}(s, \mathbf{w})]^2
\]</span> where <span class="math inline">\(\mu(\cdot)\)</span> is a distribution over states (frequency of visiting each state).</p>
<ul>
<li><strong>We observe a surrogate for <span class="math inline">\(v_\pi(S_t)\)</span>: <span class="math inline">\(U_t\)</span>:</strong> Since we don’t know <span class="math inline">\(v_\pi(S_t)\)</span> exactly, we use a sample-based estimate or target, <span class="math inline">\(U_t\)</span>, as a stand-in. This <span class="math inline">\(U_t\)</span> could be the Monte Carlo return <span class="math inline">\(G_t\)</span>, or an n-step TD target <span class="math inline">\(g_{t:t+n}\)</span>, or a 1-step TD target <span class="math inline">\((R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}))\)</span>.</li>
</ul>
<p><strong>Update Rule</strong> we don’t have direct access to <span class="math inline">\(v_\pi(s)\)</span> for all states =&gt; <strong>Stochastic Gradient Descent (SGD)</strong> for updating our parameters <span class="math inline">\(\mathbf{w}\)</span>: <span class="math display">\[
\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ U_t - \hat{v}(S_t, \mathbf{w}) \right] \nabla_{\mathbf{w}} \hat{v}(S_t, \mathbf{w})
\]</span> Where:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the learning rate (step size).</li>
<li><span class="math inline">\(\left[ U_t - \hat{v}(S_t, \mathbf{w}) \right]\)</span> is the <strong>TD error</strong> (or prediction error) based on our sample <span class="math inline">\(U_t\)</span>.</li>
<li><span class="math inline">\(\nabla_{\mathbf{w}} \hat{v}(S_t, \mathbf{w})\)</span> is the gradient of our estimated value function with respect to its parameters <span class="math inline">\(\mathbf{w}\)</span>, evaluated at state <span class="math inline">\(S_t\)</span>. This gradient tells us how to adjust <span class="math inline">\(\mathbf{w}\)</span> to change <span class="math inline">\(\hat{v}(S_t, \mathbf{w})\)</span> in the desired direction.</li>
</ul>
</section>
<section id="how-to-obtain-the-target-u_t" class="level2">
<h2 class="anchored" data-anchor-id="how-to-obtain-the-target-u_t">How to Obtain the Target <span class="math inline">\(U_t\)</span></h2>
<p>In the Stochastic Gradient Descent update rule: <span class="math display">\[
\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ U_t - \hat{v}(S_t, \mathbf{w}) \right] \nabla_{\mathbf{w}} \hat{v}(S_t, \mathbf{w})
\]</span> The term <span class="math inline">\(U_t\)</span> serves as our sample-based target for the true value <span class="math inline">\(v_\pi(S_t)\)</span>. Since <span class="math inline">\(v_\pi(S_t)\)</span> is unknown, we must derive <span class="math inline">\(U_t\)</span> from our observed experience. The choice of <span class="math inline">\(U_t\)</span> determines whether our method leans towards Monte Carlo or Temporal Difference approaches:</p>
<p><strong>1. Monte Carlo Target:</strong> If the target <span class="math inline">\(U_t\)</span> is the <strong>full Monte Carlo return</strong> from state <span class="math inline">\(S_t\)</span> to the end of the episode, then we are using a <strong>Gradient Monte Carlo</strong> method: <span class="math display">\[
U_t = G_t
\]</span> where <span class="math inline">\(G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-t-1}R_T\)</span> is the total discounted return observed from time step <span class="math inline">\(t\)</span> until the terminal state <span class="math inline">\(T\)</span>.</p>
<ul>
<li><strong>Characteristics:</strong>
<ul>
<li><strong>Unbiased:</strong> If <span class="math inline">\(G_t\)</span> is an unbiased estimate of <span class="math inline">\(v_\pi(S_t)\)</span> (which it is, on average), then using it as <span class="math inline">\(U_t\)</span> can lead to the parameters <span class="math inline">\(\mathbf{w}\)</span> converging to a local optimum of the Mean Squared Value Error (<span class="math inline">\(\overline{VE}(\mathbf{w})\)</span>).</li>
<li><strong>High Variance:</strong> <span class="math inline">\(G_t\)</span> can be noisy due to the sum of many random rewards.</li>
<li><strong>Requires complete episodes:</strong> We must wait until the episode ends to compute <span class="math inline">\(G_t\)</span>.</li>
</ul></li>
</ul>
<p><strong>2. Temporal Difference (TD) Target:</strong> If the target <span class="math inline">\(U_t\)</span> is derived using <strong>bootstrapping</strong> (i.e., using an estimate of the value of a future state), then we are using a <strong>Semi-Gradient TD</strong> method. The most common is the 1-step TD target: <span class="math display">\[
U_t = R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w})
\]</span> Here, <span class="math inline">\(R_{t+1}\)</span> is the actual reward observed, and <span class="math inline">\(\hat{v}(S_{t+1}, \mathbf{w})\)</span> is our <em>current estimate</em> of the value of the next state <span class="math inline">\(S_{t+1}\)</span>. For this specific update, <span class="math inline">\(\mathbf{w}\)</span> in <span class="math inline">\(\hat{v}(S_{t+1}, \mathbf{w})\)</span> is usually the online network’s weights, not a target network’s weights in this basic formulation.</p>
<ul>
<li><strong>Characteristics:</strong>
<ul>
<li><strong>Biased:</strong> especially in the beginning with crappy initialized value <span class="math inline">\(\hat{v}(S_{t+1}, \mathbf{w})\)</span>.</li>
<li><strong>Lower Variance:</strong> It typically has lower variance than the Monte Carlo target because it depends on only one actual reward and then uses a smoothed estimate for the rest of the future.</li>
<li><strong>Online Learning:</strong> Updates can be performed after each single time step, without waiting for the end of an episode.</li>
<li><strong>Semi-Gradient:</strong> Since <span class="math inline">\(U_t\)</span> depends on <span class="math inline">\(\mathbf{w}\)</span>, our update rule is <strong>not a true gradient step</strong>. The gradient <span class="math inline">\(\nabla_{\mathbf{w}} L(\mathbf{w})\)</span> for the loss <span class="math inline">\(\left( (R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w})) - \hat{v}(S_t, \mathbf{w}) \right)^2\)</span> would actually involve the derivative of <span class="math inline">\(U_t\)</span> (<span class="math inline">\(\hat{v}(S_{t+1}, \mathbf{w})\)</span>) with respect to <span class="math inline">\(\mathbf{w}\)</span>. =&gt; semi-gradient means: <span class="math inline">\(\nabla_{\mathbf{w}} \hat{v}(S_{t+1}, \mathbf{w})\)</span> is omitted for simplicity and stability, only taking <span class="math inline">\(\nabla_{\mathbf{w}} \hat{v}(S_{t}, \mathbf{w})\)</span>.</li>
<li><strong>No Guarantee of Convergence (to Global Optimum):</strong> Because it’s not a true gradient of the overall <span class="math inline">\(\overline{VE}(\mathbf{w})\)</span>, we generally <strong>don’t guarantee convergence</strong> to the global optimum of the Mean Squared Value Error, even if the optimal <span class="math inline">\(\mathbf{w}\)</span> is unique. However, for linear function approximation, it can still converge to a local optimum. For non-linear approximators like neural networks, theoretical guarantees are weaker, but these methods still perform very well in practice.</li>
</ul></li>
</ul>
<p>The update rule remains: <span class="math display">\[
\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w}) \right] \nabla_{\mathbf{w}} \hat{v}(S_t, \mathbf{w})
\]</span></p>
</section>
<section id="extension-to-on-policy-control-with-function-approximation" class="level2">
<h2 class="anchored" data-anchor-id="extension-to-on-policy-control-with-function-approximation">Extension to On-Policy Control with Function Approximation</h2>
<p>So far is just <strong>policy evaluation</strong> (approximating <span class="math inline">\(v_\pi(s)\)</span>). Now extend directly to <strong>control</strong> problems (finding an optimal policy), typically by approximating the action-value function <span class="math inline">\(q_\pi(s,a)\)</span> or <span class="math inline">\(q_*(s,a)\)</span>. <span class="math display">\[
\mathbf{w} \leftarrow \mathbf{w} + \alpha \left[ U_t - \hat{q}(S_t, A_t, \mathbf{w}) \right] \nabla_{\mathbf{w}} \hat{q}(S_t, A_t, \mathbf{w})
\]</span> Here, <span class="math inline">\(\hat{q}(S_t, A_t, \mathbf{w})\)</span> is our function approximator’s estimate of the action-value for the state-action pair <span class="math inline">\((S_t, A_t)\)</span> using parameters <span class="math inline">\(\mathbf{w}\)</span>. The term <span class="math inline">\(\nabla_{\mathbf{w}} \hat{q}(S_t, A_t, \mathbf{w})\)</span> is the gradient of this estimate with respect to <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>For <strong>Semi-gradient 1-step Sarsa</strong>, the target <span class="math inline">\(U_t\)</span> is defined as: <span class="math display">\[
U_t = R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w})
\]</span></p>
<p>Since both the action being evaluated (<span class="math inline">\(A_t\)</span>) and the action used to construct the target (<span class="math inline">\(A_{t+1}\)</span>) are chosen according to the <em>same behavior policy</em> (which is actively being improved based on <span class="math inline">\(\hat{q}\)</span>), this method is <strong>on-policy</strong>. The agent learns the value of the policy it is currently following, and this policy itself is improved through action selection methods (e.g., <span class="math inline">\(\epsilon\)</span>-greedy) based on the learned <span class="math inline">\(\hat{q}\)</span> values.</p>
<p><strong>Example:</strong> Linear Action-Value Function for the Mountain Car Task</p>
<p>To illustrate how function approximation can be used for action-value functions, let’s consider a scenario like <strong>The Mountain Car Task</strong>. We approximate the action-value function <span class="math inline">\(\hat{q}(s, a, \mathbf{w})\)</span> using a linear function approximator instead of a Q-Table: <span class="math display">\[
\hat{q}(s, a, \mathbf{w}) = \begin{cases}
    \mathbf{w}_{-1}^T \mathbf{x}(s) &amp; \text{if } a = -1 \\
    \mathbf{w}_{0}^T \mathbf{x}(s) &amp; \text{if } a = 0 \\
    \mathbf{w}_{1}^T \mathbf{x}(s) &amp; \text{if } a = 1
\end{cases}
\]</span> Where:</p>
<ul>
<li>action <span class="math inline">\(a\)</span> (-1, 0 -1)</li>
<li><span class="math inline">\(\mathbf{x}(s)\)</span> is the feature representation of the state <span class="math inline">\(s\)</span> (length 120 decoded from position &amp; velocity)</li>
<li><span class="math inline">\(\mathbf{w}_{-1}\)</span>, <span class="math inline">\(\mathbf{w}_{0}\)</span>, and <span class="math inline">\(\mathbf{w}_{1}\)</span> are distinct weight vectors, each corresponding to one of the possible actions.</li>
<li>The overall parameter vector <span class="math inline">\(\mathbf{w}\)</span> for the entire function approximator would be the concatenation of these individual action-specific weight vectors (i.e., <span class="math inline">\(\mathbf{w} = [\mathbf{w}_{-1}, \mathbf{w}_{0}, \mathbf{w}_{1}]\)</span>).</li>
</ul>
</section>
<section id="extension-to-on-policy-control-with-function-approximation-1" class="level2">
<h2 class="anchored" data-anchor-id="extension-to-on-policy-control-with-function-approximation-1">Extension to On-Policy Control with Function Approximation</h2>
<p>We all know what that is, just that when we combine three things together:</p>
<ul>
<li>off-policy</li>
<li>function approximation</li>
<li>bootstrapping</li>
</ul>
<p>we will have problem with convergence, which may be solved by the next topic …</p>
</section>
</section>
<section id="policy-gradient-methods" class="level1">
<h1>Policy Gradient Methods</h1>
<p>Up until now, our primary approach to solving Reinforcement Learning problems has been <strong>value-based methods</strong>. These methods involve learning an approximate value function, such as <span class="math inline">\(\hat{q}(s,a, \mathbf{w})\)</span>, and then deriving a policy from this value function. For instance, in Q-learning or Sarsa, the policy <span class="math inline">\(\pi(a|s)\)</span> is often implicitly defined by choosing the action with the highest Q-value (or an <span class="math inline">\(\epsilon\)</span>-greedy variant of it): <span class="math display">\[\mathbf{w} \in \mathbb{R}^d \rightarrow \hat{q}(s,a, \mathbf{w}) \rightarrow \pi(a|s)\]</span> Where once again: <span class="math inline">\(\hat{q}(s,a, \mathbf{w}) = \mathbb{E}_\pi[G_t \mid s_t = s, a_t = a]\)</span></p>
<p>Policy Gradient Methods more direct. <strong>directly learn a parameterized policy</strong>: <span class="math display">\[\mathbf{\theta} \in \mathbb{R}^{d'} \rightarrow \pi(a|s, \mathbf{\theta})\]</span> Here, <span class="math inline">\(\mathbf{\theta}\)</span> is a new set of parameters (distinct from <span class="math inline">\(\mathbf{w}\)</span> for value functions) that directly define the policy <span class="math inline">\(\pi(a|s, \mathbf{\theta})\)</span>. This policy directly maps states to probabilities of taking actions, or to specific actions in continuous action spaces. The dimensionality of these policy parameters is <span class="math inline">\(d'\)</span>.</p>
<p>Advantages for complex tasks:</p>
<ul>
<li><strong>Continuous Action Spaces:</strong> It’s often more natural to parameterize a policy for continuous actions (e.g., a Gaussian distribution whose mean and variance are outputs of a neural network) than to find the maximum over a continuous Q-function.</li>
<li><strong>Stochastic kind of Policies:</strong> For kind of exploratory task (where actions are chosen probabilistically).</li>
<li><strong>Simpler Policy Updates:</strong> we all know what this means…</li>
</ul>
<p><strong>Goal:</strong> The core idea of policy gradient methods is to adjust the parameters <span class="math inline">\(\mathbf{\theta}\)</span> of the policy in the direction that maximizes the expected return (or performance) of the policy.</p>
<p><strong>Problem:</strong></p>
<ul>
<li>Without baseline → high variance (different average like a state with average return G1 of 100 and change +-5 are not the same with another state with average return G2 of 10 that also +-5)</li>
<li>With baseline (e.g., <span class="math inline">\(V_\pi(s)\)</span>) → reduced variance</li>
</ul>
<section id="a-monte-carlo-style-policy-gradient-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="a-monte-carlo-style-policy-gradient-algorithm">A Monte Carlo Style Policy Gradient Algorithm</h2>
<p>The core idea is to increase the probability of actions that lead to high returns and decrease the probability of actions that lead to low returns.</p>
<p><strong>Initialize:</strong></p>
<ul>
<li><strong>Functional form for the policy:</strong> <span class="math inline">\(\pi(a|s, \mathbf{\theta})\)</span> (e.g., a neural network that outputs action probabilities given a state, parameterized by <span class="math inline">\(\mathbf{\theta}\)</span>).</li>
<li><strong>Initial parameters:</strong> <span class="math inline">\(\mathbf{\theta}\)</span> (e.g., randomly initialized weights for a neural network).</li>
<li><strong>Step size (learning rate):</strong> <span class="math inline">\(\alpha\)</span></li>
</ul>
<p><strong>Algorithm:</strong></p>
<p><strong>For</strong> <span class="math inline">\(m = 1, \dots, M\)</span> (for each episode):</p>
<ul>
<li><p><strong>Sample a trajectory under the current policy <span class="math inline">\(\pi(\cdot|\cdot, \mathbf{\theta})\)</span>:</strong> <span class="math inline">\(s_0^m, a_0^m, r_1^m, \dots, a_{T_m-1}^m, r_{T_m}^m\)</span> (where <span class="math inline">\(T_m\)</span> is the length of the episode).</p></li>
<li><p><strong>For</strong> <span class="math inline">\(t = 0, \dots, T_m - 1\)</span> (for each time step in the trajectory):</p>
<ul>
<li><p><strong>Compute the return from time <span class="math inline">\(t\)</span>:</strong> <span class="math inline">\(g_t^m \leftarrow r_{t+1}^m + \gamma r_{t+2}^m + \dots + \gamma^{T_m - t - 1} r_{T_m}^m\)</span> (This is the total discounted reward from <span class="math inline">\(t+1\)</span> until the end of the episode).</p></li>
<li><p><strong>Update the policy parameters:</strong> <span class="math display">\[
  \mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha \gamma^t g_t^m \nabla_{\mathbf{\theta}} \ln \pi(a_t^m|s_t^m, \mathbf{\theta})
  \]</span></p></li>
</ul></li>
</ul>
<p><strong>Explanation of the Update Rule:</strong></p>
<ul>
<li><span class="math inline">\(\alpha\)</span>: The learning rate, controlling the step size of the update.</li>
<li><span class="math inline">\(\gamma^t\)</span>: The discount factor raised to the power of <span class="math inline">\(t\)</span>. This term accounts for the discounting of future rewards, ensuring that immediate rewards have a stronger influence on early actions.</li>
<li><span class="math inline">\(g_t^m\)</span>: The Monte Carlo return from time step <span class="math inline">\(t\)</span>. e.g.&nbsp;If <strong>return high =&gt; big step size</strong>, if <strong>return negative =&gt; step backward</strong></li>
<li><span class="math inline">\(\nabla_{\mathbf{\theta}} \ln \pi(a_t^m|s_t^m, \mathbf{\theta})\)</span>: This is the <strong>gradient of the log-probability of the action taken</strong>.
<ul>
<li><span class="math inline">\(\ln \pi(a_t^m|s_t^m, \mathbf{\theta})\)</span>: The logarithm of the probability that the policy <span class="math inline">\(\pi\)</span> would choose action <span class="math inline">\(a_t^m\)</span> in state <span class="math inline">\(s_t^m\)</span> with current parameters <span class="math inline">\(\mathbf{\theta}\)</span>. <strong>This is very important!!!</strong> It is a <strong>normalizer for cases like a random policy <span class="math inline">\(\pi\)</span> pick a positive (but low return) action too often,</strong> leading to pushing the probability too much, so by taking the gradient of <span class="math inline">\(ln\)</span> of it, it limit down it, (and also the opposite for high reward actions but the behavior policy did not pick as many time to push enough higher). These situations usually happen with <strong>initialization of that lower reward already too high, and then add up that the policy random pick it too often pushing it even further.</strong></li>
<li><span class="math inline">\(\nabla_{\mathbf{\theta}}\)</span>: The gradient operator with respect to the policy parameters <span class="math inline">\(\mathbf{\theta}\)</span>. This term tells us how to change <span class="math inline">\(\mathbf{\theta}\)</span> to increase the log-probability of taking action <span class="math inline">\(a_t^m\)</span> in state <span class="math inline">\(s_t^m\)</span>.</li>
</ul></li>
</ul>
<p><strong>Intuition:</strong></p>
<p>The update rule essentially says: if action <span class="math inline">\(a_t^m\)</span> taken in state <span class="math inline">\(s_t^m\)</span> leads to a high return (<span class="math inline">\(g_t^m\)</span> is large and positive), then increase the probability of taking that action in that state. If it leads to a low (or negative) return, decrease its probability. The <span class="math inline">\(\gamma^t\)</span> term ensures that actions taken earlier in the trajectory, which influence more of the subsequent rewards, are given appropriate credit.</p>
<p>REINFORCE is a fundamental algorithm that demonstrates the direct optimization of a policy, laying the groundwork for more advanced policy gradient methods.</p>
</section>
<section id="credit-assignment-problem" class="level2">
<h2 class="anchored" data-anchor-id="credit-assignment-problem">Credit assignment problem</h2>
<p>within an episode, figure out what subpath made the episode bad or good a.k.a individual action impact on the episode =&gt;</p>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/tienthangdinh\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>