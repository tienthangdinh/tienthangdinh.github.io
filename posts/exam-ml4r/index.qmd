---
title: "Machine Learning 4 Robotics"
date: 2025-07-19
categories: [Machine Learning, Model-based Learning, Reinforcement Learning]
format:
  html:
    toc: true
    code-fold: true
    math: mathjax
---


# Control
* Forward dynamics: given torque vector -> angular position, velocity, acceleration at each joint
* Backward dynamics: the inverse is more interesting

## PID
3 terms: Proportional Gain (Kp), Integral (Ki), Differential (Kd)
* Proportional: we all know, current error strong -> more gas, weak->less gas
* Integral? for gradual decaying system (like car) -> continuously effort a little more gas to compensate the persistent decay, otherwise P controller keep chasing it and never reach. BUT, it needs careful choose, because it is slow reaction, this is a drawback, and a term to overcome this drawback is...
* Differential: fast change in error: it will correct! -> can react on time (sharp turn, break,...), BUT from my experience this should not be too high bc it can be noisy

## Dynamics System ID
* just generate bunch of data, then fit this data with any type regression model
* linear or non-linear (then needs linearization around interested equilibrium like the cart pole)

## Exploring a linear feedback-state control in LTI-System

General idea is like this, implemented by a dot product:
$$u(t) = K e(t) = K_x (x(t) - x_{\text{target}}(t)) + K_\theta \theta(t) + K_{\dot{x}} \dot{x}(t) + K_{\dot{\theta}} \dot{\theta}(t)$$
```
u = mj_data.ctrl = - np.dot(gain, error)
```
we just adapt the gain K to see which one is good, but this is a manual work, so how to find the optimal gain automatically? LQR

## LQR

### THE GOAL: Cost Function to minimize:

$J = \int_{0}^{\infty} (\mathbf{s}(t)^T \mathbf{Q} \mathbf{s}(t) + u(t)^T R u(t)) \, dt$

* **Q (State Weighting Matrix):**
    Penalize state deviation
    $$\mathbf{Q} = \text{diag}([1, 10, 0, 0]) = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 10 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix}$$

    This means:

    *  penalize the square of the cart's position ($x^2$) with a weight of 1.
    *  heavily penalize the square of the pendulum's angle ($\theta^2$) with a weight of 10, indicating it's crucial to keep the pendulum upright.
    *  do not directly penalize the squared cart velocity ($\dot{x}^2$) or pendulum angular velocity ($\dot{\theta}^2$) in the cost function.

* **R (Control Weighting Matrix/Scalar):**
    penalizes the magnitude of the control input. A larger $R$ encourage smoother control, but slower. smaller $R$ canverges faster, but may not always good for human and gearbox
    $$R = 0.1$$
    This means the square of the control input ($u^2$) is penalized with a weight of 0.1.

### Key Knowledge 1: Linear System Dynamics:
we use Lagrange dynamics by hand to define matrices A and B manually
then put in 

$\dot{\mathbf{s}}(t) = \mathbf{A}\mathbf{s}(t) + \mathbf{B}u(t)$

* **A (State Matrix):**
    $$\mathbf{A} = \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & -\frac{g m}{M} & 0 & 0 \\ 0 & \frac{g}{l}\left(1 + \frac{m}{M}\right) & 0 & 0 \end{bmatrix}$$

* **B (Input Matrix):**
    $$\mathbf{B} = \begin{bmatrix} 0 \\ 0 \\ \frac{k}{M} \\ -\frac{k}{M l} \end{bmatrix}$$

 Where:

    * $g$: acceleration due to gravity
    * $m$: mass of the pendulum (pole)
    * $M$: mass of the cart
    * $l$: length from the pivot to the center of mass of the pendulum
    * $k$: gear ratio, converting the control signal $u$ into a physical force $F = k \cdot u$ applied to the cart.


### Key Knowledge 2: Linear Controller
**K (State Feedback Gain Matrix):**

* For a LTI-System with infinite horizon, we can set derivation of cost function = 0 and find out that $u(t) = - \mathbf{K}\mathbf{s}(t)$ at every time point the same $K$.
* For an LTI system with a finite horizon, K is time-varying because we set extra another cost at the end of horizont(calculated via DRE).
* This is achieved by rephrasing the cost function above, then we end up with a function including two terms, both are quadratic terms (first one totally dependent on x, the second term where x and u are involved).

* From the first one we found out that x and u are in linear relationship 
$$\mathbf{u}^*(t) = -\mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x}(t)$$

* But since we do not know how much is this K gain, so we also set the first term to 0, which is Algebraic Ricatti Equation, to find $P$, and eventually find $K = \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}$


    The `control.lqr(A,B,Q,R)` computes this:
    $$\mathbf{K} = \begin{bmatrix} K_x & K_\theta & K_{\dot{x}} & K_{\dot{\theta}} \end{bmatrix}$$
    $$u(t) = - (K_x x(t) + K_\theta \theta(t) + K_{\dot{x}} \dot{x}(t) + K_{\dot{\theta}} \dot{\theta}(t))$$
    


| Feature            | Finite-Horizon LQR | Model Predictive Control (MPC) |
| :----------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Systems** | Limited to local linearization around an equilibrium.| Can handle non-linear system, because it is mostly just sampling different trajectories at each timestemp and choose the most optimal one |
| **Calculation** | Calculate **once, offline** | Calculate **repeatedly, online** at each time step |
| **Gain K** | Pre-computed **time-varying schedule** $\mathbf{K}(t)$ for the full horizon. Even for finite horizont, at K(0), can calculate K(1), K(2), K(3),... arranging recursive relationship in a linear equation system, then substitue back into cost function, we can even solve it using either DRE or transforming in Quadratic Format | a sequence of open-loop controls is generated, and only the first is applied. If the system is linear, can apply LQR easily then no need trajectory sampling, but if not we need sampling |
                                                        
| **Computational Load** | Low online load (just lookup) | High (solves an optimization problem at each step)                                                                                                                                          


## Finite Horizont LQR (can also be an MPC)
* step 1: taking dynamics A, B as input, formulate finite horizont stateinput sequences in a dynamic programmed (matrix) way $X_k = \mathbf{\Phi} x(k) + \mathbf{\Gamma} U_k$
* step 2: substituting into J cost function (just the same as LQR), now we have $J(U_k,x(k))$ into $\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}$.
* using QP solver, because this is the format that the solution method is known
* (only difference in MPC) - only apply first one, then re-calculate at the next step


# Introduction to RL
* Environment: Markov Decision Process: chain of states and their relationship of actions, rewards, based on Markov Chain Property (only based on last state is MDP), but other properties like POMDP, or n-step DP.
* MBRL: using Dynamic Programming
* Model-free: Monte Carlo --(improve)--- Temporal Difference (SARSA (on policy) vs Q-Learning (off-policy))

**Goal**
 find a policy $\pi^*$  that maximizes the discounted future return.
$$
\max_\pi \, \mathbb{E}_\pi \left[ \sum_{k = t+1}^{T} \gamma^{k - t - 1} R_k \right]
$$

# Model-free RL
## Value-based (2 steps)
### Value Iteration approach (we have $p(s', r \mid s, a)$)

Each state value $V$, or state-action value $Q$ can be represented **RECURSIVELY** by **immediate reward R + neighboring state-action values weighted by transition actions propability distribution.**

**Keypoint** Utilizing this **Bellman optimality recursion**, **immediate reward R**, they can help us converging to optimal $V^*$ and $Q^*$ by  **Value Iteration**: that starts with random $V$, $Q$, but repeatedly sweeping Bellman optimality equation to all states/state-action pairs.
```
policy = lambda s: [0.25,0.25,0.25,0.25]
V = np.zeros((4,4))
Q_pi = np.zeros((4,4,4))

for steps in range(200):
    for state_idx in range(16):
        action_probs = policy(state)
        subseq_states = np.clip(state+directions,0,3)
        V_subseq_states = np.array([V_pi[tuple(idx)] for idx in subseq_states])
        V_pi[state] = np.sum(action_probs*(rewards+gamma*V_subseq_states))
        Q_pi[state] = rewards + gamma * V_subseq_states
```

$$V^*(s) = \max_{a \in \mathcal{A}} \left[ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s, a)(s') V^*(s') \right]$$

$$Q^*(s, a) = R(s, a) + \gamma \mathbb{E}_{s' \sim P(s,a)}[V^*(s')]$$

In the end when $Q^*$ are converged, we can find a deterministic $\pi^*(s)$ via:

$$\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s, a)$$


### Sampling Approach (no access to $p(s', r \mid s, a)$)
**2 steps:** (state, action) -> (value) -> argmax deterministic (action)

every model-free algorithms has to work based on Monte Carlo for trajectory sampling.

#### 1. Monte Carlo

* sample full trajectories
* update at the end of each trajectory in reverse
* epsilon-greedy => Q* != Q_pi. epsilon greedy to sample actions, which are mostly only suboptimal and actually leads to longer trajectory


**Start** with a random value function: $V(s) = \frac{1}{|S|}$

**Collect sampling trajectories** $M$ trajectory samples:
$$
s_0^m \quad r_1^m \quad s_1^m \quad \cdots \quad s_{T_m}^m \qquad m = 1, \ldots, M
$$

* For each trajectory

    * For each state in that trajectory

    * **slowly update along that trajectory using learning rate**: 
    $$
    V(s_t^m) \leftarrow V(s_t^m) + \alpha \left( g_t^m - V(s_t^m) \right)
    $$
    return is calculated backward and trace back + adding gamma:

    $$
    g_\tau^m = R_{t+1}^m + \gamma R_{t+2}^m + \gamma^2 R_{t+3}^m + \dots + \gamma^{T - t - 1} R_T^m
    $$

**Downside:** 

* **nearer to the beginning of a trajectory gets higher return** but it should be the opposite, where nearer states (more correlative) to terminal state. Because looking closely at return $G$ is actually just like $v_\pi(s)$, but $G$ is exclusively binded to the trajectory
* **Update rules not generalized enough** because the reward could only update within the trajectory. For e.g. there is a state that is very near the goal, but somehow random sampling only sample it leading to negative terminal => not correctly negatively updated

```
for episode in range(total_episodes):
  while step<max_steps_pr_episode:
    states.append(s)
    a = epsilon_greedy_Q_policy(s, Q_pi_hat)  # Pick action.
    actions.append(a)
    next_s, r, terminal, _ = env.step(a)  # Take a step in the environment.

  while (step goes backward):
    G = gamma * G + rewards[-1]
    rewards = rewards[:-1] #pop out the last step
    s = states[-1]
    states = states[:-1]
    a = actions[-1]
    actions = actions[:-1]
    Q_pi_hat[tuple(s)][a] = (1-alpha)*Q_pi_hat[tuple(s)][a] + alpha * G #kinda an average between old and new

```

#### 2. Temporal Difference (Q-Learning, SARSA)
* similar to Monte Carlo by sampling a lot of trajectories, but update:
    * in between directly without waiting for trajectory end
    * bootstrapping the action-state value of the successor state

##### Off-policy (Q-Learning) (use Bellman Optimality)
Update rule: immediate reward + difference between the best next successor state-action value and current
$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right)$$

The main keypoint here is that: the best next successor state-action value might not be the one chosen from that sampled trajectory => update policy != sampling policy

##### On-policy (SARSA) (not use Bellman Optimality)
Update rule: immediate reward + difference between the actual next successor state-action from that sampled trajectory => update binded to sampling policy
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$


**End effect of Temporal Difference:** 

* Update not trajectory dependent like MC => more balanced update
* Can update realtime (no need reverse order)
* the one near the terminal will directly rewarded the most with the most weight, because the term $\gamma \max_{a} Q(s_{t+1}, a)$ is then the terminal reward. Sometimes people actually expand this TD(n) so that the last n-states could get even directly to terminal state.

```
for i_episode in range(total_episodes):
    while step<max_steps_per_episode:
      a = epsilon_greedy_Q_policy(s_disc, Q=Q, eps=eps)  # Pick action.
      next_s, r, terminal, truncated, _ = env.step(a)  # Take a step in the environment.
      score += r
      delta = r + gamma * max(Q[tuple(next_s_disc)]) - Q[tuple(s_disc)][a]
      Q[tuple(s_disc)][a] += alpha * delta  # Q-learning update.

```

#### 3. Function Approximation DQN

**Goal: Interpolating Q-Values using a DNN**

* MSE Loss function between the network calculation and (linear regression)
* The ground-truth from bootstrapping the max of the next step from the table (Bellman-Optimality)

  $$
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
  $$

Technical tricks in implementation:

* **Each transition separately using replay buffer** stores each transition (s, a, s+1, r+1, done?), instead of training in a consecutive sequence like from sampling
*  **Rewards come from the environment**, Gymnasium give the reward directly => use this in Bellman Optimality to calculate Ground Truth

```
class QNetwork(nn.Module):
  # input state space representation, output action space representation

for i_episode in range(1, n_episodes+1):
    for t in range(max_t):
        action = agent.act(state, eps) # Epsilon-greedy
        next_state, reward, terminated, truncated, _ = env.step(action)
        agent.step(state, action, reward, next_state, terminated)

def step(self, state, action, reward, next_state, done):
    self.memory.add(state, action, reward, next_state, done)
    # learn update_every timestep
    self.t_step = (self.t_step + 1)  
        if (enough samples for a batch):
            experiences = self.memory.sample()
            self.learn(experiences, gamma)

def learn(self, experiences, gamma):
    states, actions, rewards, next_states, dones = experiences

    ### Calculate target value from bellman equation bootstrapping the next state value in this trajectory
    q_targets_next = self.qnetwork_target(next_states)
    q_targets = rewards + gamma * q_targets_next * (1 - dones)

    ### Calculate expected value from local network
    q_expected = self.qnetwork_local(states).gather(1, actions)

    loss = F.mse_loss(q_expected, q_targets)
    # update
```

## Policy Search (1 step)
**1 Step:** (state)->(probabilistic action policy)

### Policy Gradients (First Order)

**This one is Monte Carlo style, it waits until the end then update $\pi(a_t^m|s_t^m, \mathbf{\theta})$ backwards**

Objective function is still in general for every (action|state).

 
For each sampled trajectory:

  For each (state,action) in the trajectory:

  Calculate return backward:
  $$g_t^m \leftarrow r_{t+1}^m + \gamma r_{t+2}^m + \dots$$

  Advantage function to avoid variance problem:  
  $$\delta \leftarrow g_t^m - \hat{v}(s_t^m, \mathbf{w})$$
  
  But update only Update this specific policy for that (state,action) $\pi(a_t^m|s_t^m, \mathbf{\theta})$ using gradient descent. Plug in (state,action) to quantize and update theta:
  $$\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha^{\theta} \gamma^t \delta \nabla \ln \pi(a_t^m|s_t^m, \mathbf{\theta})$$


**End effect:** 

* directly learn the behavior instead of state-action value => more data efficient? because only 1 step directly to calculate action
* using gradient descent is classic (can use all relevant techniques)

**Disadvantage:**

* local minimum
* on-policy

* PPO adding clipping into loss function to avoid learning too much

implementation similar to Q-learning
```
class Policy_Network(nn.Module):
  #input state space, output action space

for episode in range(total_num_episodes):
	while not done:
		action = agent.act(obs)
		obs, reward, terminated, truncated, info = env.step(action)
		agent.rewards.append(reward)

	agent.learn() #finish the trajectory then start to learn

def learn(self):
    #calculate G for each timestep in the trajectory
    #calculate G backwards in Monte Carlo style
    for R in self.rewards[::-1]:
        running_g = R + self.gamma * running_g
        gs.insert(0, running_g)
    deltas = torch.tensor(gs)

    loss = 0
    # this time we define loss by ourselves
    # now loss for each timestep in the trajectory
    for log_prob, delta in zip(self.probs, deltas):
        loss += log_prob.mean() * delta * (-1)

    # Update the policy network
```

### Bayesian Optimization & Gaussian Processes (Zero-order)

* Objective function as above, **Goal: find theta that funciton's min/max** but:
    * Many optimum => cannot use gradient descent
    * Each evaluation step is expensive (could not sample so much)
    * Goal of GP: find posterior mean, variance

* **Solution:** GP is a probability distribution over all possible function, all seen sampled data build together a join distribution
  * Goal: posterior mean & variance of a new datapoint given prior some limited samples!!!
  * Key Knowledge to lean on: Covariance Matrix of jointly multivariate Gaussian distribution A,B where we already know how to calculate posterior mean and covariance of (A|B)
  * Formulate (yt,f*) in that same covariance matrix structure => we can calculate these posterior mean/covariance of (f*|yt) in exactly the same way
  * Covariance matrix based on the kernel function (RBF for Gaussian, or also be Euclidian distance) K_ij = k(xi, xj), calculate these for all pair of points and put it in the cov matrix

  * BUT... where to take a new datapoint?  apply acquisition function on random 1000 new datapoints (balance exploitation & exploration)! (EI: where variance & mean are the highest)
  * Multiple Objective BO actually easy, (e.g. best material: each fixed theta give different function with different max on the graph, test many theta to find many max points for each function => use Pareto)

Pro:

* Many optimum => cannot use gradient descent
* Each evaluation step is expensive (could not sample so much)

Cons:

* Difficult to scale to high-dimensional input space
* Computationally expensive because (distance function, acquisition function sampling, covariance matrix, needs to calculate a lot)
* Quality of the model dependent from use of appropriate kernel
* No guarrantee a REAL max

```
for i in range(1):
    y_next = true_function(x_next) #evaluate/sample
    bayes_opt.update_model(x_next, y_next) #update gaussian process
    x_next = bayes_opt.get_candidate(aq_func) #choose next data to sample

def update_model(self, x, y):
    self.x_samples = np.append(self.x_samples, x)
    self.y_samples = np.append(self.y_samples, y)
    self.surrogate_model.fit(self.x_samples, self.y_samples) #using true function to update the GP covariance matrix

self.surrogate_model = GaussianProcess(kernel=lambda x1, x2: np.exp(-.5 * np.subtract.outer(x1, x2)**2))

def get_candidate(self, acquisition_function, num_candidates=1000):
    x_canditates = np.linspace(self.bounds[0], self.bounds[1], num_candidates)
    mu,sigma = self.surrogate_model.predict(x_canditates) #calculate variance and mean from all other data
    aqf_values = acquisition_function(mu, sigma, np.max(self.y_samples))
    best_index = np.argmax(aqf_values) #choose the one with highest EI
```

**CartPole Example: find the set of gain K, each time evaluate in environment, and keep finding**

# MBRL
## MPC (known Forward Dynamics)

### LQR-based MPC for Linear model and quadratic cost function
**Goal: Bring back to LQR problem to solve as QR**
* We have to know desired states
* stack all sequences of action states and outputs into vectors
* why can we do that? because we have a model given
* Substitue back into the cost function, and represent it in a quadratic function format that we know how to solve (find U) numerically
* Solve Quadratic Function

### General MPC for Non-linear model vs Non-quadratic cost function
* We have to know desired states
* Sampling (try out different input with that dynamic model in the future) using the given dynamics
* Choose the one argmin the quadratic cost using gradient descent or Gaussian Processes (easy: desired states = zeros - actually computed)^2
* CEM: In each iteration, it samples action sequences from the distribution, evaluates them using the cost function, and then fits the distribution to the best performing sequences. This process is repeated until convergence.


```
# Sampling-Based Planner
def plan(state, desired_state, dynamics_model, objective_function, cost_weights, horizon = 50, num_candidates=20000):
	action_sequences = torch.distributions.uniform.Uniform(-1,1).sample((num_candidates,horizon)).to(state.device)
	costs = evaluate_action_sequences(state, action_sequences, desired_state, dynamics_model, objective_function, cost_weights)
	best_cand = costs.argmin()
	return action_sequences[best_cand,0]

# calculate a trajectory and evaluate it using the given objective function (quadratic cost)
def evaluate_action_sequences(state, action_sequences, desired_state=torch.zeros(4), dynamics_model=predict_next_states, objective_function=quadratic_cost, cost_weights = torch.tensor([1.,2.,0.,0.])):
    trajectories= calculate_trajectories(torch.tile(state,(len(action_sequences),1)), action_sequences, dynamics_model)
    return objective_function(trajectories, desired_state, cost_weights).sum(dim=1)

def predict_next_states(states, actions):

	# Non-linear dynamics for the cart-pole system
	def equations_of_motion(xs, thetas, xs_dot, thetas_dot, actions):
		# define all the dynamics in here
		return xs_dot, thetas_dot, xs_dot_dot, thetas_dot_dot

	# find the next state using simple Euler's method
	xs_dot, thetas_dot, xs_dot_dot, thetas_dot_dot = equations_of_motion(xs, thetas, xs_dot, thetas_dot, actions)
	xs_dot += xs_dot_dot * dt
	thetas_dot += thetas_dot_dot * dt
	xs += xs_dot * dt
	thetas += thetas_dot * dt

	return torch.stack([xs, thetas, xs_dot, thetas_dot]).T    
```

## Model-based Learning in general
Compare the performance of Model-Based RL and Model-Free RL agents (DQN, REINFORCE, AC,... are Model-Free)

Model-free:

* converge guarrantee
* simple implementation
* light computation (lighter iteration)
* BUT needs A LOT of interaction to learn (not data efficient)=> converge slower (many iteration)

Model-based:

* more data efficient, it learns from every data it rolled out
* but can converge quickly (not many iteration) (data efficient)
* BUT: A LOT of computation at each timestep (bc of rollout sampling planner (longer iteration))
* BUT: MB needs a dynamic model in advance, which is very difficult to have!
* BUT converge guarrantee, because sampled planning

## Learn Dynamics (unknown Dynamics)
* Problem: there are many states that can be presented differently, but in the sense of the task it is the same, so we do not want to overfit or learn irrelevant things => wrong behavior
* Solution: Autoencoder, pack information
### Simple Dynamics Network

* similar to DQN, but input (state, action) -> output (next state)
* also similar, each transition of the trajectory is saved into replay buffer, then learn each transition separately
* So yeah: Plan => separate transition to replay buffer => learn each
```
for i in range(100):
    while not done:
        action = agent.act(state)
        next_state, reward, terminal, truncated, _ = env.step(action)
        agent.step(state.astype(np.float32), action.astype(np.float32), reward, next_state.astype(np.float32), done)
        state = next_state

    def step(self, state, action, reward, next_state, done): #next_state is ground truth, input is current state, action
        # Learn every update_every time steps.
        if self.t_step % self.update_every == 0:
            if self.t_step > self.init_period:
                for _ in range(self.update_steps):
                    self.learn()
        self.t_step += 1

    def learn(self): # mean squared error to learn the (next state)
        self.model.train()
        td = self.replay_buffer.sample(self.batch_size)
        next_state_predcited = self.model(td['state'], td['action'])
        loss = F.mse_loss(next_state_predcited, td['next_state'])
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

# Neural Network as forward dynamics and reward model
class ForwardDynamicsModel(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim = 256):
        super(ForwardDynamicsModel, self).__init__()
        self.fc1 = nn.Linear(state_dim+action_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, state_dim)
```


### VAE-based (Dreamer)
**Goal:**
$$\mathcal{L}(\theta, \phi, \mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))$$

The VAE loss combines two terms:

* Reconstruction Loss: Measures how well the decoder reconstructs the input from the latent code (actually MSE derived from Gaussian distribution loss).
* KL Divergence: Bringing learned latent distribution to be close to each other (and also standard normal distribution) => smoothness and no holes 


#### Actual Dreamer (2 networks)
**Goal:** plan => separate transition => bring to a new encoded space, train on 2 networks:

1. Q-Network (encoded latent space)->(action): Purpose to **generalized the STATE** actually the same as Q-Network, but the input is not state, it is encoded image (env observation), Ground Truth taken from table as below (Loss can be MSE).
  $$
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
  $$

2. Reconstruction-Network (VAE): MSE loss to make sure the latent space we learned was correct, reconstruction loss also MSE for each pixel

```
for step in pbar:
    # Choose an action based on the current state
    action = vae_dqn_agent.act(state, explore=True)
    # Take the chosen action in the environment
    next_state, reward, terminal, truncated, _ = env.step(action)
    # Store the experience in the agent's replay buffer
    vae_dqn_agent.store_experience(tensordict of action, reward, next_pixel, dont)
    # Update the agent's Q-network
    vae_dqn_agent.update()

def act(self, state, explore=False): 
    #either random or just take the most rewarded action from Q-network

def update(self): #update both 2 networks at the same time
    self.update_vae()
    self.update_dqn()

def update_vae(self):
    # Encode states using VAE
    mu, log_var = self.vae.encode(states_norm)
    z = self.vae.reparameterize(mu, log_var)
    
    reconstructed_states = self.vae.decode(z)

    loss = self.vae_loss_fn(states_norm, reconstructed_states, mu, log_var)
    # then update using optimize
    ...

def update_dqn(self):
    ...
    q_values = self.q_network(latent_states).gather(1, actions.unsqueeze(1))
    next_q_values = self.target_network(latent_next_states).max(1)[0].detach()
    target_q_values = rewards + self.gamma * next_q_values * (1 - dones)

    loss = self.q_loss_fn(q_values, target_q_values.unsqueeze(1))
    # then update using optimizer
    ...
```

# Learning from Demonstration
* No exploration (as in RL), just demonstrate => fast
* Works well in the real world!
* BUT (Optimal) Demonstrations are not always available
* BUT can get stuck in unseen states

## Imitation Learning / Behavior CLoning
* Actually a Supervised DNN (input:state) -> (output:action), kinda easy
* We can also extend this network to VAE+Actor+Critic Networks, which I think that is BCQ does
```
for epoch in range(num_epochs):
        for batch in self.replay_buffer:
            observations, target_actions = batch['observations'], batch['actions'] #sample input & output from dataset
            actions = self.model(observations)
            loss = self.loss(actions, target_actions) #MSELoss
            self.optimizer.zero_grad()
            loss.backward()
```
* but, can we use this data to do a reinforcement learning? - No, because we should let RL **explore** to regularize the environment, just learning from this data makes it does not know what to do in unseen state **distributional shift**, it infact overestimate because it has never seen a bad example before 
* => Dagger


## Inversed RL
* given demonstration
* learn rewards function
* then RL back to explore and find optimal policy



