<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-19">

<title>Machine Learning 4 Robotics – Đinh Tiến Thắng</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Đinh Tiến Thắng</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Đinh Tiến Thắng</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Machine Learning 4 Robotics</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Model-based Learning</div>
                <div class="quarto-category">Reinforcement Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 19, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#control" id="toc-control" class="nav-link active" data-scroll-target="#control">Control</a>
  <ul class="collapse">
  <li><a href="#pid" id="toc-pid" class="nav-link" data-scroll-target="#pid">PID</a></li>
  <li><a href="#dynamics-system-id" id="toc-dynamics-system-id" class="nav-link" data-scroll-target="#dynamics-system-id">Dynamics System ID</a></li>
  <li><a href="#exploring-a-linear-feedback-state-control-in-lti-system" id="toc-exploring-a-linear-feedback-state-control-in-lti-system" class="nav-link" data-scroll-target="#exploring-a-linear-feedback-state-control-in-lti-system">Exploring a linear feedback-state control in LTI-System</a></li>
  <li><a href="#lqr" id="toc-lqr" class="nav-link" data-scroll-target="#lqr">LQR</a>
  <ul class="collapse">
  <li><a href="#the-goal-cost-function-to-minimize" id="toc-the-goal-cost-function-to-minimize" class="nav-link" data-scroll-target="#the-goal-cost-function-to-minimize">THE GOAL: Cost Function to minimize:</a></li>
  <li><a href="#key-knowledge-1-linear-system-dynamics" id="toc-key-knowledge-1-linear-system-dynamics" class="nav-link" data-scroll-target="#key-knowledge-1-linear-system-dynamics">Key Knowledge 1: Linear System Dynamics:</a></li>
  <li><a href="#key-knowledge-2-linear-controller" id="toc-key-knowledge-2-linear-controller" class="nav-link" data-scroll-target="#key-knowledge-2-linear-controller">Key Knowledge 2: Linear Controller</a></li>
  </ul></li>
  <li><a href="#finite-horizont-lqr-can-also-be-an-mpc" id="toc-finite-horizont-lqr-can-also-be-an-mpc" class="nav-link" data-scroll-target="#finite-horizont-lqr-can-also-be-an-mpc">Finite Horizont LQR (can also be an MPC)</a></li>
  </ul></li>
  <li><a href="#introduction-to-rl" id="toc-introduction-to-rl" class="nav-link" data-scroll-target="#introduction-to-rl">Introduction to RL</a></li>
  <li><a href="#model-free-rl" id="toc-model-free-rl" class="nav-link" data-scroll-target="#model-free-rl">Model-free RL</a>
  <ul class="collapse">
  <li><a href="#value-based-2-steps" id="toc-value-based-2-steps" class="nav-link" data-scroll-target="#value-based-2-steps">Value-based (2 steps)</a>
  <ul class="collapse">
  <li><a href="#value-iteration-approach-we-have-ps-r-mid-s-a" id="toc-value-iteration-approach-we-have-ps-r-mid-s-a" class="nav-link" data-scroll-target="#value-iteration-approach-we-have-ps-r-mid-s-a">Value Iteration approach (we have <span class="math inline">\(p(s', r \mid s, a)\)</span>)</a></li>
  <li><a href="#sampling-approach-no-access-to-ps-r-mid-s-a" id="toc-sampling-approach-no-access-to-ps-r-mid-s-a" class="nav-link" data-scroll-target="#sampling-approach-no-access-to-ps-r-mid-s-a">Sampling Approach (no access to <span class="math inline">\(p(s', r \mid s, a)\)</span>)</a></li>
  </ul></li>
  <li><a href="#policy-search-1-step" id="toc-policy-search-1-step" class="nav-link" data-scroll-target="#policy-search-1-step">Policy Search (1 step)</a>
  <ul class="collapse">
  <li><a href="#policy-gradients-first-order" id="toc-policy-gradients-first-order" class="nav-link" data-scroll-target="#policy-gradients-first-order">Policy Gradients (First Order)</a></li>
  <li><a href="#bayesian-optimization-gaussian-processes-zero-order" id="toc-bayesian-optimization-gaussian-processes-zero-order" class="nav-link" data-scroll-target="#bayesian-optimization-gaussian-processes-zero-order">Bayesian Optimization &amp; Gaussian Processes (Zero-order)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#mbrl" id="toc-mbrl" class="nav-link" data-scroll-target="#mbrl">MBRL</a>
  <ul class="collapse">
  <li><a href="#mpc-known-forward-dynamics" id="toc-mpc-known-forward-dynamics" class="nav-link" data-scroll-target="#mpc-known-forward-dynamics">MPC (known Forward Dynamics)</a>
  <ul class="collapse">
  <li><a href="#lqr-based-mpc-for-linear-model-and-quadratic-cost-function" id="toc-lqr-based-mpc-for-linear-model-and-quadratic-cost-function" class="nav-link" data-scroll-target="#lqr-based-mpc-for-linear-model-and-quadratic-cost-function">LQR-based MPC for Linear model and quadratic cost function</a></li>
  <li><a href="#general-mpc-for-non-linear-model-vs-non-quadratic-cost-function" id="toc-general-mpc-for-non-linear-model-vs-non-quadratic-cost-function" class="nav-link" data-scroll-target="#general-mpc-for-non-linear-model-vs-non-quadratic-cost-function">General MPC for Non-linear model vs Non-quadratic cost function</a></li>
  </ul></li>
  <li><a href="#model-based-learning-in-general" id="toc-model-based-learning-in-general" class="nav-link" data-scroll-target="#model-based-learning-in-general">Model-based Learning in general</a></li>
  <li><a href="#learn-dynamics-unknown-dynamics" id="toc-learn-dynamics-unknown-dynamics" class="nav-link" data-scroll-target="#learn-dynamics-unknown-dynamics">Learn Dynamics (unknown Dynamics)</a>
  <ul class="collapse">
  <li><a href="#vae-based-dreamer" id="toc-vae-based-dreamer" class="nav-link" data-scroll-target="#vae-based-dreamer">VAE-based (Dreamer)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#learning-from-demonstration" id="toc-learning-from-demonstration" class="nav-link" data-scroll-target="#learning-from-demonstration">Learning from Demonstration</a>
  <ul class="collapse">
  <li><a href="#imitation-learning-behavior-cloning" id="toc-imitation-learning-behavior-cloning" class="nav-link" data-scroll-target="#imitation-learning-behavior-cloning">Imitation Learning / Behavior CLoning</a></li>
  <li><a href="#inversed-rl" id="toc-inversed-rl" class="nav-link" data-scroll-target="#inversed-rl">Inversed RL</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="control" class="level1">
<h1>Control</h1>
<ul>
<li>Forward dynamics: given torque vector -&gt; angular position, velocity, acceleration at each joint</li>
<li>Backward dynamics: the inverse is more interesting</li>
</ul>
<section id="pid" class="level2">
<h2 class="anchored" data-anchor-id="pid">PID</h2>
<p>3 terms: Proportional Gain (Kp), Integral (Ki), Differential (Kd) * Proportional: we all know, current error strong -&gt; more gas, weak-&gt;less gas * Integral? for gradual decaying system (like car) -&gt; continuously effort a little more gas to compensate the persistent decay, otherwise P controller keep chasing it and never reach. BUT, it needs careful choose, because it is slow reaction, this is a drawback, and a term to overcome this drawback is… * Differential: fast change in error: it will correct! -&gt; can react on time (sharp turn, break,…), BUT from my experience this should not be too high bc it can be noisy</p>
</section>
<section id="dynamics-system-id" class="level2">
<h2 class="anchored" data-anchor-id="dynamics-system-id">Dynamics System ID</h2>
<ul>
<li>just generate bunch of data, then fit this data with any type regression model</li>
<li>linear or non-linear (then needs linearization around interested equilibrium like the cart pole)</li>
</ul>
</section>
<section id="exploring-a-linear-feedback-state-control-in-lti-system" class="level2">
<h2 class="anchored" data-anchor-id="exploring-a-linear-feedback-state-control-in-lti-system">Exploring a linear feedback-state control in LTI-System</h2>
<p>General idea is like this, implemented by a dot product: <span class="math display">\[u(t) = K e(t) = K_x (x(t) - x_{\text{target}}(t)) + K_\theta \theta(t) + K_{\dot{x}} \dot{x}(t) + K_{\dot{\theta}} \dot{\theta}(t)\]</span></p>
<pre><code>u = mj_data.ctrl = - np.dot(gain, error)</code></pre>
<p>we just adapt the gain K to see which one is good, but this is a manual work, so how to find the optimal gain automatically? LQR</p>
</section>
<section id="lqr" class="level2">
<h2 class="anchored" data-anchor-id="lqr">LQR</h2>
<section id="the-goal-cost-function-to-minimize" class="level3">
<h3 class="anchored" data-anchor-id="the-goal-cost-function-to-minimize">THE GOAL: Cost Function to minimize:</h3>
<p><span class="math inline">\(J = \int_{0}^{\infty} (\mathbf{s}(t)^T \mathbf{Q} \mathbf{s}(t) + u(t)^T R u(t)) \, dt\)</span></p>
<ul>
<li><p><strong>Q (State Weighting Matrix):</strong> Penalize state deviation <span class="math display">\[\mathbf{Q} = \text{diag}([1, 10, 0, 0]) = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 10 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}\]</span></p>
<p>This means:</p>
<ul>
<li>penalize the square of the cart’s position (<span class="math inline">\(x^2\)</span>) with a weight of 1.</li>
<li>heavily penalize the square of the pendulum’s angle (<span class="math inline">\(\theta^2\)</span>) with a weight of 10, indicating it’s crucial to keep the pendulum upright.</li>
<li>do not directly penalize the squared cart velocity (<span class="math inline">\(\dot{x}^2\)</span>) or pendulum angular velocity (<span class="math inline">\(\dot{\theta}^2\)</span>) in the cost function.</li>
</ul></li>
<li><p><strong>R (Control Weighting Matrix/Scalar):</strong> penalizes the magnitude of the control input. A larger <span class="math inline">\(R\)</span> encourage smoother control, but slower. smaller <span class="math inline">\(R\)</span> canverges faster, but may not always good for human and gearbox <span class="math display">\[R = 0.1\]</span> This means the square of the control input (<span class="math inline">\(u^2\)</span>) is penalized with a weight of 0.1.</p></li>
</ul>
</section>
<section id="key-knowledge-1-linear-system-dynamics" class="level3">
<h3 class="anchored" data-anchor-id="key-knowledge-1-linear-system-dynamics">Key Knowledge 1: Linear System Dynamics:</h3>
<p>we use Lagrange dynamics by hand to define matrices A and B manually then put in</p>
<p><span class="math inline">\(\dot{\mathbf{s}}(t) = \mathbf{A}\mathbf{s}(t) + \mathbf{B}u(t)\)</span></p>
<ul>
<li><p><strong>A (State Matrix):</strong> <span class="math display">\[\mathbf{A} = \begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; -\frac{g m}{M} &amp; 0 &amp; 0 \\ 0 &amp; \frac{g}{l}\left(1 + \frac{m}{M}\right) &amp; 0 &amp; 0 \end{bmatrix}\]</span></p></li>
<li><p><strong>B (Input Matrix):</strong> <span class="math display">\[\mathbf{B} = \begin{bmatrix} 0 \\ 0 \\ \frac{k}{M} \\ -\frac{k}{M l} \end{bmatrix}\]</span></p></li>
</ul>
<p>Where:</p>
<pre><code>* $g$: acceleration due to gravity
* $m$: mass of the pendulum (pole)
* $M$: mass of the cart
* $l$: length from the pivot to the center of mass of the pendulum
* $k$: gear ratio, converting the control signal $u$ into a physical force $F = k \cdot u$ applied to the cart.</code></pre>
</section>
<section id="key-knowledge-2-linear-controller" class="level3">
<h3 class="anchored" data-anchor-id="key-knowledge-2-linear-controller">Key Knowledge 2: Linear Controller</h3>
<p><strong>K (State Feedback Gain Matrix):</strong></p>
<ul>
<li><p>For a LTI-System with infinite horizon, we can set derivation of cost function = 0 and find out that <span class="math inline">\(u(t) = - \mathbf{K}\mathbf{s}(t)\)</span> at every time point the same <span class="math inline">\(K\)</span>.</p></li>
<li><p>For an LTI system with a finite horizon, K is time-varying because we set extra another cost at the end of horizont(calculated via DRE).</p></li>
<li><p>This is achieved by rephrasing the cost function above, then we end up with a function including two terms, both are quadratic terms (first one totally dependent on x, the second term where x and u are involved).</p></li>
<li><p>From the first one we found out that x and u are in linear relationship <span class="math display">\[\mathbf{u}^*(t) = -\mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\mathbf{x}(t)\]</span></p></li>
<li><p>But since we do not know how much is this K gain, so we also set the first term to 0, which is Algebraic Ricatti Equation, to find <span class="math inline">\(P\)</span>, and eventually find <span class="math inline">\(K = \mathbf{R}^{-1}\mathbf{B}^T\mathbf{P}\)</span></p>
<p>The <code>control.lqr(A,B,Q,R)</code> computes this: <span class="math display">\[\mathbf{K} = \begin{bmatrix} K_x &amp; K_\theta &amp; K_{\dot{x}} &amp; K_{\dot{\theta}} \end{bmatrix}\]</span> <span class="math display">\[u(t) = - (K_x x(t) + K_\theta \theta(t) + K_{\dot{x}} \dot{x}(t) + K_{\dot{\theta}} \dot{\theta}(t))\]</span></p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 2%">
<col style="width: 50%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Finite-Horizon LQR</th>
<th style="text-align: left;">Model Predictive Control (MPC)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Systems</strong></td>
<td style="text-align: left;">Limited to local linearization around an equilibrium.</td>
<td style="text-align: left;">Can handle non-linear system, because it is mostly just sampling different trajectories at each timestemp and choose the most optimal one</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Calculation</strong></td>
<td style="text-align: left;">Calculate <strong>once, offline</strong></td>
<td style="text-align: left;">Calculate <strong>repeatedly, online</strong> at each time step</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Gain K</strong></td>
<td style="text-align: left;">Pre-computed <strong>time-varying schedule</strong> <span class="math inline">\(\mathbf{K}(t)\)</span> for the full horizon. Even for finite horizont, at K(0), can calculate K(1), K(2), K(3),… arranging recursive relationship in a linear equation system, then substitue back into cost function, we can even solve it using either DRE or transforming in Quadratic Format</td>
<td style="text-align: left;">a sequence of open-loop controls is generated, and only the first is applied. If the system is linear, can apply LQR easily then no need trajectory sampling, but if not we need sampling</td>
</tr>
</tbody>
</table>
<div class="line-block"><strong>Computational Load</strong> | Low online load (just lookup) | High (solves an optimization problem at each step)</div>
</section>
</section>
<section id="finite-horizont-lqr-can-also-be-an-mpc" class="level2">
<h2 class="anchored" data-anchor-id="finite-horizont-lqr-can-also-be-an-mpc">Finite Horizont LQR (can also be an MPC)</h2>
<ul>
<li>step 1: taking dynamics A, B as input, formulate finite horizont stateinput sequences in a dynamic programmed (matrix) way <span class="math inline">\(X_k = \mathbf{\Phi} x(k) + \mathbf{\Gamma} U_k\)</span></li>
<li>step 2: substituting into J cost function (just the same as LQR), now we have <span class="math inline">\(J(U_k,x(k))\)</span> into <span class="math inline">\(\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}\)</span>.</li>
<li>using QP solver, because this is the format that the solution method is known</li>
<li>(only difference in MPC) - only apply first one, then re-calculate at the next step</li>
</ul>
</section>
</section>
<section id="introduction-to-rl" class="level1">
<h1>Introduction to RL</h1>
<ul>
<li>Environment: Markov Decision Process: chain of states and their relationship of actions, rewards, based on Markov Chain Property (only based on last state is MDP), but other properties like POMDP, or n-step DP.</li>
<li>MBRL: using Dynamic Programming</li>
<li>Model-free: Monte Carlo –(improve)— Temporal Difference (SARSA (on policy) vs Q-Learning (off-policy))</li>
</ul>
<p><strong>Goal</strong> find a policy <span class="math inline">\(\pi^*\)</span> that maximizes the discounted future return. <span class="math display">\[
\max_\pi \, \mathbb{E}_\pi \left[ \sum_{k = t+1}^{T} \gamma^{k - t - 1} R_k \right]
\]</span></p>
</section>
<section id="model-free-rl" class="level1">
<h1>Model-free RL</h1>
<section id="value-based-2-steps" class="level2">
<h2 class="anchored" data-anchor-id="value-based-2-steps">Value-based (2 steps)</h2>
<section id="value-iteration-approach-we-have-ps-r-mid-s-a" class="level3">
<h3 class="anchored" data-anchor-id="value-iteration-approach-we-have-ps-r-mid-s-a">Value Iteration approach (we have <span class="math inline">\(p(s', r \mid s, a)\)</span>)</h3>
<p>Each state value <span class="math inline">\(V\)</span>, or state-action value <span class="math inline">\(Q\)</span> can be represented <strong>RECURSIVELY</strong> by <strong>immediate reward R + neighboring state-action values weighted by transition actions propability distribution.</strong></p>
<p><strong>Keypoint</strong> Utilizing this <strong>Bellman optimality recursion</strong>, <strong>immediate reward R</strong>, they can help us converging to optimal <span class="math inline">\(V^*\)</span> and <span class="math inline">\(Q^*\)</span> by <strong>Value Iteration</strong>: that starts with random <span class="math inline">\(V\)</span>, <span class="math inline">\(Q\)</span>, but repeatedly sweeping Bellman optimality equation to all states/state-action pairs.</p>
<pre><code>policy = lambda s: [0.25,0.25,0.25,0.25]
V = np.zeros((4,4))
Q_pi = np.zeros((4,4,4))

for steps in range(200):
    for state_idx in range(16):
        action_probs = policy(state)
        subseq_states = np.clip(state+directions,0,3)
        V_subseq_states = np.array([V_pi[tuple(idx)] for idx in subseq_states])
        V_pi[state] = np.sum(action_probs*(rewards+gamma*V_subseq_states))
        Q_pi[state] = rewards + gamma * V_subseq_states</code></pre>
<p><span class="math display">\[V^*(s) = \max_{a \in \mathcal{A}} \left[ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s, a)(s') V^*(s') \right]\]</span></p>
<p><span class="math display">\[Q^*(s, a) = R(s, a) + \gamma \mathbb{E}_{s' \sim P(s,a)}[V^*(s')]\]</span></p>
<p>In the end when <span class="math inline">\(Q^*\)</span> are converged, we can find a deterministic <span class="math inline">\(\pi^*(s)\)</span> via:</p>
<p><span class="math display">\[\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s, a)\]</span></p>
</section>
<section id="sampling-approach-no-access-to-ps-r-mid-s-a" class="level3">
<h3 class="anchored" data-anchor-id="sampling-approach-no-access-to-ps-r-mid-s-a">Sampling Approach (no access to <span class="math inline">\(p(s', r \mid s, a)\)</span>)</h3>
<p><strong>2 steps:</strong> (state, action) -&gt; (value) -&gt; argmax deterministic (action)</p>
<p>every model-free algorithms has to work based on Monte Carlo for trajectory sampling.</p>
<section id="monte-carlo" class="level4">
<h4 class="anchored" data-anchor-id="monte-carlo">1. Monte Carlo</h4>
<ul>
<li>sample full trajectories</li>
<li>update at the end of each trajectory in reverse</li>
<li>epsilon-greedy =&gt; Q* != Q_pi. epsilon greedy to sample actions, which are mostly only suboptimal and actually leads to longer trajectory</li>
</ul>
<p><strong>Start</strong> with a random value function: <span class="math inline">\(V(s) = \frac{1}{|S|}\)</span></p>
<p><strong>Collect sampling trajectories</strong> <span class="math inline">\(M\)</span> trajectory samples: <span class="math display">\[
s_0^m \quad r_1^m \quad s_1^m \quad \cdots \quad s_{T_m}^m \qquad m = 1, \ldots, M
\]</span></p>
<ul>
<li><p>For each trajectory</p>
<ul>
<li><p>For each state in that trajectory</p></li>
<li><p><strong>slowly update along that trajectory using learning rate</strong>: <span class="math display">\[
  V(s_t^m) \leftarrow V(s_t^m) + \alpha \left( g_t^m - V(s_t^m) \right)
  \]</span> return is calculated backward and trace back + adding gamma:</p></li>
</ul>
<p><span class="math display">\[
  g_\tau^m = R_{t+1}^m + \gamma R_{t+2}^m + \gamma^2 R_{t+3}^m + \dots + \gamma^{T - t - 1} R_T^m
  \]</span></p></li>
</ul>
<p><strong>Downside:</strong></p>
<ul>
<li><strong>nearer to the beginning of a trajectory gets higher return</strong> but it should be the opposite, where nearer states (more correlative) to terminal state. Because looking closely at return <span class="math inline">\(G\)</span> is actually just like <span class="math inline">\(v_\pi(s)\)</span>, but <span class="math inline">\(G\)</span> is exclusively binded to the trajectory</li>
<li><strong>Update rules not generalized enough</strong> because the reward could only update within the trajectory. For e.g.&nbsp;there is a state that is very near the goal, but somehow random sampling only sample it leading to negative terminal =&gt; not correctly negatively updated</li>
</ul>
<pre><code>for episode in range(total_episodes):
  while step&lt;max_steps_pr_episode:
    states.append(s)
    a = epsilon_greedy_Q_policy(s, Q_pi_hat)  # Pick action.
    actions.append(a)
    next_s, r, terminal, _ = env.step(a)  # Take a step in the environment.

  while (step goes backward):
    G = gamma * G + rewards[-1]
    rewards = rewards[:-1] #pop out the last step
    s = states[-1]
    states = states[:-1]
    a = actions[-1]
    actions = actions[:-1]
    Q_pi_hat[tuple(s)][a] = (1-alpha)*Q_pi_hat[tuple(s)][a] + alpha * G #kinda an average between old and new
</code></pre>
</section>
<section id="temporal-difference-q-learning-sarsa" class="level4">
<h4 class="anchored" data-anchor-id="temporal-difference-q-learning-sarsa">2. Temporal Difference (Q-Learning, SARSA)</h4>
<ul>
<li>similar to Monte Carlo by sampling a lot of trajectories, but update:
<ul>
<li>in between directly without waiting for trajectory end</li>
<li>bootstrapping the action-state value of the successor state</li>
</ul></li>
</ul>
<section id="off-policy-q-learning-use-bellman-optimality" class="level5">
<h5 class="anchored" data-anchor-id="off-policy-q-learning-use-bellman-optimality">Off-policy (Q-Learning) (use Bellman Optimality)</h5>
<p>Update rule: immediate reward + difference between the best next successor state-action value and current <span class="math display">\[Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right)\]</span></p>
<p>The main keypoint here is that: the best next successor state-action value might not be the one chosen from that sampled trajectory =&gt; update policy != sampling policy</p>
</section>
<section id="on-policy-sarsa-not-use-bellman-optimality" class="level5">
<h5 class="anchored" data-anchor-id="on-policy-sarsa-not-use-bellman-optimality">On-policy (SARSA) (not use Bellman Optimality)</h5>
<p>Update rule: immediate reward + difference between the actual next successor state-action from that sampled trajectory =&gt; update binded to sampling policy <span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\]</span></p>
<p><strong>End effect of Temporal Difference:</strong></p>
<ul>
<li>Update not trajectory dependent like MC =&gt; more balanced update</li>
<li>Can update realtime (no need reverse order)</li>
<li>the one near the terminal will directly rewarded the most with the most weight, because the term <span class="math inline">\(\gamma \max_{a} Q(s_{t+1}, a)\)</span> is then the terminal reward. Sometimes people actually expand this TD(n) so that the last n-states could get even directly to terminal state.</li>
</ul>
<pre><code>for i_episode in range(total_episodes):
    while step&lt;max_steps_per_episode:
      a = epsilon_greedy_Q_policy(s_disc, Q=Q, eps=eps)  # Pick action.
      next_s, r, terminal, truncated, _ = env.step(a)  # Take a step in the environment.
      score += r
      delta = r + gamma * max(Q[tuple(next_s_disc)]) - Q[tuple(s_disc)][a]
      Q[tuple(s_disc)][a] += alpha * delta  # Q-learning update.
</code></pre>
</section>
</section>
<section id="function-approximation-dqn" class="level4">
<h4 class="anchored" data-anchor-id="function-approximation-dqn">3. Function Approximation DQN</h4>
<p><strong>Goal: Interpolating Q-Values using a DNN</strong></p>
<ul>
<li><p>MSE Loss function between the network calculation and (linear regression)</p></li>
<li><p>The ground-truth from bootstrapping the max of the next step from the table (Bellman-Optimality)</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
\]</span></p></li>
</ul>
<p>Technical tricks in implementation:</p>
<ul>
<li><strong>Each transition separately using replay buffer</strong> stores each transition (s, a, s+1, r+1, done?), instead of training in a consecutive sequence like from sampling</li>
<li><strong>Rewards come from the environment</strong>, Gymnasium give the reward directly =&gt; use this in Bellman Optimality to calculate Ground Truth</li>
</ul>
<pre><code>class QNetwork(nn.Module):
  # input state space representation, output action space representation

for i_episode in range(1, n_episodes+1):
    for t in range(max_t):
        action = agent.act(state, eps) # Epsilon-greedy
        next_state, reward, terminated, truncated, _ = env.step(action)
        agent.step(state, action, reward, next_state, terminated)

def step(self, state, action, reward, next_state, done):
    self.memory.add(state, action, reward, next_state, done)
    # learn update_every timestep
    self.t_step = (self.t_step + 1)  
        if (enough samples for a batch):
            experiences = self.memory.sample()
            self.learn(experiences, gamma)

def learn(self, experiences, gamma):
    states, actions, rewards, next_states, dones = experiences

    ### Calculate target value from bellman equation bootstrapping the next state value in this trajectory
    q_targets_next = self.qnetwork_target(next_states)
    q_targets = rewards + gamma * q_targets_next * (1 - dones)

    ### Calculate expected value from local network
    q_expected = self.qnetwork_local(states).gather(1, actions)

    loss = F.mse_loss(q_expected, q_targets)
    # update</code></pre>
</section>
</section>
</section>
<section id="policy-search-1-step" class="level2">
<h2 class="anchored" data-anchor-id="policy-search-1-step">Policy Search (1 step)</h2>
<p><strong>1 Step:</strong> (state)-&gt;(probabilistic action policy)</p>
<section id="policy-gradients-first-order" class="level3">
<h3 class="anchored" data-anchor-id="policy-gradients-first-order">Policy Gradients (First Order)</h3>
<p><strong>This one is Monte Carlo style, it waits until the end then update <span class="math inline">\(\pi(a_t^m|s_t^m, \mathbf{\theta})\)</span> backwards</strong></p>
<p>Objective function is still in general for every (action|state).</p>
<p>For each sampled trajectory:</p>
<p>For each (state,action) in the trajectory:</p>
<p>Calculate return backward: <span class="math display">\[g_t^m \leftarrow r_{t+1}^m + \gamma r_{t+2}^m + \dots\]</span></p>
<p>Advantage function to avoid variance problem:<br>
<span class="math display">\[\delta \leftarrow g_t^m - \hat{v}(s_t^m, \mathbf{w})\]</span></p>
<p>But update only Update this specific policy for that (state,action) <span class="math inline">\(\pi(a_t^m|s_t^m, \mathbf{\theta})\)</span> using gradient descent. Plug in (state,action) to quantize and update theta: <span class="math display">\[\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha^{\theta} \gamma^t \delta \nabla \ln \pi(a_t^m|s_t^m, \mathbf{\theta})\]</span></p>
<p><strong>End effect:</strong></p>
<ul>
<li>directly learn the behavior instead of state-action value =&gt; more data efficient? because only 1 step directly to calculate action</li>
<li>using gradient descent is classic (can use all relevant techniques)</li>
</ul>
<p><strong>Disadvantage:</strong></p>
<ul>
<li><p>local minimum</p></li>
<li><p>on-policy</p></li>
<li><p>PPO adding clipping into loss function to avoid learning too much</p></li>
</ul>
<p>implementation similar to Q-learning</p>
<pre><code>class Policy_Network(nn.Module):
  #input state space, output action space

for episode in range(total_num_episodes):
    while not done:
        action = agent.act(obs)
        obs, reward, terminated, truncated, info = env.step(action)
        agent.rewards.append(reward)

    agent.learn() #finish the trajectory then start to learn

def learn(self):
    #calculate G for each timestep in the trajectory
    #calculate G backwards in Monte Carlo style
    for R in self.rewards[::-1]:
        running_g = R + self.gamma * running_g
        gs.insert(0, running_g)
    deltas = torch.tensor(gs)

    loss = 0
    # this time we define loss by ourselves
    # now loss for each timestep in the trajectory
    for log_prob, delta in zip(self.probs, deltas):
        loss += log_prob.mean() * delta * (-1)

    # Update the policy network</code></pre>
</section>
<section id="bayesian-optimization-gaussian-processes-zero-order" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-optimization-gaussian-processes-zero-order">Bayesian Optimization &amp; Gaussian Processes (Zero-order)</h3>
<ul>
<li>Objective function as above, <strong>Goal: find theta that funciton’s min/max</strong> but:
<ul>
<li>Many optimum =&gt; cannot use gradient descent</li>
<li>Each evaluation step is expensive (could not sample so much)</li>
<li>Goal of GP: find posterior mean, variance</li>
</ul></li>
<li><strong>Solution:</strong> GP is a probability distribution over all possible function, all seen sampled data build together a join distribution
<ul>
<li><p>Goal: posterior mean &amp; variance of a new datapoint given prior some limited samples!!!</p></li>
<li><p>Key Knowledge to lean on: Covariance Matrix of jointly multivariate Gaussian distribution A,B where we already know how to calculate posterior mean and covariance of (A|B)</p></li>
<li><p>Formulate (yt,f<em>) in that same covariance matrix structure =&gt; we can calculate these posterior mean/covariance of (f</em>|yt) in exactly the same way</p></li>
<li><p>Covariance matrix based on the kernel function (RBF for Gaussian, or also be Euclidian distance) K_ij = k(xi, xj), calculate these for all pair of points and put it in the cov matrix</p></li>
<li><p>BUT… where to take a new datapoint? apply acquisition function on random 1000 new datapoints (balance exploitation &amp; exploration)! (EI: where variance &amp; mean are the highest)</p></li>
<li><p>Multiple Objective BO actually easy, (e.g.&nbsp;best material: each fixed theta give different function with different max on the graph, test many theta to find many max points for each function =&gt; use Pareto)</p></li>
</ul></li>
</ul>
<p>Pro:</p>
<ul>
<li>Many optimum =&gt; cannot use gradient descent</li>
<li>Each evaluation step is expensive (could not sample so much)</li>
</ul>
<p>Cons:</p>
<ul>
<li>Difficult to scale to high-dimensional input space</li>
<li>Computationally expensive because (distance function, acquisition function sampling, covariance matrix, needs to calculate a lot)</li>
<li>Quality of the model dependent from use of appropriate kernel</li>
<li>No guarrantee a REAL max</li>
</ul>
<pre><code>for i in range(1):
    y_next = true_function(x_next) #evaluate/sample
    bayes_opt.update_model(x_next, y_next) #update gaussian process
    x_next = bayes_opt.get_candidate(aq_func) #choose next data to sample

def update_model(self, x, y):
    self.x_samples = np.append(self.x_samples, x)
    self.y_samples = np.append(self.y_samples, y)
    self.surrogate_model.fit(self.x_samples, self.y_samples) #using true function to update the GP covariance matrix

self.surrogate_model = GaussianProcess(kernel=lambda x1, x2: np.exp(-.5 * np.subtract.outer(x1, x2)**2))

def get_candidate(self, acquisition_function, num_candidates=1000):
    x_canditates = np.linspace(self.bounds[0], self.bounds[1], num_candidates)
    mu,sigma = self.surrogate_model.predict(x_canditates) #calculate variance and mean from all other data
    aqf_values = acquisition_function(mu, sigma, np.max(self.y_samples))
    best_index = np.argmax(aqf_values) #choose the one with highest EI</code></pre>
<p><strong>CartPole Example: find the set of gain K, each time evaluate in environment, and keep finding</strong></p>
</section>
</section>
</section>
<section id="mbrl" class="level1">
<h1>MBRL</h1>
<section id="mpc-known-forward-dynamics" class="level2">
<h2 class="anchored" data-anchor-id="mpc-known-forward-dynamics">MPC (known Forward Dynamics)</h2>
<section id="lqr-based-mpc-for-linear-model-and-quadratic-cost-function" class="level3">
<h3 class="anchored" data-anchor-id="lqr-based-mpc-for-linear-model-and-quadratic-cost-function">LQR-based MPC for Linear model and quadratic cost function</h3>
<p><strong>Goal: Bring back to LQR problem to solve as QR</strong> * We have to know desired states * stack all sequences of action states and outputs into vectors * why can we do that? because we have a model given * Substitue back into the cost function, and represent it in a quadratic function format that we know how to solve (find U) numerically * Solve Quadratic Function</p>
</section>
<section id="general-mpc-for-non-linear-model-vs-non-quadratic-cost-function" class="level3">
<h3 class="anchored" data-anchor-id="general-mpc-for-non-linear-model-vs-non-quadratic-cost-function">General MPC for Non-linear model vs Non-quadratic cost function</h3>
<ul>
<li>We have to know desired states</li>
<li>Sampling (try out different input with that dynamic model in the future) using the given dynamics</li>
<li>Choose the one argmin the quadratic cost using gradient descent or Gaussian Processes (easy: desired states = zeros - actually computed)^2</li>
<li>CEM: In each iteration, it samples action sequences from the distribution, evaluates them using the cost function, and then fits the distribution to the best performing sequences. This process is repeated until convergence.</li>
</ul>
<pre><code># Sampling-Based Planner
def plan(state, desired_state, dynamics_model, objective_function, cost_weights, horizon = 50, num_candidates=20000):
    action_sequences = torch.distributions.uniform.Uniform(-1,1).sample((num_candidates,horizon)).to(state.device)
    costs = evaluate_action_sequences(state, action_sequences, desired_state, dynamics_model, objective_function, cost_weights)
    best_cand = costs.argmin()
    return action_sequences[best_cand,0]

# calculate a trajectory and evaluate it using the given objective function (quadratic cost)
def evaluate_action_sequences(state, action_sequences, desired_state=torch.zeros(4), dynamics_model=predict_next_states, objective_function=quadratic_cost, cost_weights = torch.tensor([1.,2.,0.,0.])):
    trajectories= calculate_trajectories(torch.tile(state,(len(action_sequences),1)), action_sequences, dynamics_model)
    return objective_function(trajectories, desired_state, cost_weights).sum(dim=1)

def predict_next_states(states, actions):

    # Non-linear dynamics for the cart-pole system
    def equations_of_motion(xs, thetas, xs_dot, thetas_dot, actions):
        # define all the dynamics in here
        return xs_dot, thetas_dot, xs_dot_dot, thetas_dot_dot

    # find the next state using simple Euler's method
    xs_dot, thetas_dot, xs_dot_dot, thetas_dot_dot = equations_of_motion(xs, thetas, xs_dot, thetas_dot, actions)
    xs_dot += xs_dot_dot * dt
    thetas_dot += thetas_dot_dot * dt
    xs += xs_dot * dt
    thetas += thetas_dot * dt

    return torch.stack([xs, thetas, xs_dot, thetas_dot]).T    </code></pre>
</section>
</section>
<section id="model-based-learning-in-general" class="level2">
<h2 class="anchored" data-anchor-id="model-based-learning-in-general">Model-based Learning in general</h2>
<p>Compare the performance of Model-Based RL and Model-Free RL agents (DQN, REINFORCE, AC,… are Model-Free)</p>
<p>Model-free:</p>
<ul>
<li>converge guarrantee</li>
<li>simple implementation</li>
<li>light computation (lighter iteration)</li>
<li>BUT needs A LOT of interaction to learn (not data efficient)=&gt; converge slower (many iteration)</li>
</ul>
<p>Model-based:</p>
<ul>
<li>more data efficient, it learns from every data it rolled out</li>
<li>but can converge quickly (not many iteration) (data efficient)</li>
<li>BUT: A LOT of computation at each timestep (bc of rollout sampling planner (longer iteration))</li>
<li>BUT: MB needs a dynamic model in advance, which is very difficult to have!</li>
<li>BUT converge guarrantee, because sampled planning</li>
</ul>
</section>
<section id="learn-dynamics-unknown-dynamics" class="level2">
<h2 class="anchored" data-anchor-id="learn-dynamics-unknown-dynamics">Learn Dynamics (unknown Dynamics)</h2>
<ul>
<li><p>Problem: there are many states that can be presented differently, but in the sense of the task it is the same, so we do not want to overfit or learn irrelevant things =&gt; wrong behavior</p></li>
<li><p>Solution: Autoencoder, pack information ### Simple Dynamics Network</p></li>
<li><p>similar to DQN, but input (state, action) -&gt; output (next state)</p></li>
<li><p>also similar, each transition of the trajectory is saved into replay buffer, then learn each transition separately</p></li>
<li><p>So yeah: Plan =&gt; separate transition to replay buffer =&gt; learn each</p></li>
</ul>
<pre><code>for i in range(100):
    while not done:
        action = agent.act(state)
        next_state, reward, terminal, truncated, _ = env.step(action)
        agent.step(state.astype(np.float32), action.astype(np.float32), reward, next_state.astype(np.float32), done)
        state = next_state

    def step(self, state, action, reward, next_state, done): #next_state is ground truth, input is current state, action
        # Learn every update_every time steps.
        if self.t_step % self.update_every == 0:
            if self.t_step &gt; self.init_period:
                for _ in range(self.update_steps):
                    self.learn()
        self.t_step += 1

    def learn(self): # mean squared error to learn the (next state)
        self.model.train()
        td = self.replay_buffer.sample(self.batch_size)
        next_state_predcited = self.model(td['state'], td['action'])
        loss = F.mse_loss(next_state_predcited, td['next_state'])
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

# Neural Network as forward dynamics and reward model
class ForwardDynamicsModel(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim = 256):
        super(ForwardDynamicsModel, self).__init__()
        self.fc1 = nn.Linear(state_dim+action_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, state_dim)</code></pre>
<section id="vae-based-dreamer" class="level3">
<h3 class="anchored" data-anchor-id="vae-based-dreamer">VAE-based (Dreamer)</h3>
<p><strong>Goal:</strong> <span class="math display">\[\mathcal{L}(\theta, \phi, \mathbf{x}) = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] = \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})} [\log p_\theta(\mathbf{x}|\mathbf{z})] - D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))\]</span></p>
<p>The VAE loss combines two terms:</p>
<ul>
<li>Reconstruction Loss: Measures how well the decoder reconstructs the input from the latent code (actually MSE derived from Gaussian distribution loss).</li>
<li>KL Divergence: Bringing learned latent distribution to be close to each other (and also standard normal distribution) =&gt; smoothness and no holes</li>
</ul>
<section id="actual-dreamer-2-networks" class="level4">
<h4 class="anchored" data-anchor-id="actual-dreamer-2-networks">Actual Dreamer (2 networks)</h4>
<p><strong>Goal:</strong> plan =&gt; separate transition =&gt; bring to a new encoded space, train on 2 networks:</p>
<ol type="1">
<li><p>Q-Network (encoded latent space)-&gt;(action): Purpose to <strong>generalized the STATE</strong> actually the same as Q-Network, but the input is not state, it is encoded image (env observation), Ground Truth taken from table as below (Loss can be MSE). <span class="math display">\[
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
  \]</span></p></li>
<li><p>Reconstruction-Network (VAE): MSE loss to make sure the latent space we learned was correct, reconstruction loss also MSE for each pixel</p></li>
</ol>
<pre><code>for step in pbar:
    # Choose an action based on the current state
    action = vae_dqn_agent.act(state, explore=True)
    # Take the chosen action in the environment
    next_state, reward, terminal, truncated, _ = env.step(action)
    # Store the experience in the agent's replay buffer
    vae_dqn_agent.store_experience(tensordict of action, reward, next_pixel, dont)
    # Update the agent's Q-network
    vae_dqn_agent.update()

def act(self, state, explore=False): 
    #either random or just take the most rewarded action from Q-network

def update(self): #update both 2 networks at the same time
    self.update_vae()
    self.update_dqn()

def update_vae(self):
    # Encode states using VAE
    mu, log_var = self.vae.encode(states_norm)
    z = self.vae.reparameterize(mu, log_var)
    
    reconstructed_states = self.vae.decode(z)

    loss = self.vae_loss_fn(states_norm, reconstructed_states, mu, log_var)
    # then update using optimize
    ...

def update_dqn(self):
    ...
    q_values = self.q_network(latent_states).gather(1, actions.unsqueeze(1))
    next_q_values = self.target_network(latent_next_states).max(1)[0].detach()
    target_q_values = rewards + self.gamma * next_q_values * (1 - dones)

    loss = self.q_loss_fn(q_values, target_q_values.unsqueeze(1))
    # then update using optimizer
    ...</code></pre>
</section>
</section>
</section>
</section>
<section id="learning-from-demonstration" class="level1">
<h1>Learning from Demonstration</h1>
<ul>
<li>No exploration (as in RL), just demonstrate =&gt; fast</li>
<li>Works well in the real world!</li>
<li>BUT (Optimal) Demonstrations are not always available</li>
<li>BUT can get stuck in unseen states</li>
</ul>
<section id="imitation-learning-behavior-cloning" class="level2">
<h2 class="anchored" data-anchor-id="imitation-learning-behavior-cloning">Imitation Learning / Behavior CLoning</h2>
<ul>
<li>Actually a Supervised DNN (input:state) -&gt; (output:action), kinda easy</li>
<li>We can also extend this network to VAE+Actor+Critic Networks, which I think that is BCQ does</li>
</ul>
<pre><code>for epoch in range(num_epochs):
        for batch in self.replay_buffer:
            observations, target_actions = batch['observations'], batch['actions'] #sample input &amp; output from dataset
            actions = self.model(observations)
            loss = self.loss(actions, target_actions) #MSELoss
            self.optimizer.zero_grad()
            loss.backward()</code></pre>
<ul>
<li>but, can we use this data to do a reinforcement learning? - No, because we should let RL <strong>explore</strong> to regularize the environment, just learning from this data makes it does not know what to do in unseen state <strong>distributional shift</strong>, it infact overestimate because it has never seen a bad example before</li>
<li>=&gt; Dagger</li>
</ul>
</section>
<section id="inversed-rl" class="level2">
<h2 class="anchored" data-anchor-id="inversed-rl">Inversed RL</h2>
<ul>
<li>given demonstration</li>
<li>learn rewards function</li>
<li>then RL back to explore and find optimal policy</li>
</ul>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/tienthangdinh\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>