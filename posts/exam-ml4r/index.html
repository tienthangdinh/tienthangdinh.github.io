<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-07-19">

<title>Exam ML4R – Đinh Tiến Thắng</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-db03927a41f77a8af5287a812d7101f4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Đinh Tiến Thắng</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Đinh Tiến Thắng</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Exam ML4R</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Model-based Learning</div>
                <div class="quarto-category">Reinforcement Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 19, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#control" id="toc-control" class="nav-link active" data-scroll-target="#control">Control</a>
  <ul class="collapse">
  <li><a href="#exploring-a-linear-feedback-state-control" id="toc-exploring-a-linear-feedback-state-control" class="nav-link" data-scroll-target="#exploring-a-linear-feedback-state-control">Exploring a linear feedback-state control</a></li>
  <li><a href="#now-we-structure-the-control-to-lqr" id="toc-now-we-structure-the-control-to-lqr" class="nav-link" data-scroll-target="#now-we-structure-the-control-to-lqr">Now we structure the control to LQR</a>
  <ul class="collapse">
  <li><a href="#system-dynamics-matrices" id="toc-system-dynamics-matrices" class="nav-link" data-scroll-target="#system-dynamics-matrices">1. System Dynamics Matrices:</a></li>
  <li><a href="#cost-function-matrices" id="toc-cost-function-matrices" class="nav-link" data-scroll-target="#cost-function-matrices">2. Cost Function Matrices:</a></li>
  <li><a href="#optimal-gain-matrix" id="toc-optimal-gain-matrix" class="nav-link" data-scroll-target="#optimal-gain-matrix">3. Optimal Gain Matrix:</a></li>
  </ul></li>
  <li><a href="#mpc" id="toc-mpc" class="nav-link" data-scroll-target="#mpc">MPC</a></li>
  </ul></li>
  <li><a href="#introduction-to-rl" id="toc-introduction-to-rl" class="nav-link" data-scroll-target="#introduction-to-rl">Introduction to RL</a>
  <ul class="collapse">
  <li><a href="#model-based" id="toc-model-based" class="nav-link" data-scroll-target="#model-based">Model-based</a>
  <ul class="collapse">
  <li><a href="#bellman-optimally-using-value-iteration" id="toc-bellman-optimally-using-value-iteration" class="nav-link" data-scroll-target="#bellman-optimally-using-value-iteration">Bellman Optimally using Value Iteration</a></li>
  </ul></li>
  <li><a href="#model-free" id="toc-model-free" class="nav-link" data-scroll-target="#model-free">Model-free</a>
  <ul class="collapse">
  <li><a href="#learning-v-and-q-from-data-from-monte-carlo-samples" id="toc-learning-v-and-q-from-data-from-monte-carlo-samples" class="nav-link" data-scroll-target="#learning-v-and-q-from-data-from-monte-carlo-samples">Learning V* and Q* from data from Monte Carlo Samples</a></li>
  <li><a href="#mc-upgrade-to-temporal-difference-specifically-q-learning" id="toc-mc-upgrade-to-temporal-difference-specifically-q-learning" class="nav-link" data-scroll-target="#mc-upgrade-to-temporal-difference-specifically-q-learning">MC Upgrade to Temporal Difference (specifically Q-Learning)</a></li>
  <li><a href="#dqn-representing-state-using-theta" id="toc-dqn-representing-state-using-theta" class="nav-link" data-scroll-target="#dqn-representing-state-using-theta">DQN Representing state using theta</a></li>
  <li><a href="#policy-gradients-representing-policy-using-theta" id="toc-policy-gradients-representing-policy-using-theta" class="nav-link" data-scroll-target="#policy-gradients-representing-policy-using-theta">Policy Gradients Representing policy using theta</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bayesian-optimization-gaussian-processes" id="toc-bayesian-optimization-gaussian-processes" class="nav-link" data-scroll-target="#bayesian-optimization-gaussian-processes">Bayesian Optimization &amp; Gaussian Processes</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="control" class="level1">
<h1>Control</h1>
<section id="exploring-a-linear-feedback-state-control" class="level2">
<h2 class="anchored" data-anchor-id="exploring-a-linear-feedback-state-control">Exploring a linear feedback-state control</h2>
<pre><code>mj_model = mujoco.mjModel(XML)
mj_data = mujoco.mjData(mj_model)
mj_renderer = mujoco.Renderer(mj_model)
u = mj_data.ctrl = - np.clip(np.dot(gain, state_around_target), -1, 1) # u = -K*(s-st)
</code></pre>
<p><span class="math display">\[u(t) = K e(t) = K_x (x(t) - x_{\text{target}}(t)) + K_\theta \theta(t) + K_{\dot{x}} \dot{x}(t) + K_{\dot{\theta}} \dot{\theta}(t)\]</span></p>
<p>we just adapt the gain K to see which one is good, but this is a manual work, so we can find an optimal using LQR method</p>
</section>
<section id="now-we-structure-the-control-to-lqr" class="level2">
<h2 class="anchored" data-anchor-id="now-we-structure-the-control-to-lqr">Now we structure the control to LQR</h2>
<p>then we use Lagrange by hand to define matrices A and B manually then put in</p>
<p>Here’s the mathematical formulation of A, B, Q, R, and K for your LQR example in Markdown:</p>
<section id="system-dynamics-matrices" class="level3">
<h3 class="anchored" data-anchor-id="system-dynamics-matrices">1. System Dynamics Matrices:</h3>
<p>These matrices define the linearized, continuous-time state-space model of the cart-pole system:</p>
<p><span class="math inline">\(\dot{\mathbf{s}}(t) = \mathbf{A}\mathbf{s}(t) + \mathbf{B}u(t)\)</span></p>
<ul>
<li><strong>A (State Matrix):</strong> This matrix describes how the system’s current state affects its future state <em>without</em> any control input. It represents the inherent dynamics of the system. In your example: <span class="math display">\[\mathbf{A} = \begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; -\frac{g m}{M} &amp; 0 &amp; 0 \\ 0 &amp; \frac{g}{l}\left(1 + \frac{m}{M}\right) &amp; 0 &amp; 0 \end{bmatrix}\]</span> Where:
<ul>
<li><span class="math inline">\(g\)</span>: acceleration due to gravity</li>
<li><span class="math inline">\(m\)</span>: mass of the pendulum (pole)</li>
<li><span class="math inline">\(M\)</span>: mass of the cart</li>
<li><span class="math inline">\(l\)</span>: length from the pivot to the center of mass of the pendulum</li>
</ul></li>
<li><strong>B (Input Matrix):</strong> This matrix describes how the control input <span class="math inline">\(u(t)\)</span> affects the system’s state. In your example: <span class="math display">\[\mathbf{B} = \begin{bmatrix} 0 \\ 0 \\ \frac{k}{M} \\ -\frac{k}{M l} \end{bmatrix}\]</span> Where:
<ul>
<li><span class="math inline">\(k\)</span>: gear ratio, converting the control signal <span class="math inline">\(u\)</span> into a physical force <span class="math inline">\(F = k \cdot u\)</span> applied to the cart.</li>
<li><span class="math inline">\(M\)</span>: mass of the cart</li>
<li><span class="math inline">\(l\)</span>: length from the pivot to the center of mass of the pendulum</li>
</ul></li>
</ul>
</section>
<section id="cost-function-matrices" class="level3">
<h3 class="anchored" data-anchor-id="cost-function-matrices">2. Cost Function Matrices:</h3>
<p>These matrices define the quadratic cost function that the LQR controller aims to minimize. The cost function penalizes deviations from the desired state and the control effort.</p>
<p><span class="math inline">\(J = \int_{0}^{\infty} (\mathbf{s}(t)^T \mathbf{Q} \mathbf{s}(t) + u(t)^T R u(t)) \, dt\)</span></p>
<ul>
<li><strong>Q (State Weighting Matrix):</strong> This is a symmetric positive semi-definite matrix that assigns penalties to deviations of each state variable from its desired value (typically zero for stabilization). A larger value on the diagonal means a higher penalty for deviation in that specific state component. In your example: <span class="math display">\[\mathbf{Q} = \text{diag}([1, 10, 0, 0]) = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 10 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}\]</span> This means:
<ul>
<li>You penalize the square of the cart’s position (<span class="math inline">\(x^2\)</span>) with a weight of 1.</li>
<li>You heavily penalize the square of the pendulum’s angle (<span class="math inline">\(\theta^2\)</span>) with a weight of 10, indicating it’s crucial to keep the pendulum upright.</li>
<li>You do not directly penalize the squared cart velocity (<span class="math inline">\(\dot{x}^2\)</span>) or pendulum angular velocity (<span class="math inline">\(\dot{\theta}^2\)</span>) in the cost function.</li>
</ul></li>
<li><strong>R (Control Weighting Matrix/Scalar):</strong> This is a symmetric positive definite matrix (or a scalar for a single control input) that penalizes the magnitude of the control input. A larger <span class="math inline">\(R\)</span> implies that you want to use less control effort, which might lead to slower system response or larger state deviations. In your example: <span class="math display">\[R = 0.1\]</span> This means the square of the control input (<span class="math inline">\(u^2\)</span>) is penalized with a weight of 0.1.</li>
</ul>
</section>
<section id="optimal-gain-matrix" class="level3">
<h3 class="anchored" data-anchor-id="optimal-gain-matrix">3. Optimal Gain Matrix:</h3>
<ul>
<li><strong>K (State Feedback Gain Matrix):</strong> This is the constant gain matrix calculated by the LQR algorithm. It defines the optimal control law: <span class="math inline">\(u(t) = - \mathbf{K}\mathbf{s}(t)\)</span> This matrix tells you how much to adjust the control input <span class="math inline">\(u(t)\)</span> based on the current values of each state variable to minimize the defined cost function <span class="math inline">\(J\)</span>. The <code>control.lqr(A,B,Q,R)</code> function computes this <span class="math inline">\(\mathbf{K}\)</span>. In your example, <code>gain</code> will be a row vector of 4 elements: <span class="math display">\[\mathbf{K} = \begin{bmatrix} K_x &amp; K_\theta &amp; K_{\dot{x}} &amp; K_{\dot{\theta}} \end{bmatrix}\]</span> So, the control force applied is: <span class="math display">\[u(t) = - (K_x x(t) + K_\theta \theta(t) + K_{\dot{x}} \dot{x}(t) + K_{\dot{\theta}} \dot{\theta}(t))\]</span> This <span class="math inline">\(\mathbf{K}\)</span> is designed to bring the cart to <span class="math inline">\(x=0\)</span> and the pendulum to <span class="math inline">\(\theta=0\)</span> (upright) while respecting the balance between state deviations and control effort defined by <span class="math inline">\(\mathbf{Q}\)</span> and <span class="math inline">\(R\)</span>.</li>
</ul>
<p>For an LTI system with an infinite horizon, K is constant.</p>
<p>For an LTI system with a finite horizon, K is time-varying (calculated via DRE).</p>
<p>For a nonlinear system, if you linearize around a fixed equilibrium, you get a constant K (local control).</p>
<p>For a nonlinear system, if you linearize around a time-varying trajectory (e.g., for tracking) or re-linearize at each step (as in MPC), then the underlying A(t) and B(t) matrices change, and thus the effective feedback gain K will be time-varying.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 2%">
<col style="width: 50%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Finite-Horizon LQR (Example)</th>
<th style="text-align: left;">Model Predictive Control (MPC)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Calculation</strong></td>
<td style="text-align: left;">Solved <strong>once, offline</strong></td>
<td style="text-align: left;">Solved <strong>repeatedly, online</strong> at each time step</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Gain K</strong></td>
<td style="text-align: left;">Pre-computed <strong>time-varying schedule</strong> <span class="math inline">\(\mathbf{K}(t)\)</span> for the full horizon.</td>
<td style="text-align: left;"><strong>No explicit <span class="math inline">\(\mathbf{K}(t)\)</span> schedule is used for control application</strong>. Instead, a sequence of open-loop controls is generated, and only the first is applied. The underlying optimization at each step might use an LQR-like calculation, where an internal, temporary <span class="math inline">\(\mathbf{K}\)</span> might be derived for that specific horizon, but it’s not directly applied as a time-varying feedback law over the <em>entire</em> system operation.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Horizon</strong></td>
<td style="text-align: left;">Fixed starting point (<span class="math inline">\(t_0\)</span>), fixed end point (<span class="math inline">\(T\)</span>)</td>
<td style="text-align: left;"><strong>Receding horizon</strong>: Always looks <span class="math inline">\(N\)</span> steps ahead from current time</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Adaptivity</strong></td>
<td style="text-align: left;">Less adaptive to unmodeled disturbances or errors (relies on pre-computed plan).</td>
<td style="text-align: left;">Highly adaptive to disturbances and model inaccuracies because it re-optimizes at every step using the latest state.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Computational Load</strong></td>
<td style="text-align: left;">Low online computational load (just lookup)</td>
<td style="text-align: left;">High online computational load (solves an optimization problem at each step)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Nonlinear Systems</strong></td>
<td style="text-align: left;">Limited to local linearization around an equilibrium.</td>
<td style="text-align: left;">Can handle nonlinearities directly by including them in the optimization, though it often involves linearization at each step for computational tractability.</td>
</tr>
</tbody>
</table>
<p>So, while LQR for a finite horizon is a building block for understanding the optimization part of MPC, MPC adds the crucial “receding horizon” and “re-optimization at each step” features that make it distinct and powerful for complex, real-time control problems.</p>
</section>
</section>
<section id="mpc" class="level2">
<h2 class="anchored" data-anchor-id="mpc">MPC</h2>
<ul>
<li>step 1: taking dynamics A, B as input, formulate finite horizont stateinput sequences in a dynamic programmed (matrix) way <span class="math inline">\(X_k = \mathbf{\Phi} x(k) + \mathbf{\Gamma} U_k\)</span></li>
<li>step 2: substituting into J cost function (just the same as LQR), now we have <span class="math inline">\(J(U_k,x(k))\)</span> into <span class="math inline">\(\frac{1}{2}U_k^T H U_k + G^T U_k + J_{const}\)</span>.</li>
<li>using QP solver, because this is the format that the solution method is known</li>
<li>only apply the first one, and next step reapply MPC</li>
</ul>
</section>
</section>
<section id="introduction-to-rl" class="level1">
<h1>Introduction to RL</h1>
<ul>
<li>MBRL: using Dynamic Programming</li>
<li>Model-free: Monte Carlo –(improve)— Temporal Difference (SARSA (on policy) vs Q-Learning (off-policy))</li>
</ul>
<p>Monte Carlo:</p>
<ul>
<li>epsilon-greedy =&gt; Q* != Q_pi. epsilon greedy to sample actions, which are mostly only suboptimal and actually leads to longer trajectory</li>
</ul>
<p>Q-Learning:</p>
<ul>
<li>even epsilon-greedy, Q* = Q_pi, because it is off-policy. BUT epsilon-greedy still leads to suboptimal longer trajectory like above in MC</li>
</ul>
<p>How do V* and Q* relate? <span class="math display">\[Q^*(s,a)=R(s,a)+\gamma \mathbb{E}_{s' \sim P(s,a)}[V^*(s')]\]</span> for each Q slot in the environment (4, 4, 4), there are 4 Q(s,a)</p>
<p><span class="math display">\[V^*(s) = \max_{a} Q^*(s, a)\]</span> <span class="math display">\[V^*(s) = \average_{a} Q^*(s, a)\]</span> just need to sum over all Q(s, a) mentioned above, because V is environment of (4, 4)</p>
<section id="model-based" class="level2">
<h2 class="anchored" data-anchor-id="model-based">Model-based</h2>
<section id="bellman-optimally-using-value-iteration" class="level3">
<h3 class="anchored" data-anchor-id="bellman-optimally-using-value-iteration">Bellman Optimally using Value Iteration</h3>
<pre><code>policy = lambda s: [0.25,0.25,0.25,0.25]
V = np.zeros((4,4))
Q_pi = np.zeros((4,4,4))

for steps in range(200):
    for state_idx in range(16):
        action_probs = policy(state)
        subseq_states = np.clip(state+directions,0,3)
        V_subseq_states = np.array([V_pi[tuple(idx)] for idx in subseq_states])
        V_pi[state] = np.sum(action_probs*(rewards+gamma*V_subseq_states))
        Q_pi[state] = rewards + gamma * V_subseq_states</code></pre>
<p>The goal in reinforcement learning is to find a policy that maximizes the discounted future return that the agent can expect. In other words, we want to find a policy <span class="math inline">\(\pi^*\)</span> that dominates all other policies: <span class="math inline">\(V^{\pi^*} := V^* \ge V^\pi\)</span> for all <span class="math inline">\(\pi\)</span>. It turns out that there is always at least one policy that achieves the optimum. The Bellman optimality equations express this via the recurrence:</p>
<p><span class="math display">\[V^*(s) = \max_{a \in \mathcal{A}} \left[ R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s, a)(s') V^*(s') \right]\]</span></p>
<p>The state-action value function can be defined in a similar way as above:</p>
<p><span class="math display">\[Q^*(s, a) = R(s, a) + \gamma \mathbb{E}_{s' \sim P(s,a)}[V^*(s')]\]</span></p>
<p>Once we know <span class="math inline">\(Q^*\)</span> we can find a deterministic <span class="math inline">\(\pi^*(s)\)</span> via:</p>
<p><span class="math display">\[\pi^*(s) = \arg\max_{a \in \mathcal{A}} Q^*(s, a)\]</span></p>
<p>We can obtain <span class="math inline">\(V^*\)</span> and <span class="math inline">\(Q^*\)</span> by an algorithm called value iteration, that repeatedly applies the Bellman optimality equation to all states/state-action pairs.</p>
</section>
</section>
<section id="model-free" class="level2">
<h2 class="anchored" data-anchor-id="model-free">Model-free</h2>
<p>every model-free algorithms has to work based on Monte Carlo for trajectory sampling.</p>
<section id="learning-v-and-q-from-data-from-monte-carlo-samples" class="level3">
<h3 class="anchored" data-anchor-id="learning-v-and-q-from-data-from-monte-carlo-samples">Learning V* and Q* from data from Monte Carlo Samples</h3>
<p>So far we have used the Bellman Optimality Equation and our exact knowledge of R and P to compute V and Q. However, in the real world, we usually do not have access to these functions. One possible approach is Monte Carlo simulation, which estimates the value of actions or states through sampling full episodes. Monte Carlo methods sample multiple episodes of experience to approximate the expected return.</p>
<p>The state value function of a given state under a policy <span class="math inline">\(\pi\)</span> is then the average return that was obtained when starting in that state:</p>
<p><span class="math display">\[V^\pi(s) \approx \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_i\]</span></p>
<p>where <span class="math inline">\(G_i\)</span> is the obtained discounted episode return. <span class="math inline">\(Q^\pi\)</span> is defined analogously.</p>
<p>We will here use an <span class="math inline">\(\epsilon\)</span>-greedy Q-policy. The policy is therefore changing over the course of our trial. We thus use an update equation:</p>
<p><span class="math display">\[V(s) \leftarrow (1-\alpha) \cdot V(s) + \alpha \cdot G_i\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is called the learning rate.</p>
<pre><code>for episode in range(total_episodes):
  while step&lt;max_steps_pr_episode:
    states.append(s)
    a = epsilon_greedy_Q_policy(s, Q_pi_hat)  # Pick action.
    actions.append(a)
    next_s, r, terminal, _ = env.step(a)  # Take a step in the environment.

  while (step goes backward):
    G = gamma * G + rewards[-1]
    rewards = rewards[:-1] #pop out the last step
    s = states[-1]
    states = states[:-1]
    a = actions[-1]
    actions = actions[:-1]
    Q_pi_hat[tuple(s)][a] = (1-alpha)*Q_pi_hat[tuple(s)][a] + alpha * G #kinda an average between old and new
</code></pre>
</section>
<section id="mc-upgrade-to-temporal-difference-specifically-q-learning" class="level3">
<h3 class="anchored" data-anchor-id="mc-upgrade-to-temporal-difference-specifically-q-learning">MC Upgrade to Temporal Difference (specifically Q-Learning)</h3>
<p>Q-Learning process resembles dynamic programming’s iterative refinement of value functions. However, Q-learning diverges by employing sample-based updates akin to Monte Carlo simulation, where experiences from interactions with the environment inform the value estimates, a.k.a. bootstrapping</p>
<p><span class="math display">\[Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right)\]</span></p>
<ul>
<li>Different from Value Iteration (Bellman Equation) because it does SAMPLE TRAJECTORY like MC</li>
<li>But why do not we use that Bellman Equation here? because it has sth to do with transition probability, we cannot simply sample all transition same probability</li>
<li>Therefore, sampling also means that we indirectly find this reward with this probability, like what is the expected (<span class="math inline">\(r_{t+1} + \gamma \max_{a} Q(s_{t+1}, a)\)</span>), instead of the probability we already knew</li>
<li>it updates NOT IN REVERSE ORDER. It updates in forward order.</li>
<li>utilizes the reward of the next step IN THAT TRAJECTORY + its maximum expected value - previously past experienced of the current state</li>
<li>the one near the terminal will directly rewarded the most with the most weight, because the term <span class="math inline">\(\gamma \max_{a} Q(s_{t+1}, a)\)</span> is then the terminal reward. Sometimes people actually expand this TD(n) so that we could get more from the terminal state.</li>
</ul>
<pre><code>for i_episode in range(total_episodes):
    while step&lt;max_steps_per_episode:
      a = epsilon_greedy_Q_policy(s_disc, Q=Q, eps=eps)  # Pick action.
      next_s, r, terminal, truncated, _ = env.step(a)  # Take a step in the environment.
      score += r
      delta = r + gamma * max(Q[tuple(next_s_disc)]) - Q[tuple(s_disc)][a]
      Q[tuple(s_disc)][a] += alpha * delta  # Q-learning update.
</code></pre>
</section>
<section id="dqn-representing-state-using-theta" class="level3">
<h3 class="anchored" data-anchor-id="dqn-representing-state-using-theta">DQN Representing state using theta</h3>
<p><strong>Goal: Deterministic argmax Policy</strong></p>
<p>Now the network takes care of pivoting known Q-state to create a function that can apply for continuous interpolated range of states, just like linear regression. MSE Loss function involved to minimize loss between observed from sampled trajectory (using Bellman Equation bootstrapping value of the next state in trajectory as ground truth) vs network Q-values</p>
<p>BUT, simply replacing Q-table with the network not efficient =&gt; technical tricks:</p>
<ul>
<li>replay buffer stores each transition (s, a, s+1, r+1, done?), so that when training we treat each transition separately, instead of being in a consecutive sequence like from sampling</li>
<li>inferencetarget network is a weight frozen copy of the learning network and only update after 1000 steps, because learning network when changes weight might shift a little other weights</li>
</ul>
<pre><code>class QNetwork(nn.Module):
  # input n_states, output n_actions

for i_episode in range(1, n_episodes+1):
    for t in range(max_t):
        action = agent.act(state, eps)
        next_state, reward, terminated, truncated, _ = env.step(action)
        agent.step(state, action, reward, next_state, terminated)

where:
    def act(self, state, eps=0.):
        action_values = self.qnetwork_local(state) #find out the probability for each action
        # Epsilon-greedy action selection
        if random.random() &gt; eps:
            return np.argmax(action_values.cpu().data.numpy())
        else:
            return random.choice(np.arange(self.action_size))

    def step(self, state, action, reward, next_state, done):
        self.memory.add(state, action, reward, next_state, done)
        self.t_step = (self.t_step + 1) % learn update_every timestep
            if (enough samples for a batch):
                experiences = self.memory.sample()
                self.learn(experiences, gamma)

    def learn(self, experiences, gamma):
        states, actions, rewards, next_states, dones = experiences
        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)
        ### Calculate target value from bellman equation bootstrapping the next state value in this trajectory
        q_targets = rewards + gamma * q_targets_next * (1 - dones)
        ### Calculate expected value from local network
        q_expected = self.qnetwork_local(states).gather(1, actions)

        ### Loss calculation (we used Mean squared error)
        loss = F.mse_loss(q_expected, q_targets)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()</code></pre>
</section>
<section id="policy-gradients-representing-policy-using-theta" class="level3">
<h3 class="anchored" data-anchor-id="policy-gradients-representing-policy-using-theta">Policy Gradients Representing policy using theta</h3>
<p><strong>Goal: Stochastic Policy instead of Deterministic argmax Policy as DQN</strong> <strong>This one is Monte Carlo style, it waits until the end</strong> This time theta is used to directly learn the behavior instead of state-action value =&gt; better for high dimensional or continuous space. It samples trajectories, computing their returns, and adjusting the policy parameters based on these returns. It updates theta</p>
<p><span class="math display">\[
\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha \gamma^t g_t^m \nabla_{\mathbf{\theta}} \ln \pi(a_t^m|s_t^m, \mathbf{\theta})
\]</span></p>
<p>implementation similar to Q-learning</p>
<pre><code>class Policy_Network(nn.Module):
  #input observation space, output action space

for episode in range(total_num_episodes):
    while not done:
        action = agent.act(obs)
        obs, reward, terminated, truncated, info = env.step(action)
        agent.rewards.append(reward)
        score += reward
        done = terminated or truncated
    agent.learn()

where:
    def act(self, state: np.ndarray) -&gt; float:
        action_means, action_stddevs = self.net(state)
        # create a normal distribution from the predicted mean and standard deviation and sample an action
        distrib = Normal(action_means[0] + self.eps, action_stddevs[0] + self.eps)
        action = distrib.sample()
        prob = distrib.log_prob(action)  # used for update
        self.probs.append(prob)
        return action

    def learn(self):
        #calculate G for each timestep in the trajectory
        #calculate G backwards in Monte Carlo style
        for R in self.rewards[::-1]:
            running_g = R + self.gamma * running_g
            gs.insert(0, running_g)
        deltas = torch.tensor(gs)

        loss = 0
        # this time we define loss by ourselves
        # now loss for each timestep in the trajectory
        for log_prob, delta in zip(self.probs, deltas):
            loss += log_prob.mean() * delta * (-1)

        # Update the policy network
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()</code></pre>
<section id="difference-between-value-based-gradient-policy-based" class="level4">
<h4 class="anchored" data-anchor-id="difference-between-value-based-gradient-policy-based">Difference between Value-based &amp; Gradient Policy based</h4>
<ul>
<li>Value based: try to update Q -&gt; argmax Q deterministic -&gt; action</li>
<li>Policy Gradient based: try to update a stochastic distribution of action | given state. -&gt; directly action</li>
</ul>
</section>
</section>
</section>
</section>
<section id="bayesian-optimization-gaussian-processes" class="level1">
<h1>Bayesian Optimization &amp; Gaussian Processes</h1>
<ul>
<li>Given seen data samples of a function, want to find that funciton’s min/max but cannot use gradient descent =&gt; surrogate function Gaussian Process (GP)</li>
<li>GP is a probability distribution over all possible function, all seen sampled data create together a join distribution
<ul>
<li>Goal Problem setup: posterior mean &amp; variance of a new datapoint!!!</li>
<li>Key Knowledge to lean on: Formulate jointly multivariate Gaussian distribution A|B where we already know how to calculate posterior mean and covariance</li>
<li>Formulate covariance matrix (yt,f*) and try to arrange in that same structure / format =&gt; we know how to calculate these posterior mean/covariance</li>
<li>covariance matrix based on the kernel function saying how far two points from each other (RBF for Gaussian, or also be Euclidian distance) K_ij = k(xi, xj), calculate these for all pair of points and put it in the cov matrix</li>
<li>Now plug in all these covariance matrix, kernel function into the A|B jointly Gaussian distribution to calculate a new unseen datapoint</li>
<li>BUT… where to take a datapoint? sample about 1000 datapoints use of acquisition function (balance exploitation &amp; exploration)! (EI: where variance &amp; mean are the highest)</li>
<li>Multiple Objective BO actually easy, (e.g.&nbsp;best material: each fixed theta give different function with different max on the graph, test many theta to find many max points for each function =&gt; use Pareto)</li>
</ul></li>
</ul>
<pre><code>for i in range(1):
    y_next = true_function(x_next)
    bayes_opt.update_model(x_next, y_next)
    x_next = bayes_opt.get_candidate(aq_func)

def update_model(self, x, y):
    self.x_samples = np.append(self.x_samples, x)
    self.y_samples = np.append(self.y_samples, y)
    self.surrogate_model.fit(self.x_samples, self.y_samples) #using true function to update the GP covariance matrix

self.surrogate_model = GaussianProcess(kernel=lambda x1, x2: np.exp(-.5 * np.subtract.outer(x1, x2)**2))

def get_candidate(self, acquisition_function, num_candidates=1000):
    x_canditates = np.linspace(self.bounds[0], self.bounds[1], num_candidates)
    mu,sigma = self.surrogate_model.predict(x_canditates) #calculate variance and mean from all other data
    aqf_values = acquisition_function(mu, sigma, np.max(self.y_samples))
    best_index = np.argmax(aqf_values)</code></pre>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/tienthangdinh\.github\.io\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>