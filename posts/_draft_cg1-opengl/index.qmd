---
title: "Rendering"
date: 2025-08-06
categories: [Computer Graphics, Rendering]
format:
  html:
    toc: true
    code-fold: true
    math: mathjax
draft: true
---
# OpenGL

- Timeline:
  
  1.  **Create Program:** You compile your shaders and link them into a `programID`.
  2.  **Bind Program:** You make this program the active one using `glUseProgram(programID)`.
  3.  **Link VAO/VBO Attributes:** The connection between your vertex data and the shader's input variables (`aPos`, `aColor`) is set up.
  4.  **Set Uniforms:** You get the location of your uniform variable (`uniform mat4 transform;`) and then use a `glUniform*` function to send its value to the active shader program.
  5.  **Draw:** The `glDraw*` call then uses the currently bound VAO/VBO with the active shader program and its uniform values to render the geometry.
     
1. **Why is it important for graphics application development to wait for next events during the main loop?**

- "Games Architecture" runs in a continuous, busy loop, fully occupying the CPU and GPU even when there is no new input or change in the scene. 
- "Application Architecture" uses an event-driven loop. The program "waits" until an event occurs (like a mouse click, key press, or window resize), and only then do the CPU and GPU perform work to handle the event and redraw the scene => more efficient and can coexist with other applications
- Threaded Architecture:...

2. **Name three architectural concepts of modern GPU that facilitate a large throughput!**
    
GPU works on SIMT principle, it has processing units (either FP64, FP32, INT, TENSORS):
- parallel streaming processing: (like vertices or fragments) 
- Hiding memory access time: GPUs use scheduling for thousands of threads (SM, wraps), scheduler instantly switches to another group of threads if this one waiting for data
- Load balancing: distributed streaming multiprocessors (SMs) on the GPU, => no bottleneck.

3. **How do you have to prepare an analytic surface representation such that it can be rendered with the rendering pipeline of a GPU?**

- Tesselation on CPU: continuous analytic surface -> discrete surface = polygonal mesh (set of vertices, faces, color).
- GPU: Geometry building -> view transformation -> fragment generation -> color adding

1. **Step 1: How are GPU objects managed in OpenGL?**

- We first need a window and a context to that window
````
window = glfwCreateWindow(...) 
glfwMakeContextCurrent(window)
````
- We have to create these objects to be stored in GPU:

  - Buffer Objects (VBOs): vertex (positions, normals, colors)
  - Vertex Array Objects (VAOs): defining how should vertex shader read VBOs (INF, float, at what allocation, interleaving,...)
  - Textures: image to be added in shader
  - Shader Programs: Linked, executable programs composed of individual shaders (vertex shader for height generation, fragment shader for color)
  
- And then: every objects are defined as GLuint:
  
  1. Generate a name for a new object (glGenBuffers)
   ````
    GLuint vboOpsitions;
    glGenBuffers(1, &vboPositions);
    ````
  2. Bind the object to a specific target slot (convention for vertices is GL_ARRAY_BUFFER) in the OpenGL (glBindBuffer)
   ````
   glBindBuffer(GL_ARRAY_BUFFER, vboPositions); 
   ````
  3. Modify the object that is currently bound to the target (e.g., upload array of vec4 with glBufferData)
   ````
   glBufferData(GL_ARRAY_BUFFER, posvector.size() * sizeof(vec4), posvector.data())
   ````

4. 1. **After creating Buffer, how to bind them to Vertex Array Object?**

So after we did the same to each **attribute separately** position, normals, colors, each an object, we now add them on VAO.

For each attribute, bind the buffer -> enable -> specify how to read data

```
glGenVertexArrays(1, &vao);
glBindVertexArray(vao);

glBindBuffer(GL_ARRAY_BUFFER, vboPositions);
glEnableVertexArray(0); // address of the shader input
glVertexAttribPointer(0, 4, GL_FLOAT, GL_FALSE, sizeof(vec4), 0) // 4 float for each component therefore sizeof(vec4), no offset
glBindVertexArray(0) //unbind

```

Do not forget to load Shader and read Shader before that
```
program_id = glCreateProgram()
vertex_shader_id = glCreateShader()
glShaderSource(vertexShader, 1, &vertexShaderSource, NULL);
glCompileShader(vertexShader);
glAttachShader(program_id, vertex_shader_id)
glAttachShader(program_id, fragment_shader_id)
glLinkProgram(program_id)
```
4. 2. **Render it all!**
```
glBindVertexArray(vao);
glDrawArrays(GL_TRIANGLES, 0, 3) //draw triangles starting from the first vertice, each triangle use 3 vertices
glBindVertexArray(0);
```
5. **How to debug in OpenGL?**
   
  1. Define callback function, basically print out `debug_callback(message,...)`
  2. Set debug context `glfwWindowHint(GLFW_OPENGL_DEBUG_CONTEXT, GLFW_TRUE);`
  3. Register callnback function `glDebugMessageCallback(debug_callback, 0);`

6. **Given a picture of the OpenGL 3.2 pipeline without specification of the data types (vertices, primitives, fragments): Explain the data flow through the pipeline!**
   
  1. Vertex Buffer VBO
  2. individual vertex from VBO -> VAO -> Vertex Shader (each vertex separately processed) (model coords -> world coords -> projection coords)
  3. Primitive Assembly (group vertices -> triangles).
  4. (Optionally) Geometry Shader
  5. Clipping primitives against the view frustum + Back-face culling
  6. Rasterizer clipped primitives -> fragments (aka pixels) + interpolated natural color
  7. Fragment Shader, textured lighted color and depth.
  8. Fragment Buffer (depth test)

7. **Explain the difference between a shader and a shader program! What are the building blocks of shaders and shader programs respectively?**

- Shader:  vertex / pixel -> vertex / pixel in .GLSL
- Shader Program: a set of compiled shaders in .bin

8. **How can you transfer data from a C / C++ program to a shader program?**

  1. VBO -> VAO -> in Shader
  2. Uniform variables (transformation matrix, lighting parameters) -> `uniform mat4 modelViewProjection;`

9. **How is streaming input and streaming output of vertex shader and a fragment shader defined?**
    
- Vertex Shader: in from VAO -> out color, gl_Positions (this one is a must!!! vec4 clip-space coords)
- Fragment Shader: rasterizer interpolated color -> textured lighting color -> framebuffer

10. **Given a vertex and a fragment shader, explain the data flow (input, transfer from vertex to fragment shader, output)!**

- slide 20, important note: output of vertex color -> rasterized -> input of fragment color for shader

11. **Given a shader. Determine if it is a vertex or fragment shader.**
    
- vertex shader has to output gl_Positions

12. **Expand a vertex / fragment shader pair by a variable passed from vertex to the fragment shader!**

- e.g. output intensity -> input intensity of the next one (remember it is rasterized)


13. **How do you pass values for a uniform variable to a shader program?**

    1. activate the shader program `glUseProgram(programID);`
    2. create location for the uniform variable in shader program `GLint transformUniformLocation = glGetUniformLocation(programID, "transform");
    3. `glUniformMatrix4fv(transformUniformLocation)`

14. How is visibility sorting done in the rendering pipeline with the depth buffer algorithm?

For every single fragment, its depth value is compared to the value currently stored in the depth buffer at that fragment's pixel location.
if < or > then will change the color, depth in frame buffer respectively.



```
...

// Vertex data for a triangle
float vertices[] = {
    // positions         // colors
     0.5f, -0.5f, 0.0f,  1.0f, 0.0f, 0.0f, // bottom right
    -0.5f, -0.5f, 0.0f,  0.0f, 1.0f, 0.0f, // bottom left
     0.0f,  0.5f, 0.0f,  0.0f, 0.0f, 1.0f  // top
};

int main() {
    // context init
    GLFWwindow* window = glfwCreateWindow(800, 600, "OpenGL Example", NULL, NULL);
    glfwMakeContextCurrent(window);

    // 1. Create Program
    unsigned int programID = createShaderProgram("shader.vert", "shader.frag"); //glShaderSource -> glCompileShader -> glAttachShader ->  glCreateProgram -> glLinkProgram
    
    // -init VBO, VAO
    unsigned int VBO, VAO;
    glGenVertexArrays(1, &VAO);
    glGenBuffers(1, &VBO);
    glBindVertexArray(VAO);
    glBindBuffer(GL_ARRAY_BUFFER, VBO); //vertices
    glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); //vertices

    // 3. Link VAO/VBO Attributes
    // Position attribute
    glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(float), (void*)0); //take this address
    glEnableVertexAttribArray(0); //and write down here in the shader
    // Color attribute
    glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(float), (void*)(3 * sizeof(float)));
    glEnableVertexAttribArray(1);

    while (!glfwWindowShouldClose(window)) {
        // Render loop
        glClearColor(0.2f, 0.3f, 0.3f, 1.0f);
        glClear(GL_COLOR_BUFFER_BIT);
        
        // 2. Bind Program
        glUseProgram(programID);

        // 4. Set Uniforms
        GLint transformUniformLocation = glGetUniformLocation(programID, "transform");
        glm::mat4 transform = glm::mat4(1.0f);
        transform = glm::rotate(transform, (float)glfwGetTime(), glm::vec3(0.0f, 0.0f, 1.0f));
        glUniformMatrix4fv(transformUniformLocation, 1, GL_FALSE, glm::value_ptr(transform));

        // 5. Draw
        glBindVertexArray(VAO);
        glDrawArrays(GL_TRIANGLES, 0, 3);

        glfwSwapBuffers(window);
        glfwPollEvents();
    }
    ...
}
```

because look at the vertex shader
```
#version 330 core
layout (location = 0) in vec3 aPos;
layout (location = 1) in vec3 aColor;

out vec3 ourColor;

uniform mat4 transform;

void main() {
    gl_Position = transform * vec4(aPos, 1.0);
    ourColor = aColor;
}
```

# Geometry Rendering
1. **Discuss how geometry can be stored in buffers and textures on the GPU?**
   
vertex buffer (VBO) + texture

2. **Explain how geometry is rendered with OpenGL by referring to Vertex Buffer Objects, Vertex Array Objects, and Shader Programs!**
   
Initialization (done once):

   - Shader Programs: compiled, attached and linked
   - Vertex Buffer Objects (VBOs)
   - Vertex Array Objects (VAOs): glVertexAttribPointer (update VBO onto VAO)
  
Main Loop (done every frame):

  - Bind Program and VAO:  choose program_id glUseProgram + VAO to be drawn (glBindVertexArray)
  - Set Uniforms: model-view-projection matrix, are updated.
  - Draw: 

3. **Discuss advantages and disadvantages of interleaved versus non-interleaved storage of geometry in buffer objects!**
   
Interleaved Storage: this is what we have always been using!!! look at the data you will know
  - Advantage: all attributes loaded in cache, and also good for individual processing
  - Disadvantage: Updating a single attribute (e.g., only the positions for an animation) is complex, because has to skip over the other attributes.
  
Non-Interleaved Storage: (opposite)
  - Advantage: This allows for the fast replacement of individual attributes. e.g. only touching position buffer without touching the normals or texture coordinates.
  - Disadvantage: multiple fetches per vertice for each attribute

4. **How do you specify the format and component type for a vertex attribute?**
   
used in the function glVertexAttribPointer: 
- layout(location = 0) in shader
- number of scalar elements in component (e.g., 3 for a vec3).
- data type (e.g., GL_FLOAT).
- stride (distance between 2 elements (vd: 6 floats))
- offset for first component

5. **What is the difference between uniform variables and vertex attributes?**
Vertex Attributes:  per-vertex data.
Uniform Variables are constant for a single draw call (for all vertices) (model-view-projection matrix, light positions)

6. **How do you get the index of a vertex attribute array declared as input to the vertex shader necessary for specifying the corresponding attribute pointer of a vertex array object?**

- layout(location = 0) in vec4 vPosition;
- GLint i = glGetAttribLocation(program, "vPosition");

7. **Explain indexed rendering and discuss when this is advantageous over retained mode rendering?**

- Problem: In a mesh, a single vertex is often part of multiple triangles (typically 6 in triangle or 2 in stripification). Without indexed rendering, you would have to duplicate that vertex's data in the vertex buffer for every triangle it belongs to.

- Solution: linear sequence of vertices for each triangle with repeated vertices => 1 VBO + 1 VEO
- Advantage: store vertice only once
- Disadvantage: extra memory and complex access logic
- Usage: bind an extra VAO `glBufferData(GL_ELEMENT_ARRAY_BUFFER, size, datapointer);`
```
//----VEO
std::vector<GLuint> elements;
// fill elements vector with indices
elements.clear();
for (int i=0; i<N; ++i) {
  for (int j=0; j<=M; ++j)
    for (int k=0; k<2; ++k)
      elements.push_back((i+k)*(M+1)+j);
elements.push_back(RESTART_IDX);
}
// create buffer object for element data
GLuint ebo;
glGenBuffers(1, &ebo);
glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, ebo);
// transfer element data to GPU buffer object
glBufferData(GL_ELEMENT_ARRAY_BUFFER,sizeof(GLuint)*elements.size(),&elements[0], GL_STATIC_DRAW); 


//-----VBO
// bind buffer object to array buffer target
glBindBuffer(GL_ARRAY_BUFFER, vbo);
glEnableVertexAttribArray(0);
glVertexAttribPointer(0, 4, GL_FLOAT, GL_FALSE,sizeof(vertex), 0);
glEnableVertexAttribArray(1);
glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE,sizeof(vertex), sizeof(vec4));
glEnableVertexAttribArray(2);
glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE,sizeof(vertex), sizeof(vec4)+sizeof(vec3));
```

1. **What primitive types can be input and output to the geometry shader?**
   
The geometry shader is invoked once per primitive:
- Input Primitive Types: points, lines, lines_adjacency, triangles, triangles_adjacency.
- Output Primitive Types: points, line_strip, triangle_strip.
The geometry shader consumes whole primitives and emits a sequence of vertices that are organized into one of the output strip types.

9. **Why is the input data to the geometry shader to be declared as arrays?**
The geometry shader's fundamental purpose is to process an entire primitive at once, not just a single vertex. Since a primitive consists of multiple vertices (e.g., a triangle has 3, a line with adjacency has 4), the input variables that receive vertex attributes must be declared as arrays.

10. **Given a shader program. Determine the shader type and its functionality!**

- Vertex Shader: Will always write to the special output gl_Position.
- Geometry Shader: Will have layout(points) in; layout(triangle strip) out;
- Fragment Shader: Will write its final color to an out vec4 variable 
- Functionality: Reading the code in main(). For example, a geometry shader that takes lines as input and outputs a triangle_strip

11.  **How do you pass a variable that is interpolated over a triangle from the geometry shader to the fragment shader?**

- Vertex shader output vertices
- primitive assembly
- In the geometry shader, declare an out variable (e.g., out vec3 worldPos;). Before calling EmitVertex(), assign a value to this variable. (emits the vertices of a new primitive)
- Rasterizer interpolate everything across the surface of this primitive
- In the fragment shader, declare a matching in variable (e.g., in vec3 worldPos;).

12.  **Explain the principle of instanced rendering and describe a typical application!**

Render 3 trees at one call or to generate complex tesselation at once

# Lighting
1. **Explain the different stages of light transport from emission to detection!**
- Emission: A light source converts energy (e.g., electrical) into light photons.
- Transport: Light travels in straight lines through a homogeneous medium like a vacuum.
- Scattering: In a transparent medium like air, light can be deflected by particles (e.g., skin).
- Reflection: Light bounces off surfaces.
- Refraction: Light passes through transparent surfaces (like glass or water), changing its direction.
- Absorption: Light is absorbed by surfaces or media, converting its energy into another form (e.g., heat).
- Detection: An eye or an image sensor measures the spatial distribution of the light that reaches it.

2. **What is needed to specify a goniometric point light source?**
- goniometric point light source = light is emitted from a single point: emission position, intensity varied by directions & decreased by distance

3. **What is color bleeding?**
- color reflection from 1 surface to other surfaces 

4. **How do the results of light simulations with single versus multiple reflections compare?**
- Single Reflection: Only allow bounce from original light source (not so realistic)
- Multiple Reflections: Light is allowed to bounce multiple times between surfaces (indirect lighting). Each bounce adds more light to the scene. As a result, simulations with multiple reflections produce brighter, more realistic scenes with softer shadows and effects like color bleeding.

5.  **Explain the difference between hard and soft shadows!**
- Hard Shadows:  sharp (when light source is a single point)
- Soft Shadows:  fuzzy, smoothed (when the light source is an area light => partially occluded)

6. **Which light source types result in hard shadows?**
- Point light sources  -> hard shadow

7. **Penumbra**
- area light sources -> soft shadow

8. **Argue why human perception motivates the use of gamma for sRGB color space and displays!**
Human perception of brightness is not linear. Eyes are more sensitive to darker colors (use more 8bit ranges for that)

9. **What is the rendering equation and why is it difficult to solve?**
- Sum outgoing light from a point = sum Light_emitted Light_reflect
- It is difficult to solve because: integral of all incoming lights + the incoming light themself is the outgoing of the other (recursive)

10. **Name three simplifications of the rendering equation typically adopted in real-time rendering!**
- Ignore indirect lighting
- Restrict light source types: only directional or point light
- Replace physically-based BRDFs with simpler empirical models like Phong or Blinn-Phong.

11. **How does intensity fall off for a point light source?**
The intensity (the radiance) Radiance ∝ 1 / distance²

12. **Explain why it makes sense to split local lighting into ambient, diffuse and specular reflection!**
Splitting the local illumination model into ambient, diffuse, and specular components is a useful simplification because each term models a different aspect of light reflection:
Ambient: usually not a part of BRDF
Diffuse: Lambdertian
Specular: Phong or Microfacet (gaussian distributed)

12. **Explain why it makes sense to split local lighting into ambient, diffuse and specular reflection!**

- modular and computationally cheap way to approximate the appearance of different materials.

13. **What does the ambient term in local illumination models compensate for?**
- indirect lighting that we cannot calculate but can approximate


14.  **What is a Lambertian Radiator and how does it look from different viewing angles?**
- ideal diffusion
- same radiance regardless of their viewing direction L = (phi/pi)*(Icos(alpha))/r²

15.  **The Lambertian implementation of diffuse local illumination is computed mainly from the cosine term. What does this imply for the bidirectional reflectance distribution function?**
The Lambertian depends only on the cosine of the angle between the incoming light direction (not outgoing) => BRDF for a Lambertian surface is constant for all incoming and outgoing directions.

16. **Which parameter of the Phong and Blinn-Phong implementation of specular reflection defines the roughness of the surface? Where is the cosine term in these models?**
- Roughness Parameter: The shininess parameter, m, is the exponent, the higher the shinier
- Cosine Term: The cosine term is implemented via a dot product raised to the power of m.
- Phong, it is (<ω_out, ω_refl>)^m.
- Blinn-Phong, it is (<ω_half, n>)^m.

17. **Given images of illuminated objects, estimate diffuse, specular and roughness parameters!**
Diffuse, specular, roughness

18. **In which coordinate system is the illumination calculation typically performed?**
- Eye Coordinates (also called View Space) a.k.a (0,0,0)
- The requirements: no clipping, no distortion

19. **Explain which part of lighting calculation is done on CPU, in Vertex Shader and in Fragment Shader!**
- CPU: scene, materials and lights (like color, position, shininess) and passes these values to the shaders as uniforms.
- Vertex Shader: transform geometry. It takes vertex positions and normals from object coordinates and transforms them into eye coordinates, then passes these transformed values to the next stage.
- Fragment Shader: lighting calculation. It receives the interpolated position and normal from the rasterizer, accesses the uniform variables for light and material properties, and computes the final color of the fragment.

20. **How can you provide the following parameters to the Fragment Shader in eye coordinates: position, surface normal, direction to the viewer and direction to the light source?**
- Position (p): The vertex shader calculates the eye-space position and passes it to the fragment shader as an out variable. The rasterizer interpolates this value for each fragment.
- Surface Normal (n): The vertex shader transforms the object-space normal to eye-space using the inverse transpose of the model-view matrix. It passes this as an out variable. 
- Direction to Viewer (ω_out): This is calculated inside the fragment shader. Since the viewer is at the origin (0,0,0) in eye coordinates, the direction is simply normalize(-p), where p is the fragment's interpolated position.
- Direction to Light Source (ω_in): The light's position l (in eye coordinates) is passed (also calculated from vertex shader as uniform) to the fragment shader as a uniform. The direction is then calculated inside the shader as normalize(l - p).

21. **Explain how you have to account for gamma correction in the shader pipeline to avoid unnatural darkening when the display performs gamma correction!**

- Lighting calculations in a shader produce physically linear color values.
- However, Monitor expects non-linear sRGB color values (which are approximately gamma-corrected with γ ≈ 2.2) => If you write a linear color directly to the framebuffer => it will be too dark because it saves more bit for that
- Solution: directly in fragment shader OR OpenGL function
- Additionally, you must handle input textures. If a texture is stored in sRGB format (which is common for .jpg or .png files), you must convert it to linear space inside the shader before using it in lighting calculations.

## Rendering Equation

$$
L_o(p,\omega_o) = \int_\Omega f_r(p,\omega_i,\omega_o) L_i(p,\omega_i) (n \cdot \omega_i) d\omega_i
$$

- **$$L_o(p,\omega_o)$$**  
   Outgoing radiance - the light leaving surface point $$p$$ in direction $$\omega_o$$ (what the observer sees)

- **$$\int_\Omega ... d\omega_i$$**  
   Integral over the hemisphere $$\Omega$$ above the surface, summing contributions from all incoming light directions

- **$$f_r(p,\omega_i,\omega_o)$$**  
   BRDF (Bidirectional Reflectance Distribution Function) that defines the relationship:
   - How light reflects at point $$p$$  
   - From incoming direction $$\omega_i$$ to outgoing $$\omega_o$$  
   - Determines material appearance (diffuse/glossy/metallic)

- **$$L_i(p,\omega_i)$$**  
   Incoming radiance - light arriving at $$p$$ from direction $$\omega_i$$

- **$$(n \cdot \omega_i)$$**  
   Lambertian cosine term accounting for:  
   - Angle between surface normal $$n$$ and light direction  
   - Surfaces receive less light at grazing angles  


## Common Lighting Models

### 1. Lambertian (Diffuse)
$$ f_{\text{lambert}} = k_d \cdot (n \cdot l) $$
- $$k_d$$: Diffuse albedo
- $$n$$: Surface normal
- $$l$$: Light direction
- Models perfect matte surfaces

### 2. Phong Model
$$ I = k_d \cdot (n \cdot l) + k_s \cdot (r \cdot v)^\alpha $$
- $$r$$: Reflection vector ($$r = 2(n \cdot l)n - l$$)
- $$v$$: View direction
- $$\alpha$$: Shininess exponent

### 3. Blinn-Phong Model
$$ I = k_d \cdot (n \cdot l) + k_s \cdot (n \cdot h)^\alpha $$
- $$h$$: Halfway vector ($$h = \text{normalize}(l + v)$$)
- More efficient than Phong
- Better for metallic surfaces

### Key Differences:
| Feature       | Lambertian | Phong       | Blinn-Phong |
|--------------|-----------|------------|-------------|
| Diffuse      | ✓         | ✓          | ✓           |
| Specular     | ✗         | Reflection | Halfway     |
| Performance  | Fastest   | Moderate   | Fast        |
| Realism      | Matte     | Plastic    | Metallic    |


# Texturing
1. **What types of mapping are there?**
- Color
- Traditional Bump Mapping (less storage but more computational because it needs gradient from neighboring, so it needs smapling neighbors height too)
- Normal Mapping (add some extra normal (x,z,y) but actually saved as (r,g,b)) => simple, fast
- Parallax Mapping
- Displacement (add explicitely heightmap uvh)

2. **How exactly is the uv texture defined? Like someone made it explicitly?**

3. **Tangent Method for Mapping please**
- For any given fragment on the screen, there's a unique p_eye and a unique uv because when the GPU rasterizes a triangle, it interpolates all the vertex attributes, including both the 3D position and the 2D texture coordinates, for every single fragment.
- The p_eye is the 3D position of a point on the crumpled paper.
- The uv is the 2D position of that same point on the flat paper.
- We know the change in 3D position over a pixel on the screen (dFdx(p_eye))
- We also know the change in 2D texture coordinates over the same pixel on the screen (dFdx(uv)).
- We can set up a system of equations to solve for the unknown: the change in 3D position for a change in u and v.

$$\frac{\partial \mathbf{p}_{eye}}{\partial x} = \mathbf{T} \cdot \frac{\partial u}{\partial x} + \mathbf{B} \cdot \frac{\partial v}{\partial x}$$

$$\frac{\partial \mathbf{p}_{eye}}{\partial y} = \mathbf{T} \cdot \frac{\partial u}{\partial y} + \mathbf{B} \cdot \frac{\partial v}{\partial y}$$


$$\begin{bmatrix} \frac{\partial \mathbf{p}_{eye}}{\partial x} \\ \frac{\partial \mathbf{p}_{eye}}{\partial y} \end{bmatrix} = \begin{bmatrix} \frac{\partial u}{\partial x} & \frac{\partial v}{\partial x} \\ \frac{\partial u}{\partial y} & \frac{\partial v}{\partial y} \end{bmatrix} \begin{bmatrix} \mathbf{T} \\ \mathbf{B} \end{bmatrix}$$